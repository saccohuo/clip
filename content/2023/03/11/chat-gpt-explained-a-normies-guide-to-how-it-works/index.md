---
title: "ChatGPTï¼šæ™®é€šäººå¯äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„æŒ‡å—"
date: 2023-03-11T08:47:54+08:00
updated: 2023-03-11T08:47:54+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/chatgpt-explained-a-guide-for-normies
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "ChatGPT Explained: A Normie's Guide To How It Works"
  original_lang: en
---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F6866ca04-5223-46df-82dc-7ac024bc74ab_1664x1152.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6866ca04-5223-46df-82dc-7ac024bc74ab_1664x1152.jpeg)

_**The story so far:**Â Most of the discussion of ChatGPT Iâ€™m seeing from even very smart, tech-savvy people is just not good. In articles and podcasts, people are talking about this chatbot in unhelpful ways. And by â€œunhelpful ways,â€ I donâ€™t just mean that theyâ€™re anthropomorphizing (though they are doing that). Rather, what I mean is that theyâ€™re not working with a practical, productive understanding of what the botâ€™s main parts are and how they fit together.  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ã€‚æˆ‘æ‰€çœ‹åˆ°çš„å¤§å¤šæ•°å…³äºChatGPTçš„è®¨è®ºï¼Œå³ä½¿æ˜¯éå¸¸èªæ˜ã€ç²¾é€šæŠ€æœ¯çš„äººï¼Œä¹Ÿä¸æ˜¯å¾ˆå¥½ã€‚åœ¨æ–‡ç« å’Œæ’­å®¢ä¸­ï¼Œäººä»¬éƒ½åœ¨ä»¥æ— ç›Šçš„æ–¹å¼è°ˆè®ºè¿™ä¸ªèŠå¤©æœºå™¨äººã€‚æˆ‘æ‰€è¯´çš„ "æ— ç›Šçš„æ–¹å¼ "å¹¶ä¸ä»…ä»…æ˜¯æŒ‡ä»–ä»¬åœ¨æ‹ŸäººåŒ–ï¼ˆå°½ç®¡ä»–ä»¬æ­£åœ¨è¿™æ ·åšï¼‰ã€‚ç›¸åï¼Œæˆ‘çš„æ„æ€æ˜¯ï¼Œä»–ä»¬åœ¨å·¥ä½œä¸­å¯¹æœºå™¨äººçš„ä¸»è¦éƒ¨åˆ†æ˜¯ä»€ä¹ˆä»¥åŠå®ƒä»¬æ˜¯å¦‚ä½•ç»“åˆåœ¨ä¸€èµ·çš„ï¼Œæ²¡æœ‰ä¸€ä¸ªå®é™…çš„ã€æœ‰æˆæ•ˆçš„ç†è§£ã€‚_

_To put it another way, there are some [can-opener problems](https://www.jonstokes.com/p/the-can-opener-problem) manifesting in the ChatGPT conversation, and lowering the quality of The Discourse.  

æ¢å¥è¯è¯´ï¼Œæœ‰ä¸€äº›å¼€ç½å™¨çš„é—®é¢˜åœ¨ChatGPTå¯¹è¯ä¸­è¡¨ç°å‡ºæ¥ï¼Œå¹¶é™ä½äº†ã€Šè®ºè¯­ã€‹çš„è´¨é‡ã€‚_

_To be clear, I do not know everything Iâ€™d like to know about this topic. Like everyone else, including active researchers in machine learning, Iâ€™m still on my own journey with getting my head around it at multiple levels. (Speaking of, if you run a shop that sells dedicated ML workstations and would like to publicly sponsor this newsletter by sending me one, do get in touch.)_  

è¯´ç™½äº†ï¼Œæˆ‘å¹¶ä¸äº†è§£æˆ‘æƒ³çŸ¥é“çš„å…³äºè¿™ä¸ªè¯é¢˜çš„ä¸€åˆ‡ã€‚åƒå…¶ä»–äººä¸€æ ·ï¼ŒåŒ…æ‹¬æ´»è·ƒçš„æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜ï¼Œæˆ‘ä»ç„¶åœ¨è‡ªå·±çš„æ—…ç¨‹ä¸­ï¼Œåœ¨å¤šä¸ªå±‚é¢ä¸Šå¯¹å…¶è¿›è¡Œæ€è€ƒã€‚(è¯´åˆ°è¿™é‡Œï¼Œå¦‚æœä½ ç»è¥ä¸€å®¶é”€å”®ä¸“ç”¨MLå·¥ä½œç«™çš„å•†åº—ï¼Œå¹¶æ„¿æ„å…¬å¼€èµåŠ©æœ¬é€šè®¯ï¼Œç»™æˆ‘é€ä¸€å°ï¼Œè¯·è”ç³»ã€‚) .

_That said, Iâ€™m certainly far enough along that I can help others who are a few steps behind. So what follows is my effort to help others get closer to the target in their thinking and writing about this new category of technology.  

ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘å½“ç„¶å·²ç»èµ°å¾—å¤Ÿè¿œäº†ï¼Œæˆ‘å¯ä»¥å¸®åŠ©å…¶ä»–è½åå‡ æ­¥çš„äººã€‚å› æ­¤ï¼Œä»¥ä¸‹æ˜¯æˆ‘çš„åŠªåŠ›ï¼Œä»¥å¸®åŠ©å…¶ä»–äººåœ¨æ€è€ƒå’Œå†™ä½œè¿™ä¸€æ–°ç±»åˆ«çš„æŠ€æœ¯æ—¶æ›´æ¥è¿‘ç›®æ ‡ã€‚_

At the heart of ChatGPT is a large language model (LLM) that belongs to the family of generative machine learning models. This family also includes [Stable Diffusion](https://www.jonstokes.com/p/getting-started-with-stable-diffusion) and all the other prompt-driven, text-to-whatever models that are daily doing fresh miracles on everyoneâ€™s feeds right now.  

ChatGPTçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå±äºç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹å®¶æ—ã€‚è¿™ä¸ªå®¶æ—è¿˜åŒ…æ‹¬ç¨³å®šæ‰©æ•£ï¼ˆStable Diffusionï¼‰å’Œæ‰€æœ‰å…¶ä»–æç¤ºé©±åŠ¨çš„ã€ä»æ–‡æœ¬åˆ°ä»€ä¹ˆçš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹ç°åœ¨æ¯å¤©éƒ½åœ¨å¤§å®¶çš„é¥²æ–™ä¸Šåˆ›é€ æ–°é²œçš„å¥‡è¿¹ã€‚

If you want to know more about how generative ML models work and you have some time, read the following pieces:  

å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºç”Ÿæˆæ€§MLæ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œå¹¶ä¸”ä½ æœ‰ä¸€äº›æ—¶é—´ï¼Œè¯·é˜…è¯»ä»¥ä¸‹æ–‡ç« ã€‚

-   AI content generation series: [Part 1](https://www.jonstokes.com/p/ai-content-generation-part-1-machine), [Part 2](https://www.jonstokes.com/p/ai-content-generation-part-2-tasks), [Part 4](https://www.jonstokes.com/p/ai-content-generation-part-4-whats)  
    
    AIå†…å®¹ç”Ÿæˆç³»åˆ—ã€‚ ç¬¬ä¸€éƒ¨åˆ† , ç¬¬äºŒéƒ¨åˆ† , ç¬¬å››éƒ¨åˆ†
    
-   [What Does It Mean to â€œCreateâ€ Something With Generative AI?  
    
    ç”¨ç”Ÿæˆæ€§äººå·¥æ™ºèƒ½ "åˆ›é€  "ä¸œè¥¿æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ](https://www.jonstokes.com/p/what-does-it-mean-to-create-something)
    
-   [The Hall Monitors Are Winning the AI Wars, Part 1: ChatGPT  
    
    å¤§å…ç›‘æ§å™¨æ­£åœ¨èµ¢å¾—äººå·¥æ™ºèƒ½æˆ˜äº‰ï¼Œç¬¬ä¸€éƒ¨åˆ†ã€‚èŠå¤©å®¤GPT](https://www.jonstokes.com/p/the-hall-monitors-are-winning-the)
    

â­ï¸ But if you donâ€™t have time to read all that, hereâ€™s aÂ **one-sentence explanation of a generative AI model:**Â _a generative model is a function that can take a structured collection of symbols as input and produce a related structured collection of symbols as output_.  

â­ï¸ ä½†å¦‚æœä½ æ²¡æœ‰æ—¶é—´è¯»è¿™äº›ä¸œè¥¿ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªå…³äºç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æ¨¡å‹çš„ä¸€å¥è¯è§£é‡Šï¼šç”Ÿæˆæ€§æ¨¡å‹æ˜¯ä¸€ä¸ªå¯ä»¥å°†ç»“æ„åŒ–çš„ç¬¦å·é›†åˆä½œä¸ºè¾“å…¥å¹¶äº§ç”Ÿç›¸å…³ç»“æ„åŒ–çš„ç¬¦å·é›†åˆä½œä¸ºè¾“å‡ºçš„å‡½æ•°ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fbcf9a88e-e597-4868-b49c-f4fe6a08bebe_1000x543.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbcf9a88e-e597-4868-b49c-f4fe6a08bebe_1000x543.jpeg)

Here are some examples of structured collections of symbols:  

ä¸‹é¢æ˜¯ä¸€äº›ç¬¦å·ç»“æ„åŒ–é›†åˆçš„ä¾‹å­ã€‚

-   Letters in a word  
    
    ä¸€ä¸ªè¯ä¸­çš„å­—æ¯
    
-   Words in a sentence  
    
    å¥å­ä¸­çš„è¯è¯­
    
-   Pixels in an image  
    
    å›¾åƒä¸­çš„åƒç´ 
    
-   Frames in a video  
    
    è§†é¢‘ä¸­çš„å¸§æ•°
    

Now, there are plenty of ways to turn one collection of symbols into another, related collection of symbols â€” you donâ€™t even need computers to do this. Or, you can write a computer program that uses rules and lookup tables, like a certainÂ [famous chatbot from the 60s](https://return.life/2023/02/please-stop-talking-about-the-eliza-chatbot/)Â that Iâ€™m sick of hearing about.  

ç°åœ¨ï¼Œæœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥æŠŠä¸€ä¸ªç¬¦å·é›†åˆå˜æˆå¦ä¸€ä¸ªç›¸å…³çš„ç¬¦å·é›†åˆ--ä½ ç”šè‡³ä¸éœ€è¦è®¡ç®—æœºæ¥åšè¿™ä¸ªã€‚æˆ–è€…ï¼Œä½ å¯ä»¥å†™ä¸€ä¸ªä½¿ç”¨è§„åˆ™å’ŒæŸ¥æ‰¾è¡¨çš„è®¡ç®—æœºç¨‹åºï¼Œå°±åƒæˆ‘å¬åŒäº†çš„60å¹´ä»£æŸä¸ªè‘—åçš„èŠå¤©æœºå™¨äººã€‚

ğŸ‘·â™‚ï¸ The hard and expensive part of the above one-sentence explanation â€” the part that weâ€™ve only recently hit on how to do using massive amounts of electricity and leading-edge computer chips â€” is hidden deep inside the word _related_.  

ğŸ‘·â™‚ï¸ ä¸Šè¿°ä¸€å¥è¯çš„è§£é‡Šä¸­å›°éš¾å’Œæ˜‚è´µçš„éƒ¨åˆ†--æˆ‘ä»¬æœ€è¿‘æ‰æ‰“åˆ°å¦‚ä½•ä½¿ç”¨å¤§é‡çš„ç”µåŠ›å’Œé¢†å…ˆçš„è®¡ç®—æœºèŠ¯ç‰‡æ¥åšçš„éƒ¨åˆ†--æ·±è—åœ¨ç›¸å…³è¿™ä¸ªè¯é‡Œé¢ã€‚

Before we get into relationships, you need two concepts that will come up again and again throughout this piece:  

åœ¨æˆ‘ä»¬è®¨è®ºå…³ç³»ä¹‹å‰ï¼Œä½ éœ€è¦ä¸¤ä¸ªæ¦‚å¿µï¼Œè¿™ä¸¤ä¸ªæ¦‚å¿µå°†åœ¨æœ¬ç¯‡æ–‡ç« ä¸­åå¤å‡ºç°ã€‚

-   **âš™ï¸ Deterministic:** A deterministic process is a process that, given a particular input, will always produce the same output.  
    
    âš™ï¸ ç¡®å®šæ€§çš„ã€‚ç¡®å®šæ€§è¿‡ç¨‹æ˜¯ä¸€ä¸ªè¿‡ç¨‹ï¼Œç»™å®šä¸€ä¸ªç‰¹å®šçš„è¾“å…¥ï¼Œå°†æ€»æ˜¯äº§ç”Ÿç›¸åŒçš„è¾“å‡ºã€‚
    
-   **ğŸ² Stochastic:** A stochastic process is a process that, given a particular input, is more likely to produce some outputs and less likely to produce others.  
    
    ğŸ² éšæœºçš„ã€‚éšæœºè¿‡ç¨‹æ˜¯ä¸€ä¸ªè¿‡ç¨‹ï¼Œåœ¨ç‰¹å®šçš„è¾“å…¥ä¸‹ï¼Œæ›´æœ‰å¯èƒ½äº§ç”Ÿä¸€äº›è¾“å‡ºï¼Œè€Œä¸å¯èƒ½äº§ç”Ÿå…¶ä»–è¾“å‡ºã€‚
    

A classic gumball machine is deterministic â€” if I insert a quarter and turn the crank, I get a single gumball every time. So one quarter == one gumball, always.  

ç»å…¸çš„å£é¦™ç³–æœºæ˜¯ç¡®å®šçš„--å¦‚æœæˆ‘æ’å…¥ä¸€ä¸ª25ç¾åˆ†å¹¶è½¬åŠ¨æ›²æŸ„ï¼Œæˆ‘æ¯æ¬¡éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªå£é¦™ç³–ã€‚å› æ­¤ï¼Œä¸€ä¸ª25ç¾åˆ†==ä¸€ä¸ªå£é¦™ç³–ï¼Œæ€»æ˜¯å¦‚æ­¤ã€‚

But a classic gumball machine isÂ _also_Â stochastic â€” if I put in a quarter and turn the crank, the color of the gumball Iâ€™ll get is fundamentally random. Furthermore, the odds of getting one color or another depends on the ratios of different colors inside the machine. Five different gumball machines with five different gumball color ratios will have five different probability distributions for gumball color output.  

ä½†æ˜¯ï¼Œç»å…¸çš„å£é¦™ç³–æœºä¹Ÿæ˜¯ä¸å¯æ€è®®çš„--å¦‚æœæˆ‘æŠ•å…¥25ç¾åˆ†å¹¶è½¬åŠ¨æ›²æŸ„ï¼Œæˆ‘å°†å¾—åˆ°çš„å£é¦™ç³–çš„é¢œè‰²ä»æ ¹æœ¬ä¸Šè¯´æ˜¯éšæœºçš„ã€‚æ­¤å¤–ï¼Œå¾—åˆ°ä¸€ç§é¢œè‰²æˆ–å¦ä¸€ç§é¢œè‰²çš„æ¦‚ç‡å–å†³äºæœºå™¨å†…ä¸åŒé¢œè‰²çš„æ¯”ä¾‹ã€‚äº”ä¸ªä¸åŒçš„å£é¦™ç³–æœºå™¨ï¼Œæœ‰äº”ä¸ªä¸åŒçš„å£é¦™ç³–é¢œè‰²æ¯”ä¾‹ï¼Œä¼šæœ‰äº”ä¸ªä¸åŒçš„å£é¦™ç³–é¢œè‰²è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒã€‚

Now, with these key concepts out of the way, back to the reason why relationships can be hard.  

ç°åœ¨ï¼Œéšç€è¿™äº›å…³é”®æ¦‚å¿µçš„æå‡ºï¼Œå›åˆ°äººé™…å…³ç³»å¯èƒ½æ˜¯å›°éš¾çš„åŸå› ã€‚

Collections of symbols can be related in different ways, and the more abstract and subtle the relationship, the more technology weâ€™ll need to throw at the problem of capturing that relationship in a way thatâ€™s useful for knowledge work.  

ç¬¦å·é›†åˆå¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼è”ç³»åœ¨ä¸€èµ·ï¼Œå…³ç³»è¶Šæ˜¯æŠ½è±¡å’Œå¾®å¦™ï¼Œæˆ‘ä»¬å°±éœ€è¦æ›´å¤šçš„æŠ€æœ¯æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»¥ä¸€ç§å¯¹çŸ¥è¯†å·¥ä½œæœ‰ç”¨çš„æ–¹å¼æ¥æ•æ‰è¿™ç§å…³ç³»ã€‚

1.  If Iâ€™m relating the collectionsÂ `{cat}`Â andÂ `{at-cay}`, thatâ€™s a standard â€œ[Pig Latin](https://en.wikipedia.org/wiki/Pig_Latin)â€ transformation I can manage with a simple, handwritten rule set.  
    
    å¦‚æœæˆ‘æŠŠé›†åˆ `{cat}` å’Œ `{at-cay}` è”ç³»èµ·æ¥ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ "çŒªæ‹‰ä¸ "è½¬æ¢ï¼Œæˆ‘å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„ã€æ‰‹å†™çš„è§„åˆ™é›†æ¥å¤„ç†ã€‚
    
2.  If Iâ€™m relatingÂ `{cat}`Â toÂ `{dog}`, then there are many levels of abstraction on which those two collections can relate.  
    
    å¦‚æœæˆ‘æŠŠ `{cat}` å’Œ `{dog}` è”ç³»èµ·æ¥ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªé›†åˆå¯ä»¥åœ¨å¾ˆå¤šæŠ½è±¡å±‚æ¬¡ä¸Šå‘ç”Ÿå…³ç³»ã€‚
    
    1.  As ordered symbol collections (sequences), they both have three symbols.  
        
        ä½œä¸ºæœ‰åºçš„ç¬¦å·é›†åˆï¼ˆåºåˆ—ï¼‰ï¼Œå®ƒä»¬éƒ½æœ‰ä¸‰ä¸ªç¬¦å·ã€‚
        
    2.  As three-symbol sequences, theyâ€™re both words.  
        
        ä½œä¸ºä¸‰ä¸ªç¬¦å·çš„åºåˆ—ï¼Œå®ƒä»¬éƒ½æ˜¯å•è¯ã€‚
        
    3.  As words, they both refer to biological organisms.  
        
        ä½œä¸ºå•è¯ï¼Œå®ƒä»¬éƒ½æ˜¯æŒ‡ç”Ÿç‰©æœ‰æœºä½“ã€‚
        
    4.  As organisms, theyâ€™re both mammals.  
        
        ä½œä¸ºç”Ÿç‰©ä½“ï¼Œå®ƒä»¬éƒ½æ˜¯å“ºä¹³åŠ¨ç‰©ã€‚
        
    5.  As mammals, theyâ€™re both house pets. And so on.  
        
        ä½œä¸ºå“ºä¹³åŠ¨ç‰©ï¼Œå®ƒä»¬éƒ½æ˜¯å®¶åº­å® ç‰©ã€‚ä»¥æ­¤ç±»æ¨ã€‚
        
3.  If Iâ€™m relatingÂ `{the cat is alive}`Â toÂ `{the cat is dead}`, then thereâ€™s an even larger number of even more higher-order concepts that we can use to compare and contrast those two sequences of symbols.  
    
    å¦‚æœæˆ‘æŠŠ `{the cat is alive}` å’Œ `{the cat is dead}` è”ç³»èµ·æ¥ï¼Œé‚£ä¹ˆå°±ä¼šæœ‰æ›´å¤šæ›´é«˜é˜¶çš„æ¦‚å¿µï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¿™äº›æ¦‚å¿µæ¥æ¯”è¾ƒå’Œå¯¹æ¯”è¿™ä¸¤ä¸ªç¬¦å·åºåˆ—ã€‚
    
    1.  All of the concepts related toÂ `cat`Â are in play, as are concepts related toÂ â€œaliveâ€Â vs.Â â€œdead.â€  
        
        æ‰€æœ‰ä¸ `cat` æœ‰å…³çš„æ¦‚å¿µéƒ½åœ¨å‘æŒ¥ä½œç”¨ï¼Œä¸ "æ´»ç€ "ä¸ "æ­»äº† "æœ‰å…³çš„æ¦‚å¿µä¹Ÿæ˜¯å¦‚æ­¤ã€‚
        
    2.  On yet another level, many readers will spot what we might call an [intertextual](https://en.wikipedia.org/wiki/Intertextuality) reference to [SchrÃ¶dingerâ€™s Cat](https://en.wikipedia.org/wiki/Schr%C3%B6dinger%27s_cat).  
        
        åœ¨å¦ä¸€ä¸ªå±‚é¢ä¸Šï¼Œè®¸å¤šè¯»è€…ä¼šå‘ç°æˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸ºå¯¹ã€Šè–›å®šè°”çš„çŒ«ã€‹çš„äº’æ–‡å‚è€ƒã€‚
        
4.  Letâ€™s add one more relationship, just for fun:Â `{the cat is immature}`Â vs.Â `{the cat is mature}`. But are we talking about a stage of physical development or a state of emotional development?  
    
    è®©æˆ‘ä»¬å†å¢åŠ ä¸€ç§å…³ç³»ï¼Œåªæ˜¯ä¸ºäº†å¥½ç©ã€‚ `{the cat is immature}` å¯¹ `{the cat is mature}` ã€‚ä½†æˆ‘ä»¬è°ˆè®ºçš„æ˜¯èº«ä½“å‘å±•çš„ä¸€ä¸ªé˜¶æ®µè¿˜æ˜¯æƒ…æ„Ÿå‘å±•çš„ä¸€ä¸ªçŠ¶æ€ï¼Ÿ
    
    1.  Since itâ€™s a cat, â€œimmatureâ€ is probably a straightforward synonym for â€œyoungâ€ or â€œjuvenile.â€  
        
        æ—¢ç„¶æ˜¯çŒ«ï¼Œ"æœªæˆç†Ÿ "å¯èƒ½æ˜¯ "å¹´è½» "æˆ– "å°‘å¹´ "çš„ç›´æˆªäº†å½“çš„åŒä¹‰è¯ã€‚
        
    2.  If the subject of the sentence were a human, the sentence would more likely be invoking a cluster of concepts around age-appropriate behavior.Â   
        
        å¦‚æœè¿™å¥è¯çš„ä¸»ä½“æ˜¯äººï¼Œé‚£ä¹ˆè¿™å¥è¯æ›´æœ‰å¯èƒ½æ˜¯åœ¨å¼•ç”¨å›´ç»•é€‚é¾„è¡Œä¸ºçš„ä¸€ç»„æ¦‚å¿µã€‚
        

ğŸ“ˆ As you read through the list of items above, you can imagine that as you go up in number from one to four thereâ€™s an explosion of possible relationships between the symbols. And as the number of possible relationships increases, the qualities of the relationships themselves increase in abstraction, complexity, and subtlety.  

å½“ä½ é˜…è¯»ä¸Šé¢çš„é¡¹ç›®åˆ—è¡¨æ—¶ï¼Œä½ å¯ä»¥æƒ³è±¡ï¼Œå½“ä½ ä»1åˆ°4çš„æ•°å­—å¢åŠ æ—¶ï¼Œç¬¦å·ä¹‹é—´å¯èƒ½çš„å…³ç³»å°±ä¼šçˆ†ç‚¸ã€‚éšç€å¯èƒ½çš„å…³ç³»æ•°é‡çš„å¢åŠ ï¼Œå…³ç³»æœ¬èº«çš„è´¨é‡ä¹Ÿåœ¨æŠ½è±¡æ€§ã€å¤æ‚æ€§å’Œå¾®å¦™æ€§æ–¹é¢å¢åŠ ã€‚

The different relationships above take different classes of symbol storage and retrieval â€” from paper notebooks up to datacenters â€” to capture and encode in a useful way.Â   

ä¸Šè¿°ä¸åŒçš„å…³ç³»éœ€è¦ä¸åŒç­‰çº§çš„ç¬¦å·å­˜å‚¨å’Œæ£€ç´¢--ä»çº¸è´¨ç¬”è®°æœ¬åˆ°æ•°æ®ä¸­å¿ƒ--ä»¥ä¸€ç§æœ‰ç”¨çš„æ–¹å¼æ¥æ•æ‰å’Œç¼–ç ã€‚

That first, simplistic Pig Latin relationship can be described on a single sheet of paper, such that anyone with that paper can transform any word from English into Pig Latin. But by the time we get to example four, a strange thing has happened, and that strange thing is why machine learning requires tens of millions of dollars worth of resources:Â   

è¿™ç¬¬ä¸€ä¸ªç®€å•çš„çŒªæ‹‰ä¸å…³ç³»å¯ä»¥åœ¨ä¸€å¼ çº¸ä¸Šæè¿°å‡ºæ¥ï¼Œè¿™æ ·ï¼Œä»»ä½•äººæ‹¿ç€è¿™å¼ çº¸éƒ½å¯ä»¥æŠŠè‹±è¯­ä¸­çš„ä»»ä½•ä¸€ä¸ªè¯è½¬åŒ–ä¸ºçŒªæ‹‰ä¸ã€‚ä½†åˆ°äº†ä¾‹å­å››ï¼Œå°±å‘ç”Ÿäº†ä¸€ä»¶å¥‡æ€ªçš„äº‹æƒ…ï¼Œè€Œè¿™ä»¶å¥‡æ€ªçš„äº‹æƒ…å°±æ˜¯ä¸ºä»€ä¹ˆæœºå™¨å­¦ä¹ éœ€è¦ä»·å€¼æ•°åƒä¸‡ç¾å…ƒçš„èµ„æºã€‚

1.  Weâ€™ve uncovered a smallÂ **universe of possible relationships**Â between those collections. Thereâ€™s just a bewildering, densely connected network of concepts, all the way up the abstraction ladder from simple physical characteristics, to biological taxonomies, to subtle notions of physical and emotional development.  
    
    æˆ‘ä»¬å·²ç»å‘ç°äº†è¿™äº›é›†åˆä¹‹é—´å¯èƒ½çš„å…³ç³»çš„ä¸€ä¸ªå°å®‡å®™ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªä»¤äººå›°æƒ‘çš„ã€å¯†å¯†éº»éº»çš„æ¦‚å¿µç½‘ç»œï¼Œä»ç®€å•çš„ç‰©ç†ç‰¹å¾åˆ°ç”Ÿç‰©åˆ†ç±»å­¦ï¼Œå†åˆ°èº«ä½“å’Œæƒ…æ„Ÿå‘å±•çš„å¾®å¦™æ¦‚å¿µï¼Œæ‰€æœ‰çš„æŠ½è±¡é˜¶æ¢¯ã€‚
    
2.  Some of the more abstract possible relationships are more likely to be in play than others. So an element ofÂ **probability has entered the picture**.  
    
    ä¸€äº›æ›´æŠ½è±¡çš„å¯èƒ½å…³ç³»æ¯”å…¶ä»–å…³ç³»æ›´æœ‰å¯èƒ½å‘ç”Ÿä½œç”¨ã€‚å› æ­¤ï¼Œæ¦‚ç‡çš„å› ç´ å·²ç»è¿›å…¥ç”»é¢ã€‚
    
    1.  As I said in the example, given that weâ€™re talking about a cat, itâ€™sÂ _more likely_Â that the mature/immature dichotomy is related to a cluster of concepts around physical development andÂ _less likely_Â that itâ€™s related to a cluster of concepts around emotional or intellectual development.  
        
        æ­£å¦‚æˆ‘åœ¨ä¾‹å­ä¸­æ‰€è¯´ï¼Œé‰´äºæˆ‘ä»¬æ­£åœ¨è°ˆè®ºä¸€åªçŒ«ï¼Œæˆç†Ÿ/ä¸æˆç†Ÿçš„äºŒåˆ†æ³•æ›´æœ‰å¯èƒ½ä¸å›´ç»•èº«ä½“å‘å±•çš„æ¦‚å¿µç¾¤æœ‰å…³ï¼Œè€Œä¸å¯èƒ½ä¸å›´ç»•æƒ…æ„Ÿæˆ–æ™ºåŠ›å‘å±•çš„æ¦‚å¿µç¾¤æœ‰å…³ã€‚
        

Regarding the probabilities in #2 above, â€œless likelyâ€ does not mean impossible, especially if we open the door to a bit of extra context. What if we added a few additional words, and the collection pair was:Â   

å…³äºä¸Šé¢ç¬¬2æ¡ä¸­çš„æ¦‚ç‡ï¼Œ"ä¸å¤ªå¯èƒ½ "å¹¶ä¸æ„å‘³ç€ä¸å¯èƒ½ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæˆ‘ä»¬æ‰“å¼€é—¨æ¥çœ‹çœ‹é¢å¤–çš„èƒŒæ™¯ã€‚å¦‚æœæˆ‘ä»¬å¢åŠ ä¸€äº›é¢å¤–çš„è¯è¯­ï¼Œè€Œé›†åˆå¯¹æ˜¯ã€‚

-   `{Regarding the cat in the hat: the cat is mature.}`
    
-   `{Regarding the cat in the hat: the cat is immature.}`
    

ğŸŒˆ Suddenly, all the probabilities have shifted and weâ€™re in a whole other realm of likely meaning with respect to maturity and immaturity.  

ğŸŒˆçªç„¶é—´ï¼Œæ‰€æœ‰çš„æ¦‚ç‡éƒ½å‘ç”Ÿäº†å˜åŒ–ï¼Œåœ¨æˆç†Ÿå’Œä¸æˆç†Ÿæ–¹é¢ï¼Œæˆ‘ä»¬å¤„äºä¸€ä¸ªå®Œå…¨ä¸åŒçš„å¯èƒ½æ„ä¹‰çš„é¢†åŸŸã€‚

**Summary:  

æ‘˜è¦ã€‚**

-   When the relationships among collections of symbols areÂ **simple and deterministic**, you donâ€™t need much storage or computing power to relate one collection to the other.  
    
    å½“ç¬¦å·é›†åˆä¹‹é—´çš„å…³ç³»æ˜¯ç®€å•çš„å’Œç¡®å®šçš„ï¼Œä½ ä¸éœ€è¦å¤ªå¤šçš„å­˜å‚¨æˆ–è®¡ç®—èƒ½åŠ›æ¥å°†ä¸€ä¸ªé›†åˆä¸å¦ä¸€ä¸ªé›†åˆè”ç³»èµ·æ¥ã€‚
    
-   When the relationships among collections of symbolsÂ are **complex and stochastic**, then throwing more storage and computing power at the problem of relating one collection to another enables you to relate those collections in ever richer and more complex ways.  
    
    å½“ç¬¦å·é›†åˆä¹‹é—´çš„å…³ç³»æ˜¯å¤æ‚å’Œéšæœºçš„ï¼Œé‚£ä¹ˆåœ¨ä¸€ä¸ªé›†åˆä¸å¦ä¸€ä¸ªé›†åˆçš„å…³ç³»é—®é¢˜ä¸ŠæŠ•å…¥æ›´å¤šçš„å­˜å‚¨å’Œè®¡ç®—èƒ½åŠ›ï¼Œä½¿ä½ èƒ½å¤Ÿä»¥æ›´ä¸°å¯Œå’Œæ›´å¤æ‚çš„æ–¹å¼æ¥è”ç³»è¿™äº›é›†åˆã€‚
    

If you managed to get through high school chemistry, then you have a set of concepts that are useful for thinking about generative AI: atomic orbitals.  

å¦‚æœä½ è®¾æ³•é€šè¿‡äº†é«˜ä¸­åŒ–å­¦ï¼Œé‚£ä¹ˆä½ æœ‰ä¸€å¥—å¯¹æ€è€ƒç”Ÿæˆæ€§äººå·¥æ™ºèƒ½æœ‰ç”¨çš„æ¦‚å¿µï¼šåŸå­è½¨é“ã€‚

ğŸ˜³ And if you didnâ€™t get through basic chemistry, or you donâ€™t remember any of it, then donâ€™t worry. **This section is mostly about the pictures**, so look at the pictures and scan for the boldfaced words, then move on.  

ğŸ˜³ å¦‚æœä½ æ²¡æœ‰é€šè¿‡åŸºç¡€åŒ–å­¦ï¼Œæˆ–è€…ä½ ä¸è®°å¾—ä»»ä½•ä¸œè¥¿ï¼Œé‚£ä¹ˆä¸è¦æ‹…å¿ƒã€‚è¿™ä¸€éƒ¨åˆ†ä¸»è¦æ˜¯å…³äºå›¾ç‰‡çš„ï¼Œæ‰€ä»¥çœ‹ä¸€ä¸‹å›¾ç‰‡ï¼Œæ‰«æä¸€ä¸‹é»‘ä½“å­—ï¼Œç„¶åç»§ç»­å‰è¿›ã€‚

We all learn in basic chemistry that orbitals are just regions of space where an electron is likely to be at any moment. Electrons at different energy levels have orbitals that are shaped differently, which means weâ€™re likely to find them in different regions.Â   

æˆ‘ä»¬éƒ½åœ¨åŸºç¡€åŒ–å­¦ä¸­äº†è§£åˆ°ï¼Œè½¨é“åªæ˜¯ä¸€ä¸ªç”µå­åœ¨ä»»ä½•æ—¶å€™éƒ½å¯èƒ½å‡ºç°çš„ç©ºé—´åŒºåŸŸã€‚ä¸åŒèƒ½çº§çš„ç”µå­æœ‰ä¸åŒå½¢çŠ¶çš„è½¨é“ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æœ‰å¯èƒ½åœ¨ä¸åŒçš„åŒºåŸŸæ‰¾åˆ°å®ƒä»¬ã€‚

Take a look at hydrogen:  

çœ‹ä¸€çœ‹æ°¢æ°”ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fb2d67d9f-39bc-4963-a26d-fe3178d6105c_1100x1000.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d67d9f-39bc-4963-a26d-fe3178d6105c_1100x1000.jpeg)

Letâ€™s zoom in on one of the orbitals:  

è®©æˆ‘ä»¬æ”¾å¤§å…¶ä¸­ä¸€ä¸ªè½¨é“ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F67053e39-216f-4585-a5a4-f8a10dfa9df4_600x600.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67053e39-216f-4585-a5a4-f8a10dfa9df4_600x600.jpeg)

For the orbital above, the brighter the region the higher the odds youâ€™d bump into an electron there if you were to poke at the atom with something even tinier than an electron. For the black regions in the picture, that doesnâ€™t mean the probability of finding an electron is zero â€” it just means itâ€™s so low as to practically be zero.  

å¯¹äºä¸Šé¢çš„è½¨é“æ¥è¯´ï¼Œå¦‚æœä½ ç”¨æ¯”ç”µå­è¿˜å°çš„ä¸œè¥¿å»æˆ³åŸå­ï¼Œè¶Šæ˜¯æ˜äº®çš„åŒºåŸŸï¼Œä½ åœ¨é‚£é‡Œç¢°åˆ°ç”µå­çš„å‡ ç‡å°±è¶Šå¤§ã€‚å¯¹äºå›¾ç‰‡ä¸­çš„é»‘è‰²åŒºåŸŸï¼Œè¿™å¹¶ä¸æ„å‘³ç€æ‰¾åˆ°ä¸€ä¸ªç”µå­çš„æ¦‚ç‡ä¸ºé›¶--å®ƒåªæ˜¯æ„å‘³ç€å®ƒä½åˆ°å‡ ä¹ä¸ºé›¶ã€‚

Those orbitals are **probability distributions** and they have a certain shape to them â€” the one above has **four**Â **lobes**, so that if you observe a point in one of those four areas youâ€™re more likely to spot an electron than if you observe one of the black regions. The other orbitals have different lobes that are arranged in different ways.  

è¿™äº›è½¨é“æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œå®ƒä»¬æœ‰ä¸€å®šçš„å½¢çŠ¶--ä¸Šé¢é‚£ä¸ªæœ‰å››ä¸ªè£‚ç‰‡ï¼Œæ‰€ä»¥å¦‚æœä½ è§‚å¯Ÿè¿™å››ä¸ªåŒºåŸŸä¸­çš„ä¸€ä¸ªç‚¹ï¼Œä½ æ¯”è§‚å¯Ÿé»‘è‰²åŒºåŸŸä¸­çš„ä¸€ä¸ªæ›´æœ‰å¯èƒ½å‘ç°ä¸€ä¸ªç”µå­ã€‚å…¶ä»–çš„è½¨é“æœ‰ä¸åŒçš„è£‚ç‰‡ï¼Œä»¥ä¸åŒçš„æ–¹å¼æ’åˆ—ã€‚

ğŸ¥³ Ok, you made it! Thatâ€™s all the quantum chemistry you need, and in fact, itâ€™s all the background you need at the moment. We can now talk about the thing everything is trying to understand: ChatGPT.  

ğŸ¥³ å¥½äº†ï¼Œä½ æˆåŠŸäº†ï¼è¿™å°±æ˜¯ä½ éœ€è¦çš„æ‰€æœ‰é‡å­åŒ–å­¦çŸ¥è¯†ã€‚è¿™å°±æ˜¯ä½ éœ€è¦çš„æ‰€æœ‰é‡å­åŒ–å­¦çŸ¥è¯†ï¼Œäº‹å®ä¸Šï¼Œè¿™ä¹Ÿæ˜¯ä½ ç›®å‰éœ€è¦çš„æ‰€æœ‰èƒŒæ™¯ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥è°ˆä¸€è°ˆæ‰€æœ‰ä¸œè¥¿éƒ½æƒ³äº†è§£çš„äº‹æƒ…äº†ã€‚ChatGPTã€‚

You can imagine that for a model like ChatGPT, each possible blob of text the model could generate â€” everything from a few words of gibberish to a full page of coherent English â€” is a single point in a probability distribution, like electron positions in the atomic orbitals of the previous sectionâ€™s hydrogen atom.Â   

ä½ å¯ä»¥æƒ³è±¡ï¼Œå¯¹äºåƒChatGPTè¿™æ ·çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„æ¯ä¸€ä¸ªå¯èƒ½çš„æ–‡æœ¬åœ†çƒ--ä»å‡ ä¸ªå­—çš„èƒ¡è¨€ä¹±è¯­åˆ°ä¸€æ•´é¡µè¿è´¯çš„è‹±è¯­--éƒ½æ˜¯æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªå•ç‚¹ï¼Œå°±åƒä¸Šä¸€èŠ‚æ°¢åŸå­çš„åŸå­è½¨é“ä¸­çš„ç”µå­ä½ç½®ã€‚

When you feed ChatGPTâ€™s input box a collection of words â€” e.g.Â `{Tell me about the state of a cat in a box with a flask of poison and a bit of radioactive material}`Â â€” you can think of that action of clicking the â€œSubmitâ€ button as making an observation that collapses a wave function and results in an observation of a single collection of symbols (out of many possible collections).  

å½“ä½ ç»™ChatGPTçš„è¾“å…¥æ¡†è¾“å…¥ä¸€ä¸ªè¯çš„é›†åˆ--ä¾‹å¦‚ `{Tell me about the state of a cat in a box with a flask of poison and a bit of radioactive material}` --ä½ å¯ä»¥æŠŠç‚¹å‡» "æäº¤ "æŒ‰é’®çš„åŠ¨ä½œçœ‹ä½œæ˜¯åœ¨è¿›è¡Œè§‚å¯Ÿï¼Œè¿™ä¸ªè§‚å¯Ÿä½¿æ³¢å‡½æ•°æŠ˜å ï¼Œç»“æœæ˜¯å¯¹ä¸€ä¸ªå•ä¸€çš„ç¬¦å·é›†åˆï¼ˆåœ¨è®¸å¤šå¯èƒ½çš„é›†åˆä¸­ï¼‰è¿›è¡Œè§‚å¯Ÿã€‚

ğŸ§  **Note for more informed readers:** Some of you who read the above will be aware that a text-to-text LLM is actually locating single words in the probability space and successively stringing them together to build up sentences. The distinction between â€œlatent space is the multidimensional space of all likely words the model might outputâ€ vs. â€œlatent space is the multidimensional space of all likely word sequences the model might outputâ€ is pretty academic at this level of abstraction, though. So for the purpose of downloading better intuitions into readers and minimizing complexity, Iâ€™m going with the second option.  

ğŸ§  ç»™æ›´å¤šçŸ¥æƒ…çš„è¯»è€…çš„è¯´æ˜ã€‚æœ‰äº›äººçœ‹äº†ä¸Šé¢çš„å†…å®¹å°±ä¼šçŸ¥é“ï¼Œæ–‡æœ¬åˆ°æ–‡æœ¬çš„LLMå®é™…ä¸Šæ˜¯åœ¨æ¦‚ç‡ç©ºé—´ä¸­å®šä½å•å­—ï¼Œå¹¶ä¾æ¬¡å°†å®ƒä»¬ä¸²è”èµ·æ¥ä»¥å»ºç«‹å¥å­ã€‚ä¸è¿‡ï¼Œ"æ½œç©ºé—´æ˜¯æ¨¡å‹å¯èƒ½è¾“å‡ºçš„æ‰€æœ‰å¯èƒ½çš„å•è¯çš„å¤šç»´ç©ºé—´ "ä¸ "æ½œç©ºé—´æ˜¯æ¨¡å‹å¯èƒ½è¾“å‡ºçš„æ‰€æœ‰å¯èƒ½çš„å•è¯åºåˆ—çš„å¤šç»´ç©ºé—´ "ä¹‹é—´çš„åŒºåˆ«ï¼Œåœ¨è¿™ä¸ªæŠ½è±¡çš„å±‚é¢ä¸Šæ˜¯ç›¸å½“å­¦æœ¯æ€§çš„ã€‚å› æ­¤ï¼Œä¸ºäº†å‘è¯»è€…ä¸‹è½½æ›´å¥½çš„ç›´è§‰ï¼Œå¹¶å°†å¤æ‚æ€§é™åˆ°æœ€ä½ï¼Œæˆ‘é€‰æ‹©äº†ç¬¬äºŒç§æ–¹æ¡ˆã€‚

ğŸ² Sometimes, your text prompt input will take you to a point in the probability distribution that corresponds to a collection like,Â `{The cat is alive}`, and at other times youâ€™ll end up at a point that corresponds toÂ `{The cat is dead}`.  

ğŸ² æœ‰æ—¶ï¼Œä½ çš„æ–‡æœ¬æç¤ºè¾“å…¥ä¼šæŠŠä½ å¸¦åˆ°æ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªç‚¹ï¼Œå¯¹åº”äºä¸€ä¸ªé›†åˆï¼Œå¦‚ `{The cat is alive}` ï¼Œè€Œåœ¨å…¶ä»–æ—¶å€™ï¼Œä½ ä¼šåœ¨ä¸€ä¸ªç‚¹ä¸Šç»“æŸï¼Œå¯¹åº”äº `{The cat is dead}` ã€‚

Note that it isÂ _possible_, though not at all likely, that the aforementioned input symbols could take you to the point in the modelâ€™sÂ **latent space**Â corresponding to the following collection:Â `{ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn}`. It all depends on the shape of the probability distributions youâ€™re poking at with your text inputs and on the dice that the universe â€” or, rather, the computerâ€™s random number generator â€” is rolling.  

è¯·æ³¨æ„ï¼Œä¸Šè¿°çš„è¾“å…¥ç¬¦å·æœ‰å¯èƒ½ï¼Œå°½ç®¡ä¸å¤ªå¯èƒ½ï¼ŒæŠŠä½ å¸¦åˆ°æ¨¡å‹çš„latent spaceä¸­å¯¹åº”äºä»¥ä¸‹é›†åˆçš„ç‚¹ã€‚è¿™ä¸€åˆ‡éƒ½å–å†³äºä½ ç”¨æ–‡å­—è¾“å…¥æˆ³åˆ°çš„æ¦‚ç‡åˆ†å¸ƒçš„å½¢çŠ¶ï¼Œä»¥åŠå®‡å®™--æˆ–è€…è¯´ï¼Œè®¡ç®—æœºçš„éšæœºæ•°å‘ç”Ÿå™¨--æ­£åœ¨æ»šåŠ¨çš„éª°å­ã€‚

**ğŸ‘‰ Important implication:** Itâ€™s common but not necessarily always useful to say the language model â€œknowsâ€ the state of the cat in this example â€” i.e., whether itâ€™s alive or dead. Itâ€™s suboptimal to lean too hard on the idea that the model has a kind of internal, true/false orientation to the cat and different claims about its circumstances.Â   

ğŸ‘‰é‡è¦çš„å«ä¹‰ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¯´è¯­è¨€æ¨¡å‹ "çŸ¥é“ "çŒ«çš„çŠ¶æ€--å³å®ƒæ˜¯æ´»çš„è¿˜æ˜¯æ­»çš„ï¼Œè¿™å¾ˆå¸¸è§ï¼Œä½†ä¸ä¸€å®šæ€»æ˜¯æœ‰ç”¨ã€‚è¿‡åˆ†å€šé‡æ¨¡å‹å¯¹çŒ«æœ‰ä¸€ç§å†…éƒ¨çš„ã€çœŸ/å‡çš„å®šä½ï¼Œä»¥åŠå¯¹çŒ«çš„æƒ…å†µæœ‰ä¸åŒçš„ä¸»å¼ ï¼Œæ˜¯ä¸å¯å–çš„ã€‚

Instead, itâ€™s better to think of it like this:  

ç›¸åï¼Œæœ€å¥½è¿™æ ·æƒ³ã€‚

â¡ In the space of all the possible collections of symbols the model could produce â€” from collections that are snippets of gibberish to collections that are pages of Shakespeare â€” _there are regions in the modelâ€™s probability distributions that contain collections of symbols we humans interpret to mean that the cat is alive_. And there are also adjacent regions in that probability space containing collections of symbols we interpret to mean the cat is dead.  

â¡ åœ¨è¯¥æ¨¡å‹å¯èƒ½äº§ç”Ÿçš„æ‰€æœ‰ç¬¦å·é›†åˆçš„ç©ºé—´ä¸­--ä»èƒ¡è¨€ä¹±è¯­çš„é›†åˆåˆ°èå£«æ¯”äºšçš„é›†åˆ--åœ¨è¯¥æ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒä¸­ï¼Œæœ‰ä¸€äº›åŒºåŸŸåŒ…å«äº†æˆ‘ä»¬äººç±»è§£é‡Šä¸ºçŒ«æ˜¯æ´»ç€çš„ç¬¦å·é›†åˆã€‚è€Œåœ¨è¿™ä¸ªæ¦‚ç‡ç©ºé—´ä¸­ä¹Ÿæœ‰ä¸€äº›ç›¸é‚»çš„åŒºåŸŸï¼ŒåŒ…å«äº†æˆ‘ä»¬è§£é‡Šä¸ºæ„å‘³ç€çŒ«å·²ç»æ­»äº¡çš„ç¬¦å·é›†åˆã€‚

ğŸˆ Here are some cat-related symbol collections we might encounter in ChatGPTâ€™s **latent space** â€” i.e., the space of possible outputs, which has been deliberately sculpted into a particular shape by an expensive training process:  

ä»¥ä¸‹æ˜¯æˆ‘ä»¬åœ¨ChatGPTçš„æ½œåœ¨ç©ºé—´ä¸­å¯èƒ½é‡åˆ°çš„ä¸€äº›ä¸çŒ«æœ‰å…³çš„ç¬¦å·é›†åˆ--å³å¯èƒ½è¾“å‡ºçš„ç©ºé—´ï¼Œå®ƒå·²ç»è¢«æ˜‚è´µçš„è®­ç»ƒè¿‡ç¨‹åˆ»æ„é›•ç¢æˆç‰¹å®šçš„å½¢çŠ¶ã€‚

-   `{The cat roused herself from slumber and blinked her eyes.}`
    
-   `{The soft breathing of the sleeping cat greeted SchrÃ¶dinger as he opened the box.}`
    
-   `{Ja mon, de cat him dead.}`
    
-   `{â€œIâ€™ve killed my favorite cat!â€ screamed SchrÃ¶dinger as he pulled his petâ€™s lifeless corpse from the box.}`
    
-   `{Patches watched the scene from above, his astral cat form floating near the ceiling as his master lifted his lifeless body from the box and wept.}`
    

When you poke at the model with different input collections, youâ€™re more likely to encounter some of these output collections than others, but theyâ€™re all at least theoretically possible to encounter.  

å½“ä½ ç”¨ä¸åŒçš„è¾“å…¥é›†åˆæ¢æµ‹æ¨¡å‹æ—¶ï¼Œä½ æ›´æœ‰å¯èƒ½é‡åˆ°å…¶ä¸­ä¸€äº›è¾“å‡ºé›†åˆï¼Œè€Œä¸æ˜¯å…¶ä»–ï¼Œä½†å®ƒä»¬è‡³å°‘åœ¨ç†è®ºä¸Šéƒ½æœ‰å¯èƒ½é‡åˆ°ã€‚

ğŸ‘ So when you and I are both interacting with ChatGPT around a topic thatâ€™s related to some set of facts in the world â€” e.g., the status of the Bengal tiger, and if itâ€™s endangered or not â€” donâ€™t imagine that weâ€™re both asking some **unitary entity with a personal history** and set of experiences to communicate to us some of the facts about Bengal tigers that it has tucked away in its memory, where if it tells you one thing and me the opposite then itâ€™s somehow lying to one of us.  

æ‰€ä»¥ï¼Œå½“ä½ å’Œæˆ‘éƒ½åœ¨ä¸ChatGPTäº’åŠ¨æ—¶ï¼Œå›´ç»•ç€ä¸€ä¸ªä¸ä¸–ç•Œä¸Šçš„ä¸€äº›äº‹å®ç›¸å…³çš„è¯é¢˜--ä¾‹å¦‚ï¼Œå­ŸåŠ æ‹‰è™çš„åœ°ä½ï¼Œä»¥åŠå®ƒæ˜¯å¦æ¿’ä¸´ç­ç»--ä¸è¦æƒ³è±¡æˆ‘ä»¬éƒ½åœ¨è¦æ±‚æŸä¸ªå…·æœ‰ä¸ªäººå†å²å’Œç»éªŒçš„ç»Ÿä¸€å®ä½“å‘æˆ‘ä»¬ä¼ è¾¾å®ƒè—åœ¨è®°å¿†ä¸­çš„ä¸€äº›å…³äºå­ŸåŠ æ‹‰è™çš„äº‹å®ï¼Œå¦‚æœå®ƒå‘Šè¯‰ä½ ä¸€ä»¶äº‹ï¼Œè€Œæˆ‘å‘Šè¯‰ä½ ç›¸åçš„äº‹ï¼Œé‚£ä¹ˆå®ƒå°±åœ¨æŸç§ç¨‹åº¦ä¸Šå¯¹æˆ‘ä»¬ä¸­çš„ä¸€ä¸ªäººæ’’è°ã€‚

ğŸ‘ Rather, think of our activity like this: I am using my text prompt to **observe a single point in a probability distribution** that corresponds (at least on my reading of the modelâ€™s output symbols) to a cluster of facts and concepts around Bengal tigers, and you are doing the same thing. When both of us are getting different sequences of words that seem (again, to us as humans who are interpreting this sequence) to represent a divergent fact pattern â€” e.g., it tells me the tiger is endangered, and it tells you the tiger is so prevalent that itâ€™s a nuisance animal â€” itâ€™s because weâ€™re poking at different lobes of a probability distribution and finding different points in those divergent lobes.Â   

æ°æ°ç›¸åï¼Œè¯·æŠŠæˆ‘ä»¬çš„æ´»åŠ¨æƒ³æˆè¿™æ ·ã€‚æˆ‘æ­£åœ¨ä½¿ç”¨æˆ‘çš„æ–‡æœ¬æç¤ºæ¥è§‚å¯Ÿæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸€ä¸ªç‚¹ï¼Œè¯¥ç‚¹å¯¹åº”äºï¼ˆè‡³å°‘åœ¨æˆ‘å¯¹æ¨¡å‹è¾“å‡ºç¬¦å·çš„é˜…è¯»ä¸­ï¼‰å›´ç»•å­ŸåŠ æ‹‰è™çš„ä¸€ç»„äº‹å®å’Œæ¦‚å¿µï¼Œè€Œä½ ä¹Ÿåœ¨åšåŒæ ·çš„äº‹æƒ…ã€‚å½“æˆ‘ä»¬ä¸¤ä¸ªäººéƒ½å¾—åˆ°ä¸åŒçš„è¯è¯­åºåˆ—ï¼Œä¼¼ä¹ï¼ˆåŒæ ·ï¼Œå¯¹äºè§£é‡Šè¿™ä¸ªåºåˆ—çš„äººç±»æ¥è¯´ï¼‰ä»£è¡¨äº†ä¸åŒçš„äº‹å®æ¨¡å¼--ä¾‹å¦‚ï¼Œå®ƒå‘Šè¯‰æˆ‘è€è™æ¿’ä¸´ç­ç»ï¼Œè€Œå®ƒå‘Šè¯‰ä½ è€è™éå¸¸æ™®éï¼Œæ˜¯ä¸€ç§è®¨åŒçš„åŠ¨ç‰©--è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åœ¨æ¢ç©¶ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„ä¸åŒè£‚ç‰‡ï¼Œå¹¶åœ¨è¿™äº›ä¸åŒçš„è£‚ç‰‡ä¸­æ‰¾åˆ°ä¸åŒçš„ç‚¹ã€‚

-   The lobe in the probability distribution that Iâ€™m poking at encompasses word sequences that generally correspond to what I, a human English speaker, interpret as claims about the **endangered status** of Bengal tigers.Â   
    
    æˆ‘æ‰€æ¢ç©¶çš„æ¦‚ç‡åˆ†å¸ƒä¸­çš„å¶å­åŒ…å«äº†é€šå¸¸ä¸æˆ‘ï¼ˆä¸€ä¸ªè®²è‹±è¯­çš„äººç±»ï¼‰è§£é‡Šä¸ºå…³äºå­ŸåŠ æ‹‰è™æ¿’å±åœ°ä½çš„è¯´æ³•ç›¸å¯¹åº”çš„è¯åºåˆ—ã€‚
    
-   The lobe youâ€™re poking at encompasses word sequences that generally correspond to what you interpret as claims about the **superabundance** of Bengal tigers.  
    
    ä½ æ‰€æˆ³çš„é‚£ä¸ªå¶å­åŒ…å«äº†ä¸€äº›è¯åºï¼Œè¿™äº›è¯åºé€šå¸¸å¯¹åº”äºä½ æ‰€ç†è§£çš„å…³äºå­ŸåŠ æ‹‰è™æ•°é‡è¿‡å¤šçš„è¯´æ³•ã€‚
    

So how do we fix this?  

é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿ

Given that the Bengal tiger is, in fact, endangered (or, at least this is what I take to be the state of the world based on the words that Googleâ€™s ML-powered machines are telling right now when I type in a search query), how do we eliminate or at least shrink the probability distribution lobe that youâ€™re poking at â€” the one that covers text blobs about Bengal tigers overrunning the cities, digging in peopleâ€™s garbage, and other nonsense?  

é‰´äºå­ŸåŠ æ‹‰è™äº‹å®ä¸Šå·²ç»æ¿’ä¸´ç­ç»ï¼ˆæˆ–è€…ï¼Œè‡³å°‘æˆ‘è®¤ä¸ºè¿™æ˜¯åŸºäºæˆ‘è¾“å…¥æœç´¢æŸ¥è¯¢æ—¶è°·æ­Œçš„MLæœºå™¨ç°åœ¨æ‰€è®²çš„è¯ï¼‰ï¼Œæˆ‘ä»¬å¦‚ä½•æ¶ˆé™¤æˆ–è‡³å°‘ç¼©å°ä½ æ‰€æˆ³çš„æ¦‚ç‡åˆ†å¸ƒå¶--æ¶µç›–å…³äºå­ŸåŠ æ‹‰è™éœ¸å åŸå¸‚ã€åœ¨äººä»¬çš„åƒåœ¾ä¸­æŒ–æ˜ä»¥åŠå…¶ä»–æ— ç¨½ä¹‹è°ˆçš„æ–‡å­—å—çš„é‚£ä¸ªï¼Ÿ

âœ‹ Not only is it not clear we can, but itâ€™s also not really clear weâ€™d always want to. Let me explain.  

âœ‹ä¸ä»…ä¸æ¸…æ¥šæˆ‘ä»¬å¯ä»¥ï¼Œè€Œä¸”ä¹Ÿä¸æ¸…æ¥šæˆ‘ä»¬æ˜¯å¦æ€»æ˜¯æƒ³è¿™æ ·åšã€‚è®©æˆ‘è§£é‡Šä¸€ä¸‹ã€‚

When an LLM spits out a word combination that an observer takes to misrepresent the state of reality, we say that the model is â€œhallucinating.â€  

å½“ä¸€ä¸ªLLMåå‡ºä¸€ä¸ªè§‚å¯Ÿè€…è®¤ä¸ºè¯¯å¯¼ç°å®çŠ¶æ€çš„è¯æ±‡ç»„åˆæ—¶ï¼Œæˆ‘ä»¬è¯´è¿™ä¸ªæ¨¡å‹æ˜¯ "å¹»è§‰"ã€‚

**ğŸ˜µğŸ’« Aside:**Â I gotta say, this â€œhallucinationâ€ term is pretty loaded. His hallucination is herÂ [mythopoesis](https://en.wikipedia.org/wiki/Mythopoeia)Â is my prophecy of things which must shortly come to pass (he who has ears to hear, let him hear). Wars have been fought over this stuff, and may be yet again if a famous collection of alleged hallucinations is interpreted as â€œtrueâ€ in some wayâ€¦ But letâ€™s not get ahead of ourselves by veering into hermeneutics just yet. Weâ€™ll pick up this thread again in a moment.  

ğŸ˜µğŸ’« Aside:æˆ‘å¾—è¯´ï¼Œè¿™ä¸ª "å¹»è§‰ "ä¸€è¯å¾ˆæœ‰åˆ†é‡ã€‚ä»–çš„å¹»è§‰å°±æ˜¯å¥¹çš„ç¥è¯ï¼Œæ˜¯æˆ‘å¯¹ä¸ä¹…åå¿…é¡»å®ç°çš„äº‹æƒ…çš„é¢„è¨€ï¼ˆæœ‰è€³å¯å¬çš„äººï¼Œè¯·å¬ï¼‰ã€‚æˆ˜äº‰å·²ç»ä¸ºè¿™äº›ä¸œè¥¿æ‰“è¿‡äº†ï¼Œè€Œä¸”å¦‚æœä¸€ä¸ªè‘—åçš„æ‰€è°“å¹»è§‰é›†è¢«è§£é‡Šä¸ºæŸç§ç¨‹åº¦ä¸Šçš„ "çœŸå®"ï¼Œé‚£ä¹ˆæˆ˜äº‰å¯èƒ½è¿˜ä¼šå‘ç”Ÿ......ä½†æ˜¯ï¼Œè®©æˆ‘ä»¬å…ˆåˆ«æ€¥ç€è¿›å…¥è§£é‡Šå­¦çš„èŒƒç•´ã€‚æˆ‘ä»¬ä¸€ä¼šå„¿å†æ¥è®¨è®ºè¿™ä¸ªè¯é¢˜ã€‚

Right now in early 2023, we have a set of tools that allow us to **shape an LLMâ€™s probability distributions**, so that some regions get smaller or less dense and some regions get larger or denser:  

ç°åœ¨åœ¨2023å¹´åˆï¼Œæˆ‘ä»¬æœ‰ä¸€å¥—å·¥å…·ï¼Œå…è®¸æˆ‘ä»¬å¡‘é€ ä¸€ä¸ªLLMçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½¿ä¸€äº›åŒºåŸŸå˜å¾—æ›´å°æˆ–ä¸é‚£ä¹ˆå¯†é›†ï¼Œä¸€äº›åŒºåŸŸå˜å¾—æ›´å¤§æˆ–æ›´å¯†é›†ã€‚

1.  Training  
    
    åŸ¹è®­
    
2.  Fine-tuning  
    
    å¾®è°ƒ
    
3.  Reinforcement learning with human feedback (RLHF)  
    
    å¸¦æœ‰äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚
    

ğŸ‹ï¸ We canÂ **train**Â a foundation model on high-quality data â€” on collections of symbols that we, as human observers, take to be meaningful and to correspond to true facts about the world. In this way, the trained model is like an atom where the orbitals are shaped in a way we find to be useful.  

ğŸ‹ï¸ æˆ‘ä»¬å¯ä»¥åœ¨é«˜è´¨é‡çš„æ•°æ®ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹--åœ¨æˆ‘ä»¬ä½œä¸ºäººç±»è§‚å¯Ÿè€…è®¤ä¸ºæœ‰æ„ä¹‰å¹¶ä¸ä¸–ç•Œçš„çœŸå®äº‹å®ç›¸å¯¹åº”çš„ç¬¦å·é›†åˆä¸Šã€‚è¿™æ ·ä¸€æ¥ï¼Œè®­ç»ƒè¿‡çš„æ¨¡å‹å°±åƒä¸€ä¸ªåŸå­ï¼Œå…¶ä¸­çš„è½¨é“æ˜¯ä»¥æˆ‘ä»¬è®¤ä¸ºæœ‰ç”¨çš„æ–¹å¼å¡‘é€ çš„ã€‚

ğŸª› Then after poking at that model for a bit and discovering regions in the space of likely outputs that we observers would prefer to stop encountering in the course of our observations, weÂ **fine-tune**Â it with more focused, carefully curated training data. This fine-tuning shrinks some lobes and expands others, and hopefully, after poking some more at the results of this lobe-shaping process weâ€™re happier with the outputs weâ€™re seeing when we collapse the wave function again and again.  

ç„¶åï¼Œåœ¨å¯¹è¯¥æ¨¡å‹è¿›è¡Œäº†ä¸€ç•ªæ¢ç©¶ï¼Œå‘ç°äº†æˆ‘ä»¬è§‚å¯Ÿè€…åœ¨è§‚å¯Ÿè¿‡ç¨‹ä¸­æœ€å¥½ä¸è¦é‡åˆ°çš„å¯èƒ½è¾“å‡ºç©ºé—´çš„åŒºåŸŸä¹‹åï¼Œæˆ‘ä»¬ç”¨æ›´åŠ é›†ä¸­ã€ç²¾å¿ƒç­–åˆ’çš„è®­ç»ƒæ•°æ®å¯¹å…¶è¿›è¡Œäº†å¾®è°ƒã€‚è¿™ç§å¾®è°ƒç¼©å°äº†ä¸€äº›è£‚ç‰‡ï¼Œæ‰©å¤§äº†å¦ä¸€äº›è£‚ç‰‡ï¼Œå¸Œæœ›åœ¨å¯¹è¿™ç§è£‚ç‰‡æ•´å½¢è¿‡ç¨‹çš„ç»“æœè¿›è¡Œæ›´å¤šçš„æ¢ç©¶ä¹‹åï¼Œæˆ‘ä»¬ä¼šå¯¹æˆ‘ä»¬åœ¨ä¸€æ¬¡åˆä¸€æ¬¡æŠ˜å æ³¢å‡½æ•°æ—¶çœ‹åˆ°çš„è¾“å‡ºæ›´åŠ æ»¡æ„ã€‚

ğŸ‘©ğŸ« Finally, we can bring in some humans in theÂ **RLHF**Â phase to help us really to dial in the shape of the modelâ€™s probability space so that it covers as tightly as possible only the points in the space of all possible outputs that correspond to â€œtrue factsâ€ (whatever those are!) about things in the world, and so that it does not cover any points that correspond to â€œfake newsâ€ (whatever that is!) about things in the world. If weâ€™ve done our job right, then all the observations we make of the modelâ€™s probability space will seem true to us.  

ğŸ‘©ğŸ« æœ€åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨RLHFé˜¶æ®µå¼•å…¥ä¸€äº›äººï¼Œå¸®åŠ©æˆ‘ä»¬çœŸæ­£åœ°è°ƒæ•´æ¨¡å‹çš„æ¦‚ç‡ç©ºé—´çš„å½¢çŠ¶ï¼Œä½¿å…¶å°½å¯èƒ½ç´§å¯†åœ°è¦†ç›–æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºç©ºé—´ä¸­å¯¹åº”äºä¸–ç•Œä¸Šäº‹ç‰©çš„ "çœŸå®äº‹å®"ï¼ˆä¸ç®¡é‚£äº›æ˜¯ä»€ä¹ˆï¼ï¼‰çš„ç‚¹ï¼Œå¹¶ä¸”ä¸è¦†ç›–å¯¹åº”äºä¸–ç•Œä¸Šäº‹ç‰©çš„ "å‡æ–°é—»"ï¼ˆä¸ç®¡é‚£æ˜¯ä»€ä¹ˆï¼ï¼‰çš„ä»»ä½•ç‚¹ã€‚å¦‚æœæˆ‘ä»¬çš„å·¥ä½œåšå¯¹äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯¹æ¨¡å‹çš„æ¦‚ç‡ç©ºé—´æ‰€åšçš„æ‰€æœ‰è§‚å¯Ÿå¯¹æˆ‘ä»¬æ¥è¯´éƒ½æ˜¯çœŸå®çš„ã€‚

ğŸ™‹â™‚ï¸ Iâ€™m personally left with **two primary questions** about everything in this section:  

ğŸ™‹â™‚ï¸ æˆ‘ä¸ªäººå¯¹æœ¬èŠ‚ä¸­çš„ä¸€åˆ‡ç•™ä¸‹äº†ä¸¤ä¸ªä¸»è¦é—®é¢˜ã€‚

1.  **Who is this â€œobserverâ€** we keep referencing â€” this human whoâ€™s interpreting the modelâ€™s output and giving it a ğŸ‘ or ğŸ‘? Are they in-group or out-group? Are they smart or stupid? Are they really interested in the truth, or are they just trying to sculpt this modelâ€™s probability distributions to serve their own ends?  
    
    æˆ‘ä»¬ä¸€ç›´æåˆ°çš„ "è§‚å¯Ÿè€… "æ˜¯è°--è¿™ä¸ªè§£é‡Šæ¨¡å‹çš„è¾“å‡ºå¹¶ç»™å®ƒä¸€ä¸ªğŸ‘æˆ–ğŸ‘çš„äººï¼Ÿä»–ä»¬æ˜¯ç¾¤ä½“å†…çš„è¿˜æ˜¯ç¾¤ä½“å¤–çš„ï¼Ÿä»–ä»¬æ˜¯èªæ˜è¿˜æ˜¯æ„šè ¢ï¼Ÿä»–ä»¬æ˜¯çœŸçš„å¯¹çœŸç›¸æ„Ÿå…´è¶£ï¼Œè¿˜æ˜¯åªæ˜¯æƒ³é›•ç¢è¿™ä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒæ¥ä¸ºè‡ªå·±çš„ç›®çš„æœåŠ¡ï¼Ÿ
    
2.  In cases where Iâ€™m lucky enough to be the observer who decides whether the modelâ€™s output is true or not, **do I really always want the truth?** If I ask the model for a comforting bedtime story about a unicorn, do I really want it to go all Neil deGrasse Tyson on me and open with â€œwellÂ _actually_, Jonâ€¦â€? Isnâ€™t the modelâ€™s ability to make things up often a feature, not a bug?  
    
    åœ¨æˆ‘æœ‰å¹¸æˆä¸ºå†³å®šæ¨¡å‹è¾“å‡ºæ˜¯å¦çœŸå®çš„è§‚å¯Ÿè€…çš„æƒ…å†µä¸‹ï¼Œæˆ‘çœŸçš„æ€»æ˜¯æƒ³è¦çœŸç›¸å—ï¼Ÿå¦‚æœæˆ‘è¦æ±‚æ¨¡å‹æä¾›ä¸€ä¸ªå…³äºç‹¬è§’å…½çš„å®‰æ…°æ€§ç¡å‰æ•…äº‹ï¼Œæˆ‘çœŸçš„å¸Œæœ›å®ƒåƒå°¼å°”-å¾·æ ¼æ‹‰æ–¯-æ³°æ£®é‚£æ ·å¯¹æˆ‘è¯´ "äº‹å®ä¸Šï¼Œä¹”æ©...... "å—ï¼Ÿæ¨¡å‹ç¼–é€ äº‹æƒ…çš„èƒ½åŠ›å¾€å¾€æ˜¯ä¸€ä¸ªç‰¹ç‚¹ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªé”™è¯¯å—ï¼Ÿ
    

These questions are central to the issue of â€œhallucination,â€ but they are not technical questions. They are questions of philosophy of language, of hermeneutics, of politics, and, most of all, of **power**.  

è¿™äº›é—®é¢˜æ˜¯ "å¹»è§‰ "é—®é¢˜çš„æ ¸å¿ƒï¼Œä½†å®ƒä»¬ä¸æ˜¯æŠ€æœ¯é—®é¢˜ã€‚å®ƒä»¬æ˜¯è¯­è¨€å“²å­¦çš„é—®é¢˜ï¼Œæ˜¯è§£é‡Šå­¦çš„é—®é¢˜ï¼Œæ˜¯æ”¿æ²»çš„é—®é¢˜ï¼Œæœ€é‡è¦çš„æ˜¯æƒåŠ›çš„é—®é¢˜ã€‚

**This really is the payoff**Â to all this articleâ€™s talk of probability distributions, atomic orbitals, and alive/dead cats:Â _When I, a human being with a history and an identity and a set of goals and interests, observe a single point in the modelâ€™s probability space, itâ€™s always up to me to interpret that point as language and to imbue it with meaning._  

è¿™çœŸçš„æ˜¯è¿™ç¯‡æ–‡ç« ä¸­æ‰€æœ‰å…³äºæ¦‚ç‡åˆ†å¸ƒã€åŸå­è½¨é“å’Œç”Ÿ/æ­»çŒ«çš„è®¨è®ºçš„å›æŠ¥ï¼šå½“æˆ‘ï¼Œä¸€ä¸ªæœ‰å†å²ã€æœ‰èº«ä»½ã€æœ‰ä¸€ç³»åˆ—ç›®æ ‡å’Œå…´è¶£çš„äººï¼Œè§‚å¯Ÿåˆ°æ¨¡å‹çš„æ¦‚ç‡ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹æ—¶ï¼Œæ€»æ˜¯ç”±æˆ‘æ¥æŠŠè¿™ä¸ªç‚¹è§£é‡Šä¸ºè¯­è¨€ï¼Œå¹¶èµ‹äºˆå®ƒæ„ä¹‰ã€‚

Indeed, LLMs may be humanityâ€™s first practical use for [reader response theory](https://www.poetryfoundation.org/learn/glossary-terms/reader-response-theory), apart from academic self-promotion. Authorial intent doesnâ€™t matter because there literally is no author and no intent â€” only a text object that a machine assembles from a vast, multidimensional probability distribution.  

äº‹å®ä¸Šï¼Œé™¤äº†å­¦æœ¯ä¸Šçš„è‡ªæˆ‘æ¨é”€ï¼ŒLLMå¯èƒ½æ˜¯äººç±»å¯¹è¯»è€…ååº”ç†è®ºçš„ç¬¬ä¸€ä¸ªå®é™…ç”¨é€”ã€‚ä½œè€…çš„æ„å›¾å¹¶ä¸é‡è¦ï¼Œå› ä¸ºå®é™…ä¸Šæ²¡æœ‰ä½œè€…ï¼Œä¹Ÿæ²¡æœ‰æ„å›¾--åªæœ‰æœºå™¨ä»å·¨å¤§çš„ã€å¤šç»´çš„æ¦‚ç‡åˆ†å¸ƒä¸­ç»„è£…çš„æ–‡æœ¬å¯¹è±¡ã€‚

The ChatGPT experience of back-and-forth interaction is powerful, but itâ€™s really a kind of [skeuomorphic UI affordance](https://uxplanet.org/a-look-at-skeuomorphic-ui-design-32f50016a50a), like when a calculator app looks like a little physical calculator.  

ChatGPTçš„æ¥å›äº’åŠ¨ä½“éªŒéå¸¸å¼ºå¤§ï¼Œä½†å®ƒå®é™…ä¸Šæ˜¯ä¸€ç§skeuomorphic UIè´Ÿæ‹…ï¼Œå°±åƒä¸€ä¸ªè®¡ç®—å™¨åº”ç”¨çœ‹èµ·æ¥åƒä¸€ä¸ªå°çš„ç‰©ç†è®¡ç®—å™¨ã€‚

ğŸ—£ï¸ It certainly _feels_ like the model itself is learning about you just like youâ€™re learning about it â€” like as the conversation develops youâ€™re each updating your own internal understanding of the other. But this is mostly not whatâ€™s happening, and to the extent that it _is_ happening, itâ€™s happening in a very limited way.  

ğŸ—£ï¸ è¿™å½“ç„¶æ„Ÿè§‰åƒæ˜¯æ¨¡å‹æœ¬èº«åœ¨äº†è§£ä½ ï¼Œå°±åƒä½ åœ¨äº†è§£å®ƒä¸€æ ·--å°±åƒéšç€å¯¹è¯çš„å‘å±•ï¼Œä½ ä»¬éƒ½åœ¨æ›´æ–°è‡ªå·±å¯¹å¯¹æ–¹çš„å†…éƒ¨ç†è§£ã€‚ä½†è¿™å¤§å¤šä¸æ˜¯æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œè€Œä¸”åœ¨å‘ç”Ÿçš„ç¨‹åº¦ä¸Šï¼Œå®ƒæ˜¯ä»¥ä¸€ç§éå¸¸æœ‰é™çš„æ–¹å¼å‘ç”Ÿçš„ã€‚

Thereâ€™s a final concept you need to grasp in order to thoroughly untangle yourself from the idea that ChatGPT is truly talking to _you_, vs. just shouting language-like symbol collections into the void: **the token window**.  

ä¸ºäº†å½»åº•æ‘†è„±ChatGPTçœŸæ­£åœ¨ä¸ä½ äº¤è°ˆçš„æƒ³æ³•ï¼Œè€Œä¸æ˜¯ä»…ä»…åœ¨è™šç©ºä¸­å–Šå‡ºç±»ä¼¼è¯­è¨€çš„ç¬¦å·é›†åˆï¼Œä½ éœ€è¦æŒæ¡ä¸€ä¸ªæœ€åçš„æ¦‚å¿µï¼šä»£å¸çª—å£ã€‚

With a normal language model like what Iâ€™ve described so far in this article, you give the model a set of inputs, it does its probability things, and it gives you an output. The inputs are entered into the modelâ€™s **token window**.  

å¯¹äºä¸€ä¸ªæ­£å¸¸çš„è¯­è¨€æ¨¡å‹ï¼Œå°±åƒæˆ‘åœ¨æœ¬æ–‡ä¸­æ‰€æè¿°çš„é‚£æ ·ï¼Œä½ ç»™æ¨¡å‹ä¸€ç»„è¾“å…¥ï¼Œå®ƒåšå®ƒçš„æ¦‚ç‡äº‹æƒ…ï¼Œç„¶åå®ƒç»™ä½ ä¸€ä¸ªè¾“å‡ºã€‚è¾“å…¥è¢«è¾“å…¥åˆ°æ¨¡å‹çš„æ ‡è®°çª—å£ã€‚

-   **Tokens**Â are the ML term for the â€œsymbolsâ€ Iâ€™ve been talking about in this whole article. I wanted to reduce the specialist lingo so I said â€œsymbols,â€ but really the models take in tokens and spit out tokens.  
    
    ä»£å¸æ˜¯æˆ‘åœ¨è¿™ç¯‡æ–‡ç« ä¸­ä¸€ç›´åœ¨è°ˆè®ºçš„ "ç¬¦å· "çš„MLæœ¯è¯­ã€‚æˆ‘æƒ³å‡å°‘ä¸“å®¶çš„è¡Œè¯ï¼Œæ‰€ä»¥æˆ‘è¯´ "ç¬¦å·"ï¼Œä½†å®é™…ä¸Šï¼Œæ¨¡å‹å¸æ”¶äº†ç¬¦å·å¹¶åå‡ºç¬¦å·ã€‚
    
-   TheÂ **window**Â is the number of tokens a model can ingest and turn into a sequence of related output tokens.  
    
    çª—å£æ˜¯æŒ‡ä¸€ä¸ªæ¨¡å‹èƒ½å¤Ÿæ‘„å…¥å¹¶è½¬åŒ–ä¸ºç›¸å…³è¾“å‡ºæ ‡è®°åºåˆ—çš„æ ‡è®°æ•°é‡ã€‚
    

So when I use a pre-ChatGPT LLM, I put tokens into a token window, then the model returns a point in its latent space thatâ€™s located near the tokens I put in.  

å› æ­¤ï¼Œå½“æˆ‘ä½¿ç”¨ChatGPTä¹‹å‰çš„LLMæ—¶ï¼Œæˆ‘æŠŠä»¤ç‰Œæ”¾å…¥ä»¤ç‰Œçª—å£ï¼Œç„¶åæ¨¡å‹åœ¨å…¶æ½œåœ¨ç©ºé—´ä¸­è¿”å›ä¸€ä¸ªä½äºæˆ‘æ”¾å…¥çš„ä»¤ç‰Œé™„è¿‘çš„ç‚¹ã€‚

Letâ€™s return to the diagram at the start of the article and look at it again with our new knowledge of token windows and probability distributions. That weird-looking thing in the middle is supposed to be the model:  

è®©æˆ‘ä»¬å›åˆ°æ–‡ç« å¼€å¤´çš„é‚£å¼ å›¾ï¼Œç”¨æˆ‘ä»¬å¯¹ä»£å¸çª—å£å’Œæ¦‚ç‡åˆ†å¸ƒçš„æ–°çŸ¥è¯†å†çœ‹ä¸€éã€‚ä¸­é—´é‚£ä¸ªçœ‹èµ·æ¥å¾ˆå¥‡æ€ªçš„ä¸œè¥¿åº”è¯¥æ˜¯æ¨¡å‹ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F68dcee14-7cdb-4f5d-a670-b353cb068d12_1000x543.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68dcee14-7cdb-4f5d-a670-b353cb068d12_1000x543.jpeg)

**ğŸ”‘ This point is key:**Â the probability spaceÂ _does not change shape_Â in response to the tokens I put into the token window. The modelâ€™s weights remain static as I interact with it from one window to the other. It doesnâ€™t remember the contents of previous token windows.  

è¿™ä¸€ç‚¹å¾ˆå…³é”®ï¼šæ¦‚ç‡ç©ºé—´ä¸ä¼šå› ä¸ºæˆ‘æ”¾å…¥æ ‡è®°çª—å£çš„æ ‡è®°è€Œæ”¹å˜å½¢çŠ¶ã€‚å½“æˆ‘ä»ä¸€ä¸ªçª—å£åˆ°å¦ä¸€ä¸ªçª—å£ä¸å®ƒäº’åŠ¨æ—¶ï¼Œæ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ã€‚å®ƒå¹¶ä¸è®°å¾—ä»¥å‰çš„æ ‡è®°çª—å£çš„å†…å®¹ã€‚

Think of it this way: Every time I put tokens into the modelâ€™s token window and hit â€œSubmit,â€ itâ€™s like Iâ€™m walking up to it on the street and speaking to it for the very first time. It has no place to store any mutually evolving, shared history with me.  

è¿™æ ·æƒ³å§ã€‚æ¯æ¬¡æˆ‘æŠŠä»£å¸æ”¾å…¥æ¨¡å‹çš„ä»£å¸çª—å£å¹¶ç‚¹å‡» "æäº¤"ï¼Œå°±åƒæˆ‘åœ¨è¡—ä¸Šèµ°åˆ°å®ƒé¢å‰ï¼Œç¬¬ä¸€æ¬¡å’Œå®ƒè¯´è¯ã€‚å®ƒæ²¡æœ‰åœ°æ–¹å‚¨å­˜ä»»ä½•ä¸æˆ‘å…±åŒå‘å±•çš„ã€å…±äº«çš„å†å²ã€‚

**Another analogy** thatâ€™s not perfect, but itâ€™s pretty close: Every time I feed a specific prompt and a seed into Stable Diffusion, I get back the same image. Each prompt and seed combination will take me to a single point in latent space where a particular image is located, and if I keep giving it the same input tokens Iâ€™ll keep getting the same output tokens. Iâ€™m not building up any shared history with the model and Stable Diffusion isnâ€™t â€œlearningâ€ anything about me as I use it.  

å¦ä¸€ä¸ªæ¯”å–»å¹¶ä¸å®Œç¾ï¼Œä½†ä¹Ÿå¾ˆæ¥è¿‘ã€‚æ¯æ¬¡æˆ‘æŠŠä¸€ä¸ªç‰¹å®šçš„æç¤ºå’Œä¸€ä¸ªç§å­è¾“å…¥ç¨³å®šæ‰©æ•£ï¼Œæˆ‘å¾—åˆ°çš„éƒ½æ˜¯ç›¸åŒçš„å›¾åƒã€‚æ¯ä¸€ä¸ªæç¤ºå’Œç§å­çš„ç»„åˆéƒ½ä¼šæŠŠæˆ‘å¸¦åˆ°æ½œåœ¨ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œé‚£é‡Œæœ‰ä¸€ä¸ªç‰¹å®šçš„å›¾åƒï¼Œå¦‚æœæˆ‘ç»§ç»­ç»™å®ƒç›¸åŒçš„è¾“å…¥æ ‡è®°ï¼Œæˆ‘å°±ä¼šç»§ç»­å¾—åˆ°ç›¸åŒçš„è¾“å‡ºæ ‡è®°ã€‚æˆ‘æ²¡æœ‰ä¸æ¨¡å‹å»ºç«‹ä»»ä½•å…±åŒçš„å†å²ï¼Œç¨³å®šæ‰©æ•£ä¹Ÿæ²¡æœ‰åœ¨æˆ‘ä½¿ç”¨å®ƒæ—¶ "å­¦ä¹  "ä»»ä½•å…³äºæˆ‘çš„ä¸œè¥¿ã€‚

(Language models like GPT-3 **purposefully inject randomness** in places where Stable Diffusion does not, in order to vary the output and make it seem more natural. Thatâ€™s why you get different responses to the same prompt from a language model â€” itâ€™s a little bit like using Stable Diffusion but the software is forcing you to vary the seed number on each submission. Itâ€™s notÂ _exactly_Â like that, but itâ€™s close enough.)  

(åƒGPT-3è¿™æ ·çš„è¯­è¨€æ¨¡å‹æ•…æ„åœ¨ç¨³å®šæ‰©æ•£æ²¡æœ‰çš„åœ°æ–¹æ³¨å…¥éšæœºæ€§ï¼Œä»¥æ”¹å˜è¾“å‡ºï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´è‡ªç„¶ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ ä»è¯­è¨€æ¨¡å‹ä¸­å¾—åˆ°å¯¹åŒä¸€æç¤ºçš„ä¸åŒååº”--è¿™æœ‰ç‚¹åƒä½¿ç”¨ç¨³å®šæ‰©æ•£ï¼Œä½†è½¯ä»¶å¼ºè¿«ä½ åœ¨æ¯æ¬¡æäº¤æ—¶æ”¹å˜ç§å­æ•°ã€‚è¿™å¹¶ä¸å®Œå…¨æ˜¯è¿™æ ·ï¼Œä½†å·²ç»å¾ˆæ¥è¿‘äº†ï¼‰ã€‚

ğŸ¤ The end result is that the only brand new, post-training information that the model can obtain about the world is **limited to whatever I put into a token window**. If I type a word sequence containing facts about my marital status into the token window, then the model takes those sequences and returns a point in latent space thatâ€™s adjacent to the input sequence â€” a point that probably represents facts that seem related to the facts I gave it in the token window.  

æœ€ç»ˆçš„ç»“æœæ˜¯ï¼Œæ¨¡å‹èƒ½å¤Ÿè·å¾—çš„å…³äºä¸–ç•Œçš„å”¯ä¸€å…¨æ–°çš„ã€è®­ç»ƒåçš„ä¿¡æ¯ä»…é™äºæˆ‘è¾“å…¥åˆ°æ ‡è®°çª—å£ä¸­çš„ä¸œè¥¿ã€‚å¦‚æœæˆ‘åœ¨æ ‡è®°çª—å£ä¸­é”®å…¥ä¸€ä¸ªåŒ…å«å…³äºæˆ‘å©šå§»çŠ¶å†µçš„äº‹å®çš„å•è¯åºåˆ—ï¼Œé‚£ä¹ˆæ¨¡å‹å°±ä¼šæ¥å—è¿™äº›åºåˆ—ï¼Œå¹¶åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿”å›ä¸€ä¸ªä¸è¾“å…¥åºåˆ—ç›¸é‚»çš„ç‚¹--è¿™ä¸ªç‚¹å¯èƒ½ä»£è¡¨ä¸æˆ‘åœ¨æ ‡è®°çª—å£ä¸­æä¾›çš„äº‹å®ç›¸å…³çš„äº‹å®ã€‚

ChatGPT is an LLM with a **single, large token window**, but it uses its token window in such a way as to make the experience of using the model feel more interactive, like a real conversation.  

ChatGPTæ˜¯ä¸€ä¸ªå…·æœ‰å•ä¸€çš„ã€å¤§çš„ä»¤ç‰Œçª—å£çš„LLMï¼Œä½†å®ƒä½¿ç”¨å…¶ä»¤ç‰Œçª—å£çš„æ–¹å¼ä½¿ä½¿ç”¨è¯¥æ¨¡å‹çš„ä½“éªŒæ„Ÿè§‰æ›´åŠ äº’åŠ¨ï¼Œåƒä¸€ä¸ªçœŸæ­£çš„å¯¹è¯ã€‚

**Warning**: I havenâ€™t read an explicit explanation of how it works, so whatâ€™s in this section is my informed speculation based on how it kindaÂ _has_Â to work. There really arenâ€™t any other options given how LLMs function, but as always if Iâ€™m wrong Iâ€™m happy to be corrected.  

è­¦å‘Šã€‚æˆ‘æ²¡æœ‰è¯»è¿‡å…³äºå®ƒå¦‚ä½•å·¥ä½œçš„æ˜ç¡®è§£é‡Šï¼Œæ‰€ä»¥æœ¬èŠ‚ä¸­çš„å†…å®¹æ˜¯æˆ‘æ ¹æ®å®ƒå¦‚ä½•å·¥ä½œçš„çŸ¥æƒ…çŒœæµ‹ã€‚è€ƒè™‘åˆ°LLMçš„è¿ä½œæ–¹å¼ï¼Œç¡®å®æ²¡æœ‰ä»»ä½•å…¶ä»–é€‰æ‹©ï¼Œä½†æ˜¯ï¼Œå¦‚æœæˆ‘é”™äº†ï¼Œæˆ‘å¾ˆä¹æ„è¢«çº æ­£ã€‚

ChatGPTâ€™s clever UI trick is easier to show than to describe, so letâ€™s imagine that I open up my browser and start a fresh ChatGPT session that goes something like this:  

ChatGPTå·§å¦™çš„ç”¨æˆ·ç•Œé¢æŠ€å·§ï¼Œå±•ç¤ºèµ·æ¥æ¯”æè¿°èµ·æ¥æ›´å®¹æ˜“ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘æ‰“å¼€æµè§ˆå™¨ï¼Œå¼€å§‹ä¸€ä¸ªæ–°çš„ChatGPTä¼šè¯ï¼Œå†…å®¹æ˜¯è¿™æ ·çš„ã€‚

-   Me: Whatâ€™s up, man.  
    
    æˆ‘ï¼šæ€ä¹ˆäº†ï¼Œä¼™è®¡ã€‚
    
-   ChatGPT: Not much. Whatâ€™s up with you?  
    
    ChatGPT:ä¸å¤šã€‚ä½ æ˜¯æ€ä¹ˆäº†ï¼Ÿ
    
-   Me: Eh, my cat just died in a lab accident.  
    
    æˆ‘ï¼šå‘ƒï¼Œæˆ‘çš„çŒ«åˆšåˆšåœ¨å®éªŒå®¤äº‹æ•…ä¸­æ­»äº¡ã€‚
    
-   ChatGPT: Iâ€™m very sorry to hear that.  
    
    ChatGPTï¼šæˆ‘å¾ˆé—æ†¾å¬åˆ°è¿™ä¸ªæ¶ˆæ¯ã€‚
    
-   Me: Yeah, it sucks. Iâ€™ve already picked out a dog at the shelter, though. I pick him up this afternoon.Â   
    
    æˆ‘ï¼šæ˜¯çš„ï¼Œè¿™å¾ˆç³Ÿç³•ã€‚ä¸è¿‡ï¼Œæˆ‘å·²ç»åœ¨æ”¶å®¹æ‰€æŒ‘é€‰äº†ä¸€åªç‹—ã€‚æˆ‘ä»Šå¤©ä¸‹åˆå»æ¥å®ƒã€‚
    
-   ChatGPT: Thatâ€™s exciting! Iâ€™m sure your new dog will be a as good a companion for you as your now deceased cat.  
    
    ChatGPT:è¿™çœŸè®©äººå…´å¥‹ï¼æˆ‘ç›¸ä¿¡ä½ çš„æ–°ç‹—ä¼šåƒä½ ç°åœ¨å»ä¸–çš„çŒ«ä¸€æ ·æˆä¸ºä½ çš„å¥½ä¼™ä¼´ã€‚æˆ‘ç›¸ä¿¡ä½ çš„æ–°ç‹—ä¼šåƒä½ ç°åœ¨å»ä¸–çš„çŒ«ä¸€æ ·æˆä¸ºä½ çš„å¥½ä¼™ä¼´ã€‚
    

Letâ€™s break this session down into multiple token windows, each of which gets fed into the model over the course of our session:  

è®©æˆ‘ä»¬æŠŠè¿™ä¸ªä¼šè¯åˆ†è§£æˆå¤šä¸ªæ ‡è®°çª—å£ï¼Œæ¯ä¸ªçª—å£éƒ½åœ¨æˆ‘ä»¬çš„ä¼šè¯è¿‡ç¨‹ä¸­è¢«é€å…¥æ¨¡å‹ã€‚

Token window 1:Â   

ä»£å¸çª—å£1ã€‚

```
{What's up, man.}
```

Output 1:  

è¾“å‡º1ã€‚

```
{Not much. Whatâ€™s up with you?}
```

Token window 2:Â   

ä»£å¸çª—å£ 2:

```
{Me: Whatâ€™s up, man.
 ChatGPT: Not much. Whatâ€™s up with you?
 Me: Eh, my cat just died in a lab accident.}
```

Output 2:  

è¾“å‡º2ã€‚

```
{I'm very sorry to hear that.}
```

Token window 3:  

ä»£å¸çª—å£ 3:

```
{Me: Whatâ€™s up, man.
 ChatGPT: Not much. Whatâ€™s up with you?
 Me: Eh, my cat just died in a lab accident.
 ChatGPT: Iâ€™m very sorry to hear that.
 Me: Yeah, it sucks. Iâ€™ve already picked out a dog at 
     the shelter, though.}
```

Output 3:  

è¾“å‡º3ã€‚

```
{Thatâ€™s exciting! Iâ€™m sure your new dog will be a as good a companion for you as your now deceased cat.}
```

ğŸª„ Do you see the way it works? OpenAI keeps appending the output of each new exchange to the existing output so that the content of the token window grows as the exchange progresses.  

ğŸª„ä½ çœ‹åˆ°å®ƒçš„å·¥ä½œæ–¹å¼äº†å—ï¼ŸOpenAIä¸æ–­åœ°å°†æ¯ä¸ªæ–°çš„äº¤æ¢çš„è¾“å‡ºé™„åŠ åˆ°ç°æœ‰çš„è¾“å‡ºä¸Šï¼Œè¿™æ ·ï¼Œä»¤ç‰Œçª—å£çš„å†…å®¹å°±ä¼šéšç€äº¤æ¢çš„è¿›è¡Œè€Œå¢åŠ ã€‚

Every time I hit â€œSend,â€ OpenAI is not only submitting my latest input to the model â€” itâ€™s also adding to the token window all the previous exchanges in the session so that the model can take the whole â€œchat historyâ€ and use it to steer me toward the right lobe in its probability space.  

æ¯æ¬¡æˆ‘ç‚¹å‡» "å‘é€"ï¼ŒOpenAIä¸ä»…å‘æ¨¡å‹æäº¤æˆ‘çš„æœ€æ–°è¾“å…¥--å®ƒè¿˜å‘æ ‡è®°çª—å£æ·»åŠ ä¼šè¯ä¸­çš„æ‰€æœ‰å…ˆå‰çš„äº¤æµï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥åˆ©ç”¨æ•´ä¸ª "èŠå¤©å†å²"ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥å¼•å¯¼æˆ‘èµ°å‘å…¶æ¦‚ç‡ç©ºé—´ä¸­çš„æ­£ç¡®å¶ã€‚

In other words, as my ChatGPT session progresses, the â€œtext promptâ€ Iâ€™m feeding this model is getting longer and more information-rich. So in that third exchange, the only reason ChatGPT â€œknowsâ€ my cat is dead and can respond appropriately is because OpenAI has secretly been slipping our entire â€œchatâ€ history into every new token window.  

æ¢å¥è¯è¯´ï¼Œéšç€æˆ‘çš„ChatGPTä¼šè¯çš„è¿›è¡Œï¼Œæˆ‘æä¾›ç»™è¿™ä¸ªæ¨¡å‹çš„ "æ–‡æœ¬æç¤º "è¶Šæ¥è¶Šé•¿ï¼Œä¿¡æ¯ä¹Ÿè¶Šæ¥è¶Šä¸°å¯Œã€‚å› æ­¤ï¼Œåœ¨ç¬¬ä¸‰æ¬¡äº¤æµä¸­ï¼ŒChatGPTä¹‹æ‰€ä»¥ "çŸ¥é“ "æˆ‘çš„çŒ«æ­»äº†ï¼Œå¹¶èƒ½åšå‡ºé€‚å½“çš„ååº”ï¼Œæ˜¯å› ä¸ºOpenAIä¸€ç›´åœ¨ç§˜å¯†åœ°å°†æˆ‘ä»¬çš„æ•´ä¸ª "èŠå¤© "å†å²å¡è¿›æ¯ä¸ªæ–°çš„æ ‡è®°çª—å£ã€‚

The model, then, does not and cannot â€œknowâ€ anything about me other than whatâ€™s there in our chat history. The token window, then, is a form of **shared, mutable state** that the model and I are modifying together and that represents everything the model can possibly use to find a relevant word sequence to show me.  

é‚£ä¹ˆï¼Œé™¤äº†æˆ‘ä»¬èŠå¤©å†å²ä¸­çš„å†…å®¹ï¼Œæ¨¡å‹æ²¡æœ‰ä¹Ÿä¸å¯èƒ½ "çŸ¥é“ "å…³äºæˆ‘çš„ä»»ä½•äº‹æƒ…ã€‚é‚£ä¹ˆï¼Œä»¤ç‰Œçª—å£æ˜¯ä¸€ç§å…±äº«çš„ã€å¯å˜çš„çŠ¶æ€ï¼Œæ¨¡å‹å’Œæˆ‘ä¸€èµ·ä¿®æ”¹ï¼Œå®ƒä»£è¡¨äº†æ¨¡å‹å¯ä»¥ç”¨æ¥å¯»æ‰¾ç›¸å…³è¯åºç»™æˆ‘çœ‹çš„ä¸€åˆ‡ã€‚

ğŸ“ To summarize:  

ğŸ“æ€»ç»“ä¸€ä¸‹ã€‚

-   **Frozen and immutable** during normal use: the _modelâ€™s weights_, which give rise to the probability distributions described above and that represent everything it â€œknowsâ€ about language and the world.  
    
    åœ¨æ­£å¸¸ä½¿ç”¨è¿‡ç¨‹ä¸­è¢«å†»ç»“å’Œä¸å¯æ”¹å˜çš„ï¼šæ¨¡å‹çš„æƒé‡ï¼Œå®ƒäº§ç”Ÿäº†ä¸Šè¿°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œä»£è¡¨äº†å®ƒå¯¹è¯­è¨€å’Œä¸–ç•Œçš„ä¸€åˆ‡ "äº†è§£"ã€‚
    
-   **Can be updated with new facts** about the world: the _token window_ that I dump into the model in order to get fresh output from it.  
    
    å¯ä»¥ç”¨å…³äºä¸–ç•Œçš„æ–°äº‹å®æ¥æ›´æ–°ï¼šæˆ‘å€¾å€’åˆ°æ¨¡å‹ä¸­çš„ä»¤ç‰Œçª—å£ï¼Œä»¥ä¾¿ä»å®ƒé‚£é‡Œè·å¾—æ–°çš„è¾“å‡ºã€‚
    

Note that in the case of BingGPT, it seems there was probably an extra step in there where that newer model was doing a web search, then dumping the results into the token window alongside the entirety of the chat history.  

è¯·æ³¨æ„ï¼Œåœ¨BingGPTçš„æ¡ˆä¾‹ä¸­ï¼Œä¼¼ä¹å¯èƒ½æœ‰ä¸€ä¸ªé¢å¤–çš„æ­¥éª¤ï¼Œå³é‚£ä¸ªè¾ƒæ–°çš„æ¨¡å‹æ­£åœ¨è¿›è¡Œç½‘ç»œæœç´¢ï¼Œç„¶åå°†ç»“æœä¸æ•´ä¸ªèŠå¤©å†å²ä¸€èµ·å€¾å€’åœ¨æ ‡è®°çª—å£ä¸­ã€‚

ğŸ³ If youâ€™ve grasped all of the above, then you know why the **32K-token window** on the upcoming versions of OpenAIâ€™s models is a really big deal. Thatâ€™s enough tokens to really load up the model with fresh facts, like customer service histories, book chapters or scripts, action sequences, and many other things.  

ğŸ³å¦‚æœä½ æŒæ¡äº†ä¸Šè¿°æ‰€æœ‰å†…å®¹ï¼Œé‚£ä¹ˆä½ å°±çŸ¥é“ä¸ºä»€ä¹ˆOpenAIçš„æ¨¡å‹å³å°†æ¨å‡ºçš„ç‰ˆæœ¬ä¸Šçš„32Kä»¤ç‰Œçª—å£æ˜¯ä¸€ä¸ªçœŸæ­£çš„å¤§é—®é¢˜ã€‚è¿™æœ‰è¶³å¤Ÿçš„ä»¤ç‰Œæ¥çœŸæ­£ç»™æ¨¡å‹åŠ è½½æ–°é²œçš„äº‹å®ï¼Œæ¯”å¦‚å®¢æˆ·æœåŠ¡å†å²ã€ä¹¦ç±ç« èŠ‚æˆ–è„šæœ¬ã€åŠ¨ä½œåºåˆ—ï¼Œä»¥åŠå…¶ä»–è®¸å¤šä¸œè¥¿ã€‚

As token windows get bigger, pre-trained models can â€œlearnâ€ more things on-the-fly. So watch for bigger token windows to unlock brand-new AI capabilities.  

éšç€ä»£å¸çª—å£è¶Šæ¥è¶Šå¤§ï¼Œé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥å³æ—¶ "å­¦ä¹  "æ›´å¤šä¸œè¥¿ã€‚å› æ­¤ï¼Œè¯·æ³¨æ„æ›´å¤§çš„ä»£å¸çª—å£å°†é‡Šæ”¾å‡ºå…¨æ–°çš„äººå·¥æ™ºèƒ½èƒ½åŠ›ã€‚
