---
title: "CHATå †æ ˆã€GPT-4å’Œè½¯ä»¶çš„è¿‘æœŸå‰æ™¯"
date: 2023-03-15T11:34:20+08:00
updated: 2023-03-15T11:34:20+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/the-chat-stack-gpt-4-and-the-near
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "The CHAT Stack, GPT-4, And The Near-Term Future Of Software"
  original_lang: en
---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fed777251-96cb-4318-afa7-c7b7459d49eb_4096x3430.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed777251-96cb-4318-afa7-c7b7459d49eb_4096x3430.jpeg)

_**The story so far:** _GPT-4_ wasÂ [announced](https://openai.com/research/gpt-4)Â a few hours ago, and Iâ€™m sure you can find good coverage of what the model can do in your outlet of choice. Iâ€™ll link to a few resources at the end of this article, and you can explore from there.  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ã€‚_GPT-4_åœ¨å‡ ä¸ªå°æ—¶å‰å®£å¸ƒï¼Œæˆ‘ç›¸ä¿¡ä½ å¯ä»¥åœ¨ä½ é€‰æ‹©çš„åª’ä½“ä¸­æ‰¾åˆ°å…³äºè¯¥æ¨¡å‹èƒ½åšä»€ä¹ˆçš„è‰¯å¥½æŠ¥é“ã€‚æˆ‘å°†åœ¨æœ¬æ–‡æœ«å°¾é“¾æ¥åˆ°ä¸€äº›èµ„æºï¼Œä½ å¯ä»¥ä»é‚£é‡Œè¿›è¡Œæ¢ç´¢ã€‚_

_In this post, Iâ€™m going to skip all the usual feeds-and-speeds coverage and try to place the announcement in the context of a larger topic I was already planning to write about this week: how developers will build apps on top of _ChatGPT_ and similar models.Â   

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è·³è¿‡æ‰€æœ‰é€šå¸¸çš„feeds-and-speedsæŠ¥é“ï¼Œå¹¶è¯•å›¾å°†è¿™ä¸€å®£å¸ƒæ”¾åœ¨æˆ‘æœ¬å‘¨å·²ç»è®¡åˆ’å†™çš„ä¸€ä¸ªæ›´å¤§çš„è¯é¢˜ä¸­ï¼šå¼€å‘è€…å°†å¦‚ä½•åœ¨_ChatGPT_å’Œç±»ä¼¼çš„æ¨¡å‹ä¹‹ä¸Šå»ºç«‹åº”ç”¨ç¨‹åºã€‚_

_As I say in the piece, I often encounter confusion about the basic mechanics of how these models can be used in software, so this is my attempt to help clear this up for everyone whoâ€™s looking at this space and trying to understand how they can adapt to it.  

æ­£å¦‚æˆ‘åœ¨æ–‡ç« ä¸­æ‰€è¯´ï¼Œæˆ‘ç»å¸¸é‡åˆ°å…³äºè¿™äº›æ¨¡å‹å¦‚ä½•åœ¨è½¯ä»¶ä¸­ä½¿ç”¨çš„åŸºæœ¬æœºåˆ¶çš„å›°æƒ‘ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘è¯•å›¾å¸®åŠ©æ¯ä¸€ä¸ªå…³æ³¨è¿™ä¸€é¢†åŸŸå¹¶è¯•å›¾äº†è§£å¦‚ä½•é€‚åº”è¿™ä¸€é¢†åŸŸçš„äººæ¾„æ¸…è¿™ä¸€ç‚¹ã€‚_

ğŸ˜¶ğŸŒ«ï¸ Right now, everyone whoâ€™s watching the advances in AI fantasizes about building roughly the same app: a version of _ChatGPT_ that can do all the miracles we just saw in todayâ€™s _GPT-4_ demo, but thatÂ _also_Â knows a host of facts and concepts that are stashed in some proprietary database.  

ğŸ˜¶ğŸŒ«ï¸ ç°åœ¨ï¼Œæ¯ä¸ªå…³æ³¨äººå·¥æ™ºèƒ½è¿›å±•çš„äººéƒ½å¹»æƒ³ç€å»ºç«‹å¤§è‡´ç›¸åŒçš„åº”ç”¨ç¨‹åºï¼šä¸€ä¸ª_ChatGPT_çš„ç‰ˆæœ¬ï¼Œå®ƒå¯ä»¥å®Œæˆæˆ‘ä»¬åœ¨ä»Šå¤©çš„_GPT-4_æ¼”ç¤ºä¸­åˆšåˆšçœ‹åˆ°çš„æ‰€æœ‰å¥‡è¿¹ï¼Œä½†å®ƒä¹Ÿå¯ä»¥æ˜¾ç¤ºè—åœ¨ä¸€äº›ä¸“æœ‰æ•°æ®åº“ä¸­çš„å¤§é‡äº‹å®å’Œæ¦‚å¿µã€‚

A few examples:  

ä¸¾å‡ ä¸ªä¾‹å­ã€‚

-   As a **writer**, I want a version of _ChatGPT_ that knows my entire publicly available corpus of 25 yearsâ€™ worth of web writing and can interactively answer questions about it â€” a sort of _gpt-jon_.  
    
    ä½œä¸ºä¸€ä¸ªä½œå®¶ï¼Œæˆ‘å¸Œæœ›æœ‰ä¸€ä¸ªç‰ˆæœ¬çš„_ChatGPT_ï¼ŒçŸ¥é“æˆ‘çš„æ•´ä¸ªå…¬å¼€çš„25å¹´çš„ç½‘ç»œå†™ä½œè¯­æ–™åº“ï¼Œå¹¶èƒ½äº’åŠ¨åœ°å›ç­”æœ‰å…³é—®é¢˜--ä¸€ç§gpt-jonã€‚
    
-   A **SaaS company** wants a version of _ChatGPT_ that can answer customer support questions because it knows every customerâ€™s support ticket history and all the information in the companyâ€™s knowledge base.  
    
    ä¸€å®¶SaaSå…¬å¸æƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå›ç­”å®¢æˆ·æ”¯æŒé—®é¢˜çš„_ChatGPT_ç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒçŸ¥é“æ¯ä¸ªå®¢æˆ·çš„æ”¯æŒç¥¨å†å²å’Œå…¬å¸çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰ä¿¡æ¯ã€‚
    
-   A **large corporation** may have reams of siloed data that it wants to use for onboarding new employees via an interactive chat interface â€” everything from HR manuals to department-specific secrets.  
    
    ä¸€å®¶å¤§å…¬å¸å¯èƒ½æœ‰æˆå †çš„å­¤ç«‹æ•°æ®ï¼Œå®ƒæƒ³é€šè¿‡äº’åŠ¨èŠå¤©ç•Œé¢ç”¨äºæ–°å‘˜å·¥çš„å…¥èŒåŸ¹è®­--ä»äººåŠ›èµ„æºæ‰‹å†Œåˆ°éƒ¨é—¨çš„å…·ä½“ç§˜å¯†ï¼Œåº”æœ‰å°½æœ‰ã€‚
    

In talking to people outside of ML hyper-nerd circles, Iâ€™ve realized that most people â€” even very technically savvy people â€” donâ€™t quite understand how the above products will get built on top of _GPT-4_.  

åœ¨ä¸MLè¶…çº§ä¹¦å‘†å­åœˆå­ä»¥å¤–çš„äººäº¤è°ˆæ—¶ï¼Œæˆ‘æ„è¯†åˆ°å¤§å¤šæ•°äºº--ç”šè‡³æ˜¯éå¸¸æ‡‚æŠ€æœ¯çš„äºº--éƒ½ä¸å¤ªç†è§£ä¸Šè¿°äº§å“å°†å¦‚ä½•å»ºç«‹åœ¨_GPT-4_ä¹‹ä¸Šã€‚

Most people imagine, for instance, that in order to put _GPT-4_ to work on their own data, theyâ€™ll need to train a version of it on their proprietary data. So in their mind, the process will work something like this:  

ä¾‹å¦‚ï¼Œå¤§å¤šæ•°äººè®¤ä¸ºï¼Œä¸ºäº†è®©_GPT-4_åœ¨ä»–ä»¬è‡ªå·±çš„æ•°æ®ä¸Šå‘æŒ¥ä½œç”¨ï¼Œä»–ä»¬éœ€è¦åœ¨ä»–ä»¬çš„ä¸“æœ‰æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªç‰ˆæœ¬çš„è½¯ä»¶ã€‚å› æ­¤ï¼Œåœ¨ä»–ä»¬çœ‹æ¥ï¼Œè¿™ä¸ªè¿‡ç¨‹å°†æ˜¯è¿™æ ·çš„ã€‚

_All the modelâ€™s training data + my proprietary data â†’ training â†’ inference â†’ ğŸ¤‘  

æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒæ•°æ®+æˆ‘çš„ä¸“æœ‰æ•°æ®â†’è®­ç»ƒâ†’æ¨ç†â†’ğŸ¤‘_

You could do all of the above, but it would be extremely expensive. Itâ€™s also not even remotely necessary.  

ä½ å¯ä»¥åšä¸Šè¿°æ‰€æœ‰çš„äº‹æƒ…ï¼Œä½†è¿™å°†æ˜¯éå¸¸æ˜‚è´µçš„ã€‚è¿™ä¹Ÿæ˜¯æ ¹æœ¬æ²¡æœ‰å¿…è¦çš„ã€‚

An alternate approach would be to **fine-tune a pre-trained model** so that it knows about a much smaller corpus. Recently, this fine-tuning approach was the best way to build apps like the above. _OpenAI_ offers fine-tuning capabilities via their API, in fact.  

But of course, you need someone on your team who knows how to do this, and for other reasons, itâ€™s not necessarily ideal for many use cases.Â   

å¦ä¸€ç§æ–¹æ³•æ˜¯å¯¹é¢„è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶äº†è§£ä¸€ä¸ªå°å¾—å¤šçš„è¯­æ–™åº“ã€‚æœ€è¿‘ï¼Œè¿™ç§å¾®è°ƒæ–¹æ³•æ˜¯å»ºç«‹ç±»ä¼¼ä¸Šè¿°åº”ç”¨çš„æœ€ä½³æ–¹å¼ã€‚äº‹å®ä¸Šï¼Œ_OpenAI_é€šè¿‡ä»–ä»¬çš„APIæä¾›å¾®è°ƒåŠŸèƒ½ã€‚ä½†å½“ç„¶ï¼Œä½ éœ€è¦ä½ çš„å›¢é˜Ÿä¸­æœ‰äººçŸ¥é“å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€Œä¸”ç”±äºå…¶ä»–åŸå› ï¼Œè¿™å¯¹è®¸å¤šç”¨ä¾‹æ¥è¯´ä¸ä¸€å®šæ˜¯ç†æƒ³çš„ã€‚

ğŸ’¬ But thereâ€™s another way that was sort of working with GPT-3.5, and as of todayâ€™s _GPT-4_ announcement should really work quite well. Iâ€™ve taken to calling it **the CHAT stack**:  

ä½†è¿˜æœ‰å¦ä¸€ç§æ–¹æ³•ï¼Œåœ¨GPT-3.5ä¸­è¿˜ç®—å¯è¡Œï¼Œä»ä»Šå¤©çš„_GPT-4_å…¬å‘Šæ¥çœ‹ï¼Œåº”è¯¥çœŸçš„å¾ˆå¥½ç”¨ã€‚æˆ‘æŠŠå®ƒå«åšCHATæ ˆã€‚

-   **C**ontext
    
-   **H**istory
    
-   **A**PI
    
-   **T**oken window  
    
    ä»£å¸çª—å£
    

Not only is the CHAT stack (under whatever name we eventually settle on) how huge parts of the software ecosystem will begin to work very shortly, but itâ€™s actually already being done â€” I personally just implemented it this past weekend in order to write the aforementioned _gpt-jon_ using a combination of Postgresql (with pgvector), _OpenAI_â€™s embeddings API, _ChatGPT_, and Discord. BingGPT worked like this, too â€” in its case, the specially structured database was Bingâ€™s database of crawled web pages.  

CHATå †æ ˆï¼ˆä¸ç®¡æˆ‘ä»¬æœ€ç»ˆç¡®å®šçš„åç§°æ˜¯ä»€ä¹ˆï¼‰ä¸ä»…æ˜¯è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿçš„å·¨å¤§éƒ¨åˆ†å°†å¾ˆå¿«å¼€å§‹å·¥ä½œï¼Œè€Œä¸”å®é™…ä¸Šå·²ç»åœ¨åšäº†--æˆ‘ä¸ªäººåœ¨ä¸Šå‘¨æœ«åˆšåˆšå®ç°äº†å®ƒï¼Œä»¥ä¾¿ä½¿ç”¨Postgresqlï¼ˆä¸pgvectorï¼‰ã€_OpenAI_çš„åµŒå…¥APIã€_ChatGPT_å’ŒDiscordçš„ç»„åˆæ¥ç¼–å†™ä¸Šè¿°çš„gpt-jonã€‚BingGPTä¹Ÿæ˜¯è¿™æ ·å·¥ä½œçš„--åœ¨å®ƒçš„æ¡ˆä¾‹ä¸­ï¼Œç‰¹æ®Šç»“æ„çš„æ•°æ®åº“æ˜¯Bingçš„æŠ“å–çš„ç½‘é¡µæ•°æ®åº“ã€‚

So letâ€™s take a closer look at the CHAT stack, and then weâ€™ll look at a few features of the newly announced _GPT-4_ model that will turbocharge it.  

å› æ­¤ï¼Œè®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹CHATå †æ ˆï¼Œç„¶åæˆ‘ä»¬å†çœ‹çœ‹æ–°å®£å¸ƒçš„_GPT-4_æ¨¡å‹çš„ä¸€äº›åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½å°†ä½¿å®ƒå˜å¾—æ›´åŠ å¼ºå¤§ã€‚

The shortest, most effective route to building the apps described in the opener is to take advantage of _GPT-4_â€™s token window via a flow like the following:  

æ„å»ºå¼€åœºç™½ä¸­æè¿°çš„åº”ç”¨ç¨‹åºçš„æœ€çŸ­ã€æœ€æœ‰æ•ˆçš„é€”å¾„æ˜¯é€šè¿‡ä»¥ä¸‹æµç¨‹åˆ©ç”¨_GPT-4_çš„ä»¤ç‰Œçª—å£ã€‚

1.  The user submits aÂ **prompt**Â to the chatbot, which is appended to a longer chat history that contains the most recent few user-supplied prompts and LLM-supplied responses (the chatÂ **history**).  
    
    ç”¨æˆ·å‘èŠå¤©æœºå™¨äººæäº¤ä¸€ä¸ªæç¤ºï¼Œè¯¥æç¤ºä¼šè¢«é™„åŠ åˆ°ä¸€ä¸ªè¾ƒé•¿çš„èŠå¤©å†å²ä¸­ï¼Œå…¶ä¸­åŒ…å«æœ€è¿‘çš„å‡ ä¸ªç”¨æˆ·æä¾›çš„æç¤ºå’ŒLLMæä¾›çš„å›åº”ï¼ˆèŠå¤©å†å²ï¼‰ã€‚
    
2.  The chatbot searches a specially structured database for the documents that are most relevant to the userâ€™s prompt and history. This collection of relevant documents is calledÂ **context**.Â   
    
    èŠå¤©æœºå™¨äººä¼šåœ¨ä¸€ä¸ªç‰¹æ®Šç»“æ„çš„æ•°æ®åº“ä¸­æœç´¢ä¸ç”¨æˆ·çš„æç¤ºå’Œå†å²æœ€ç›¸å…³çš„æ–‡ä»¶ã€‚è¿™ä¸ªç›¸å…³æ–‡ä»¶çš„é›†åˆè¢«ç§°ä¸ºcontextã€‚
    
3.  The chatbot then combines the user prompt, the userâ€™s chat history, and the context documents into theÂ **language modelâ€™s token window**Â via anÂ **API**.  
    
    ç„¶åï¼ŒèŠå¤©æœºå™¨äººé€šè¿‡ä¸€ä¸ªAPIå°†ç”¨æˆ·æç¤ºã€ç”¨æˆ·çš„èŠå¤©å†å²å’Œä¸Šä¸‹æ–‡æ–‡ä»¶ç»“åˆåˆ°è¯­è¨€æ¨¡å‹çš„æ ‡è®°çª—å£ä¸­ã€‚
    
4.  The LLM now knows everything it was trained on plus the chat history and context we just fed it, so it canÂ **respond**Â to the userâ€™s prompt in light of that context.  
    
    LLMç°åœ¨çŸ¥é“å®ƒæ‰€è®­ç»ƒçš„æ‰€æœ‰å†…å®¹ï¼Œå†åŠ ä¸Šæˆ‘ä»¬åˆšåˆšæä¾›ç»™å®ƒçš„èŠå¤©å†å²å’ŒèƒŒæ™¯ï¼Œæ‰€ä»¥å®ƒå¯ä»¥æ ¹æ®è¿™äº›èƒŒæ™¯å¯¹ç”¨æˆ·çš„æç¤ºä½œå‡ºååº”ã€‚
    

In the four-stage chatbot setup described above, the LLM understands a large body of facts about the world, and it also has been fine-tuned to respond to instructions and answer questions.  

åœ¨ä¸Šè¿°å››ä¸ªé˜¶æ®µçš„èŠå¤©æœºå™¨äººè®¾ç½®ä¸­ï¼ŒLLMç†è§£äº†å¤§é‡å…³äºä¸–ç•Œçš„äº‹å®ï¼Œå®ƒä¹Ÿè¢«å¾®è°ƒä¸ºå“åº”æŒ‡ä»¤å’Œå›ç­”é—®é¢˜ã€‚  

It can then turn that knowledge and those abilities to the task of examining the context documents the chatbot platform supplied to it alongside a chat history and a prompt in order to pull fresh facts and concepts out of that context material to _craft_ a response.  

ç„¶åï¼Œå®ƒå¯ä»¥å°†è¿™äº›çŸ¥è¯†å’Œèƒ½åŠ›ç”¨äºæ£€æŸ¥èŠå¤©æœºå™¨äººå¹³å°æä¾›ç»™å®ƒçš„èƒŒæ™¯æ–‡ä»¶ï¼Œä»¥åŠèŠå¤©å†å²å’Œæç¤ºï¼Œä»¥ä¾¿ä»èƒŒæ™¯ææ–™ä¸­æå–æ–°çš„äº‹å®å’Œæ¦‚å¿µï¼Œç²¾å¿ƒåˆ¶ä½œä¸€ä¸ªå›åº”ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F6c944da0-17b0-42b8-8720-35500d04d546_1999x1353.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c944da0-17b0-42b8-8720-35500d04d546_1999x1353.png)

The CHAT stack  

CHATå †æ ˆ

That diagram would probably be better as an animation, but to walk through it quickly:  

è¿™å¼ å›¾ä½œä¸ºä¸€ä¸ªåŠ¨ç”»å¯èƒ½ä¼šæ›´å¥½ï¼Œä½†è¦å¿«é€Ÿèµ°å®Œå®ƒã€‚

1.  The context and history (which includes the most recent user-supplied prompt) are combined to form a collection ofÂ **prompt tokens**, and theyâ€™re sent through the API to the token window.  
    
    ä¸Šä¸‹æ–‡å’Œå†å²è®°å½•ï¼ˆåŒ…æ‹¬æœ€è¿‘çš„ç”¨æˆ·æä¾›çš„æç¤ºï¼‰è¢«ç»„åˆæˆä¸€ä¸ªæç¤ºä»¤ç‰Œçš„é›†åˆï¼Œå¹¶é€šè¿‡APIå‘é€åˆ°ä»¤ç‰Œçª—å£ã€‚
    
2.  Those prompt tokens then go into the model, which runs an inference and produces a set ofÂ **completion tokens**Â that have to fit into the token window alongside the input tokens (this is related to the fact that the output tokens are actually generated one at a time, so itâ€™s filling up the token window one token at a time).  
    
    è¿™äº›æç¤ºæ ‡è®°ç„¶åè¿›å…¥æ¨¡å‹ï¼Œæ¨¡å‹è¿è¡Œæ¨ç†å¹¶äº§ç”Ÿä¸€ç»„å®Œæˆæ ‡è®°ï¼Œè¿™äº›æ ‡è®°å¿…é¡»ä¸è¾“å…¥æ ‡è®°ä¸€èµ·æ”¾å…¥æ ‡è®°çª—å£ï¼ˆè¿™ä¸è¾“å‡ºæ ‡è®°å®é™…ä¸Šæ˜¯ä¸€æ¬¡ç”Ÿæˆä¸€ä¸ªçš„äº‹å®æœ‰å…³ï¼Œæ‰€ä»¥å®ƒæ˜¯ä¸€æ¬¡ä¸€ä¸ªæ ‡è®°åœ°å¡«æ»¡æ ‡è®°çª—å£ï¼‰ã€‚
    
3.  TheÂ **completion tokens**Â then go back into the chat history, so the cycle can repeat at step 1.  
    
    å®Œæˆåçš„ä»¤ç‰Œä¼šå›åˆ°èŠå¤©å†å²ä¸­ï¼Œå› æ­¤å¯ä»¥é‡å¤æ­¥éª¤1çš„å¾ªç¯ã€‚
    

The developer owns the stuff to the left of the API boundary â€” the message history and the context database â€” while the AI provider owns the API itself and the stuff to the right of it.  

å¼€å‘è€…æ‹¥æœ‰APIè¾¹ç•Œå·¦è¾¹çš„ä¸œè¥¿--æ¶ˆæ¯å†å²å’Œä¸Šä¸‹æ–‡æ•°æ®åº“--è€Œäººå·¥æ™ºèƒ½æä¾›è€…æ‹¥æœ‰APIæœ¬èº«å’Œå®ƒå³è¾¹çš„ä¸œè¥¿ã€‚

If you want a peek at my janky implementation, hereâ€™s the single function of (n00b) elixir that implements the whole CHAT stack:  

å¦‚æœä½ æƒ³å·çœ‹æˆ‘çš„å¤æ€ªå®ç°ï¼Œè¿™é‡Œæ˜¯ï¼ˆn00bï¼‰elixirçš„å•ä¸€å‡½æ•°ï¼Œå®ç°äº†æ•´ä¸ªCHATå †æ ˆã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fe84e1721-6b3e-4207-9250-a3465e7ee7ec_1150x532.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe84e1721-6b3e-4207-9250-a3465e7ee7ec_1150x532.png)

ğŸ—„ï¸ I expect there will probably be software products and services that focus entirely on context storage and others that focus entirely on message history storage, where both types of storage are able to service requests for specific token amounts and make heavy use of embeddings to optimize token usage.  

ğŸ—„ï¸ï¼Œæˆ‘é¢„è®¡å¯èƒ½ä¼šæœ‰å®Œå…¨ä¸“æ³¨äºä¸Šä¸‹æ–‡å­˜å‚¨çš„è½¯ä»¶äº§å“å’ŒæœåŠ¡ï¼Œä¹Ÿæœ‰å®Œå…¨ä¸“æ³¨äºæ¶ˆæ¯å†å²å­˜å‚¨çš„è½¯ä»¶äº§å“å’ŒæœåŠ¡ï¼Œè¿™ä¸¤ç§ç±»å‹çš„å­˜å‚¨éƒ½èƒ½å¤Ÿä¸ºç‰¹å®šä»¤ç‰Œæ•°é‡çš„è¯·æ±‚æä¾›æœåŠ¡ï¼Œå¹¶å¤§é‡ä½¿ç”¨åµŒå…¥æŠ€æœ¯æ¥ä¼˜åŒ–ä»¤ç‰Œçš„ä½¿ç”¨ã€‚

Note that we can actually generalize the CHAT stack flow a bit more so that itâ€™s not quite so chat-specific and looks more like a standard event-driven software flow:  

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥æŠŠCHATå †æ ˆçš„æµç¨‹æ¦‚æ‹¬å¾—æ›´å¤šä¸€äº›ï¼Œè¿™æ ·å®ƒå°±ä¸å¤ªé’ˆå¯¹èŠå¤©ï¼Œçœ‹èµ·æ¥æ›´åƒä¸€ä¸ªæ ‡å‡†çš„äº‹ä»¶é©±åŠ¨è½¯ä»¶æµç¨‹ã€‚

1.  User action creates anÂ **event**Â with associated data.  
    
    ç”¨æˆ·è¡Œä¸ºåˆ›é€ äº†ä¸€ä¸ªå¸¦æœ‰ç›¸å…³æ•°æ®çš„äº‹ä»¶ã€‚
    
2.  Bot does aÂ **relevance query**Â of a proprietary database for documents or other assets related to that user event.  
    
    æœºå™¨äººå¯¹ä¸€ä¸ªä¸“æœ‰æ•°æ®åº“è¿›è¡Œç›¸å…³æ€§æŸ¥è¯¢ï¼Œä»¥å¯»æ‰¾ä¸è¯¥ç”¨æˆ·äº‹ä»¶ç›¸å…³çš„æ–‡ä»¶æˆ–å…¶ä»–èµ„äº§ã€‚
    
3.  Bot puts the userâ€™s event stream (including the latest event) plus the results of the relevance query into an LLMâ€™sÂ **token window**.  
    
    Botå°†ç”¨æˆ·çš„äº‹ä»¶æµï¼ˆåŒ…æ‹¬æœ€æ–°çš„äº‹ä»¶ï¼‰åŠ ä¸Šç›¸å…³æ€§æŸ¥è¯¢çš„ç»“æœæ”¾å…¥ä¸€ä¸ªLLM'stokençª—å£ã€‚
    
4.  LLM now produces aÂ **response**.  
    
    LLMç°åœ¨äº§ç”Ÿäº†ååº”ã€‚
    

I think weâ€™ll see a lot of this generalized pattern crop up in all kinds of places as developers get their heads around it, the tools are built out, and the APIs mature. Anything that **has an event stream** and **can benefit from predictions** is a candidate for being revolutionized by this pattern. The software landscape of 2024 will look pretty different than what we see right now as a result.  

æˆ‘æƒ³æˆ‘ä»¬ä¼šçœ‹åˆ°å¾ˆå¤šè¿™ç§é€šç”¨çš„æ¨¡å¼å‡ºç°åœ¨å„ç§åœ°æ–¹ï¼Œå› ä¸ºå¼€å‘è€…ä»¬éƒ½åœ¨æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼Œå·¥å…·è¢«å»ºç«‹èµ·æ¥äº†ï¼ŒAPIä¹Ÿæˆç†Ÿäº†ã€‚ä»»ä½•æœ‰äº‹ä»¶æµå¹¶èƒ½ä»é¢„æµ‹ä¸­å—ç›Šçš„ä¸œè¥¿éƒ½æ˜¯è¢«è¿™ç§æ¨¡å¼é©æ–°çš„å€™é€‰è€…ã€‚å› æ­¤ï¼Œ2024å¹´çš„è½¯ä»¶æ™¯è§‚å°†ä¸æˆ‘ä»¬ç°åœ¨çœ‹åˆ°çš„ç›¸å½“ä¸åŒã€‚

In fact, if you have a company of any type, whether itâ€™s software or not, you should have someone dedicated to figuring out how to apply the CHAT stack to your business. Because if you donâ€™t, youâ€™ll be shocked at how quickly youâ€™re left behind.  

äº‹å®ä¸Šï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªä»»ä½•ç±»å‹çš„å…¬å¸ï¼Œæ— è®ºæ˜¯å¦æ˜¯è½¯ä»¶å…¬å¸ï¼Œä½ éƒ½åº”è¯¥æœ‰ä¸€ä¸ªäººä¸“é—¨è´Ÿè´£æ‰¾å‡ºå¦‚ä½•å°†CHATå †æ ˆåº”ç”¨äºä½ çš„ä¸šåŠ¡ã€‚å› ä¸ºå¦‚æœä½ ä¸è¿™æ ·åšï¼Œä½ ä¼šå¯¹ä½ è¢«ç”©åœ¨åé¢çš„é€Ÿåº¦æ„Ÿåˆ°éœ‡æƒŠã€‚

In the CHAT stack Iâ€™ve described here, there are two types of persistent state:  

åœ¨æˆ‘è¿™é‡Œæè¿°çš„CHATå †æ ˆä¸­ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„æŒä¹…æ€§çŠ¶æ€ã€‚

1.  The chat history (or event stream)  
    
    èŠå¤©å†å²ï¼ˆæˆ–äº‹ä»¶æµï¼‰ã€‚
    
2.  The context database  
    
    èƒŒæ™¯æ•°æ®åº“
    

The **context database** is the part that contains all the proprietary that companies and writers like me want to _GPT-4_ to know about and work its magic on.  

You might think you could use ordinary document retrieval systems for this â€” such systems have had the ability to do relevance searches for a very long time.  

ä¸Šä¸‹æ–‡æ•°æ®åº“æ˜¯åŒ…å«æ‰€æœ‰åƒæˆ‘è¿™æ ·çš„å…¬å¸å’Œä½œå®¶å¸Œæœ›_GPT-4_äº†è§£å¹¶å‘æŒ¥å…¶é­”åŠ›çš„ä¸“æœ‰éƒ¨åˆ†ã€‚ä½ å¯èƒ½è®¤ä¸ºä½ å¯ä»¥ä½¿ç”¨æ™®é€šçš„æ–‡ä»¶æ£€ç´¢ç³»ç»Ÿæ¥åšè¿™ä»¶äº‹--è¿™ç§ç³»ç»Ÿæœ‰èƒ½åŠ›è¿›è¡Œç›¸å…³æ€§æœç´¢å·²ç»æœ‰å¾ˆé•¿ä¸€æ®µæ—¶é—´äº†ã€‚

ğŸ¤ But because of the limited number of tokens that can go into the token window, you donâ€™t usually want entire documents to form the basis of the context youâ€™re supplying for every prompt.  

ä½†ç”±äºå¯ä»¥è¿›å…¥æ ‡è®°çª—å£çš„æ ‡è®°æ•°é‡æœ‰é™ï¼Œä½ é€šå¸¸ä¸å¸Œæœ›æ•´ä¸ªæ–‡ä»¶æ„æˆä½ ä¸ºæ¯ä¸ªæç¤ºæä¾›çš„ä¸Šä¸‹æ–‡çš„åŸºç¡€ã€‚  

Rather, you want only the material within the documents thatâ€™s relevant to the prompt, and for extracting that material and bringing it to the surface in a context search, we can use some machine learningâ€¦. But thatâ€™s a topic for another post!  

ç›¸åï¼Œä½ åªæƒ³å¾—åˆ°æ–‡ä»¶ä¸­ä¸æç¤ºæœ‰å…³çš„ææ–™ï¼Œä¸ºäº†æå–è¿™äº›ææ–™å¹¶åœ¨ä¸Šä¸‹æ–‡æœç´¢ä¸­æŠŠå®ƒä»¬å‘ˆç°å‡ºæ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æœºå™¨å­¦ä¹ ....ã€‚ä½†è¿™æ˜¯å¦ä¸€ç¯‡æ–‡ç« çš„ä¸»é¢˜!

Thereâ€™s currently some activity going on in the area of making data stores that work for the specific use case described here, where the relevance query results should ideally take the form of a certain number of tokens worth of context (and not whole documents).  

ç›®å‰ï¼Œåœ¨ä½¿æ•°æ®å­˜å‚¨é€‚ç”¨äºè¿™é‡Œæ‰€æè¿°çš„ç‰¹å®šç”¨ä¾‹æ–¹é¢æœ‰ä¸€äº›æ´»åŠ¨ï¼Œå…¶ä¸­ç›¸å…³æ€§æŸ¥è¯¢ç»“æœæœ€å¥½é‡‡å–ä¸€å®šæ•°é‡çš„æ ‡è®°å€¼çš„ä¸Šä¸‹æ–‡å½¢å¼ï¼ˆè€Œä¸æ˜¯æ•´ä¸ªæ–‡æ¡£ï¼‰ã€‚

[](https://twitter.com/atroyn/status/1625568377766035456)

![Image](https3A2F2Fpbs.substack.com2Fmedia2FFo8tvqcaAAARFKM.jpg)

[](https://twitter.com/atroyn/status/1625568377766035456)

ğŸŒŠ These newer, AI-native data stores will combine with larger token windows, lower costs per token, and better-behaving bots (i.e., theyâ€™re more responsive to instructions from users) to unlock a tsunami of innovation in the coming weeks.  

è¿™äº›æ›´æ–°çš„ã€äººå·¥æ™ºèƒ½åŸç”Ÿçš„æ•°æ®å­˜å‚¨å°†ä¸æ›´å¤§çš„ä»£å¸çª—å£ã€æ›´ä½çš„æ¯ä¸ªä»£å¸æˆæœ¬ä»¥åŠè¡Œä¸ºæ›´å¥½çš„æœºå™¨äººï¼ˆå³å®ƒä»¬å¯¹ç”¨æˆ·çš„æŒ‡ä»¤ååº”æ›´çµæ•ï¼‰ç›¸ç»“åˆï¼Œåœ¨æœªæ¥å‡ å‘¨å†…é‡Šæ”¾å‡ºåˆ›æ–°çš„æµ·å•¸ã€‚

Yeah, I said â€œweeks.â€ Buckle up.  

æ˜¯çš„ï¼Œæˆ‘è¯´ "å‡ å‘¨"ã€‚ç³»å¥½å®‰å…¨å¸¦ã€‚

ğŸ‘·â™‚ï¸ Because AI safety is the topic on everyoneâ€™s mind, I should unpack how the CHAT stack intersects with the challenge of testing the bots before production so they donâ€™t end up trying to convince journalists to divorce their wives.  

ğŸ‘·â™‚ï¸ å› ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨æ˜¯æ¯ä¸ªäººå¿ƒä¸­çš„è¯é¢˜ï¼Œæˆ‘åº”è¯¥è§£å¼€CHATå †æ ˆä¸ç”Ÿäº§å‰æµ‹è¯•æœºå™¨äººçš„æŒ‘æˆ˜ä¹‹é—´çš„äº¤é›†ï¼Œä»¥ä¾¿å®ƒä»¬ä¸ä¼šæœ€ç»ˆè¯•å›¾è¯´æœè®°è€…ä¸ä»–ä»¬çš„å¦»å­ç¦»å©šã€‚

The chat history or event stream I described in the preceding section itself contains two types of data:  

æˆ‘åœ¨ä¸Šä¸€èŠ‚æè¿°çš„èŠå¤©å†å²æˆ–äº‹ä»¶æµæœ¬èº«åŒ…å«ä¸¤ç§ç±»å‹çš„æ•°æ®ã€‚

1.  User-supplied data (events, messages)  
    
    ç”¨æˆ·æä¾›çš„æ•°æ®ï¼ˆäº‹ä»¶ã€æ¶ˆæ¯ï¼‰ã€‚
    
2.  LLM-supplied data (responses to #1 above)  
    
    LLMæä¾›çš„æ•°æ®ï¼ˆå¯¹ä¸Šè¿°ç¬¬1æ¡çš„å›åº”ï¼‰ã€‚
    

To see how this works in a real-life API, take a look at this set of chat messages in the standard message format for the _ChatGPT_ API, taken from the APIâ€™s documentation:  

è¦æƒ³çŸ¥é“è¿™åœ¨ç°å®ç”Ÿæ´»ä¸­çš„APIä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè¯·çœ‹è¿™ç»„_ChatGPT_ APIçš„æ ‡å‡†æ¶ˆæ¯æ ¼å¼çš„èŠå¤©æ¶ˆæ¯ï¼Œå®ƒå–è‡ªAPIçš„æ–‡æ¡£ã€‚

```
  messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
```

You can see from the above that there are three roles recognized by the API, two input roles (system and user) and an output role (assistant) for the botâ€™s response. That last entry in the messages list,Â `{"role": "user", "content": "Where was it played?"}`, is actually what weâ€™d call the â€œpromptâ€ or â€œqueryâ€ â€” itâ€™s the most recent message the user typed to the bot.  

ä½ å¯ä»¥ä»ä¸Šé¢çœ‹åˆ°ï¼Œæœ‰ä¸‰ä¸ªè§’è‰²è¢«APIè®¤å¯ï¼Œä¸¤ä¸ªè¾“å…¥è§’è‰²ï¼ˆç³»ç»Ÿå’Œç”¨æˆ·ï¼‰å’Œä¸€ä¸ªè¾“å‡ºè§’è‰²ï¼ˆåŠ©ç†ï¼‰ï¼Œç”¨äºæœºå™¨äººçš„å“åº”ã€‚æ¶ˆæ¯åˆ—è¡¨ä¸­çš„æœ€åä¸€ä¸ªæ¡ç›®ï¼Œ `{"role": "user", "content": "Where was it played?"}` ï¼Œå®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬æ‰€è¯´çš„ "æç¤º "æˆ– "æŸ¥è¯¢"--å®ƒæ˜¯ç”¨æˆ·å‘æœºå™¨äººè¾“å…¥çš„æœ€è¿‘çš„æ¶ˆæ¯ã€‚

But again, itâ€™s that wholeÂ `messages`Â history with the prompt appended to the end, and not merely the prompt in isolation, that gets submitted to the LLM on each pass and that produces the botâ€™s next batch of output tokens.  

ä½†åŒæ ·åœ°ï¼Œæ¯æ¬¡æäº¤ç»™LLMçš„æ˜¯æ•´ä¸ª `messages` å†å²å’Œé™„åŠ åœ¨æœ«å°¾çš„æç¤ºï¼Œè€Œä¸ä»…ä»…æ˜¯å•ç‹¬çš„æç¤ºï¼Œå¹¶ä¸”äº§ç”Ÿæœºå™¨äººçš„ä¸‹ä¸€æ‰¹è¾“å‡ºä»¤ç‰Œã€‚

ğŸ”„ This chat history structure, then, introduces a very important component into the chatbot experience, a component thatâ€™s the source of the â€œchatâ€ paradigmâ€™s power as a UI affordance and also its unpredictability:Â **reflexivity**.  

é‚£ä¹ˆï¼Œè¿™ç§èŠå¤©å†å²ç»“æ„åœ¨èŠå¤©æœºå™¨äººçš„ä½“éªŒä¸­å¼•å…¥äº†ä¸€ä¸ªéå¸¸é‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œè¿™ä¸ªç»„æˆéƒ¨åˆ†æ˜¯ "èŠå¤© "èŒƒå¼ä½œä¸ºç”¨æˆ·ç•Œé¢è´Ÿæ‹…çš„åŠ›é‡çš„æ¥æºï¼Œä¹Ÿæ˜¯å®ƒçš„ä¸å¯é¢„æµ‹æ€§ï¼šåæ€æ€§ã€‚

Because the outputs of previous bot exchanges are supplied as part of the input to current and future bot exchanges, the botâ€™s overall output is **necessarily going to drift** in ways that are absolutely impossible to predict.  

ç”±äºä»¥å‰çš„æœºå™¨äººäº¤æ¢çš„äº§å‡ºæ˜¯ä½œä¸ºå½“å‰å’Œæœªæ¥çš„æœºå™¨äººäº¤æ¢çš„è¾“å…¥çš„ä¸€éƒ¨åˆ†æä¾›çš„ï¼Œæ‰€ä»¥æœºå™¨äººçš„æ•´ä½“äº§å‡ºå¿…ç„¶ä¼šä»¥ç»å¯¹æ— æ³•é¢„æµ‹çš„æ–¹å¼æ¼‚ç§»ã€‚

This is a bit like Conwaysâ€™ Game of Life, where a few simple rules applied in a reflexive manner give rise to complex, chaotic emergent behavior.  

è¿™æœ‰ç‚¹åƒåº·å¨çš„ "ç”Ÿå‘½æ¸¸æˆ"ï¼Œä¸€äº›ç®€å•çš„è§„åˆ™ä»¥åå°„æ€§çš„æ–¹å¼åº”ç”¨ï¼Œäº§ç”Ÿäº†å¤æ‚ã€æ··ä¹±çš„çªå‘è¡Œä¸ºã€‚  

But in the case of the chatbot, the potential for chaos and drift is greatly increased by the fact that the user keeps introducing fresh randomness with his queries.Â   

ä½†æ˜¯åœ¨èŠå¤©æœºå™¨äººçš„æƒ…å†µä¸‹ï¼Œç”±äºç”¨æˆ·ä¸æ–­åœ°ç”¨ä»–çš„æŸ¥è¯¢å¼•å…¥æ–°çš„éšæœºæ€§ï¼Œæ‰€ä»¥æ··ä¹±å’Œæ¼‚ç§»çš„å¯èƒ½æ€§å¤§å¤§å¢åŠ ã€‚

ğŸ‘º All of this is why you canâ€™t properly red-team these LLMs in the thorough way many people imagine. Itâ€™s not just that you canâ€™t anticipate most of the likely chat prompts people will feed the bot â€” itâ€™s that you cannot possibly imagine most of the likelyÂ _chat histories_Â people will feed the bot.  

æ‰€æœ‰è¿™äº›éƒ½æ˜¯ä¸ºä»€ä¹ˆä½ ä¸èƒ½åƒè®¸å¤šäººæƒ³è±¡çš„é‚£æ ·ï¼Œå¯¹è¿™äº›LLMè¿›è¡Œé€‚å½“çš„çº¢é˜Ÿã€‚è¿™ä¸ä»…ä»…æ˜¯å› ä¸ºä½ æ— æ³•é¢„æµ‹äººä»¬å¯èƒ½ä¼šç»™æœºå™¨äººæä¾›çš„å¤§éƒ¨åˆ†èŠå¤©æç¤ºï¼Œè€Œæ˜¯å› ä¸ºä½ ä¸å¯èƒ½æƒ³è±¡äººä»¬å¯èƒ½ä¼šç»™æœºå™¨äººæä¾›çš„å¤§éƒ¨åˆ†èŠå¤©å†å²ã€‚

Thatâ€™s the key point thatâ€™s missed over and over again in simplistic discussions of safety from people who should know better â€” you canâ€™t possibly iterate through a meaningful part of the space of all possible chat histories in order to validate or evenÂ [smoke test](https://en.wikipedia.org/wiki/Smoke_testing_(software))Â an LLM chatbotâ€™s latent space before production.  

è¿™å°±æ˜¯é‚£äº›åº”è¯¥æ›´äº†è§£å®‰å…¨é—®é¢˜çš„äººåœ¨ç®€å•åŒ–çš„è®¨è®ºä¸­ä¸€å†å¿½ç•¥çš„å…³é”®ç‚¹--ä½ ä¸å¯èƒ½åœ¨æ‰€æœ‰å¯èƒ½çš„èŠå¤©å†å²ç©ºé—´ä¸­è¿­ä»£å‡ºä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†ï¼Œä»¥ä¾¿åœ¨ç”Ÿäº§å‰éªŒè¯æˆ–ç”šè‡³çƒŸç†æµ‹è¯•LLMèŠå¤©æœºå™¨äººçš„æ½œåœ¨ç©ºé—´ã€‚

âœ¨ _GPT-4_ does many startling miracles, which you can witness by following the links in the last section of this post or by browsing your tech news outlet of choice.  

âœ¨ _GPT-4_åˆ›é€ äº†è®¸å¤šæƒŠäººçš„å¥‡è¿¹ï¼Œä½ å¯ä»¥é€šè¿‡å…³æ³¨æœ¬å¸–æœ€åä¸€èŠ‚çš„é“¾æ¥æˆ–æµè§ˆä½ é€‰æ‹©çš„ç§‘æŠ€æ–°é—»æ¸ é“æ¥è§è¯ã€‚  

But it also does a few less glamorous things that will make those miracles easier for developers to turn into products.  

ä½†å®ƒä¹Ÿåšäº†ä¸€äº›ä¸é‚£ä¹ˆå…‰å½©çš„äº‹æƒ…ï¼Œè¿™å°†ä½¿è¿™äº›å¥‡è¿¹æ›´å®¹æ˜“è¢«å¼€å‘è€…è½¬åŒ–ä¸ºäº§å“ã€‚

ğŸ›« The most important place where _GPT-4_ excels is at offering what we might callÂ **control surfaces**Â â€” these are ways that we as humans can steer the model to get better outputs from it.  

_GPT-4_æœ€æ“…é•¿çš„åœ°æ–¹æ˜¯æä¾›æˆ‘ä»¬ç§°ä¹‹ä¸ºæ§åˆ¶é¢çš„ä¸œè¥¿--è¿™äº›æ˜¯æˆ‘ä»¬ä½œä¸ºäººç±»å¯ä»¥å¼•å¯¼æ¨¡å‹è·å¾—æ›´å¥½è¾“å‡ºçš„æ–¹æ³•ã€‚

For instance, thereâ€™s better support forÂ `system`Â role messages â€” this is one of the two standard input types of messages in the API example I showed in the section on safety.Â   

ä¾‹å¦‚ï¼Œå¯¹ `system` è§’è‰²ä¿¡æ¯æœ‰æ›´å¥½çš„æ”¯æŒ--è¿™æ˜¯æˆ‘åœ¨å®‰å…¨ä¸€èŠ‚ä¸­å±•ç¤ºçš„APIä¾‹å­ä¸­ä¸¤ç§æ ‡å‡†è¾“å…¥ç±»å‹çš„ä¿¡æ¯ä¹‹ä¸€ã€‚

The previous version of _ChatGPT_ supported system messages for providing additional instructions and context, but these messages were often ignored.  

ä¸Šä¸€ç‰ˆæœ¬çš„_ChatGPT_æ”¯æŒç³»ç»Ÿæ¶ˆæ¯ï¼Œä»¥æä¾›é¢å¤–çš„æŒ‡ç¤ºå’ŒèƒŒæ™¯ï¼Œä½†è¿™äº›æ¶ˆæ¯ç»å¸¸è¢«å¿½ç•¥ã€‚  

It was hard to get the model to answer in a specific voice or personality â€” believe me, I know because I never really got gpt-jon to sound like me.  

è¦è®©æ¨¡ç‰¹ç”¨ç‰¹å®šçš„å£°éŸ³æˆ–ä¸ªæ€§æ¥å›ç­”æ˜¯å¾ˆéš¾çš„--ç›¸ä¿¡æˆ‘ï¼Œæˆ‘çŸ¥é“ï¼Œå› ä¸ºæˆ‘ä»æœªçœŸæ­£è®©gpt-jonå¬èµ·æ¥åƒæˆ‘ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F8694540d-08ec-445f-a38f-cbd6094e69fd_2166x836.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8694540d-08ec-445f-a38f-cbd6094e69fd_2166x836.png)

_GPT-4_ promises better system role support and a more responsive model that itâ€™s easier to put into a specific persona or voice. This will unlock possibilities for users to bring their own voice or style to the botâ€™s responses.  

_GPT-4_æ‰¿è¯ºæä¾›æ›´å¥½çš„ç³»ç»Ÿè§’è‰²æ”¯æŒå’Œæ›´çµæ•çš„æ¨¡å‹ï¼Œå®ƒæ›´å®¹æ˜“è¢«æ”¾å…¥ç‰¹å®šçš„è§’è‰²æˆ–å£°éŸ³ä¸­ã€‚è¿™å°†ä¸ºç”¨æˆ·æŠŠè‡ªå·±çš„å£°éŸ³æˆ–é£æ ¼å¸¦å…¥æœºå™¨äººçš„ååº”ä¸­æä¾›å¯èƒ½æ€§ã€‚

-   The kinds of companies and academic departments that currently issue updated lists of banned words, along with suggested replacements, will love this. They can make the bot speak in up-to-the-nanosecond social justice lingo.  
    
    é‚£äº›ç›®å‰å‘å¸ƒæœ€æ–°ç¦è¯æ¸…å•å’Œå»ºè®®æ›¿ä»£è¯çš„å…¬å¸å’Œå­¦æœ¯éƒ¨é—¨ä¼šå–œæ¬¢è¿™ä¸ªã€‚ä»–ä»¬å¯ä»¥è®©æœºå™¨äººè¯´ä¸Šä¸€ç§’çš„ç¤¾ä¼šæ­£ä¹‰è¡Œè¯ã€‚
    
-   Companies that are based and red-pilled, and want their internal corporate bots to address users in King James English, liturgical Latin, or BAP-speak (wat mean?) will likewise have their prayers answered.  
    
    é‚£äº›åŸºäºå’Œçº¢è‰²å¡«å……çš„å…¬å¸ï¼Œå¹¶å¸Œæœ›ä»–ä»¬çš„å†…éƒ¨ä¼ä¸šæœºå™¨äººèƒ½ç”¨è©¹å§†æ–¯å›½ç‹è‹±è¯­ã€ç¤¼ä»ªæ‹‰ä¸è¯­æˆ–BAPè¯­ï¼ˆwat mean?ï¼‰æ¥ç§°å‘¼ç”¨æˆ·ï¼Œä»–ä»¬çš„ç¥ˆç¥·åŒæ ·ä¼šå¾—åˆ°å›åº”ã€‚
    
-   Authors who want the bot to write the next chapter of their opus in their signature style can do that.  
    
    æƒ³è®©æœºå™¨äººä»¥å…¶æ ‡å¿—æ€§çš„é£æ ¼å†™å‡ºå…¶ä½œå“çš„ä¸‹ä¸€ç« çš„ä½œè€…å¯ä»¥è¿™ä¹ˆåšã€‚
    

I could go on making stuff up, but you get the idea.  

æˆ‘å¯ä»¥ç»§ç»­ç¼–é€ ä¸œè¥¿ï¼Œä½†ä½ ä¼šæ˜ç™½çš„ã€‚

Finally, the model is just **better at following human orders** and instructions of all kinds. Much of this is a result of improvements in the fine-tuning and RLHF phases, and his covered under discussions of â€œsafetyâ€ in the technical report that was released.  

æœ€åï¼Œè¯¥æ¨¡å‹åœ¨éµå¾ªäººç±»å‘½ä»¤å’Œå„ç§æŒ‡ä»¤æ–¹é¢åšå¾—æ›´å¥½ã€‚è¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å¾®è°ƒå’ŒRLHFé˜¶æ®µæ”¹è¿›çš„ç»“æœï¼Œä»–åœ¨å·²å‘å¸ƒçš„æŠ€æœ¯æŠ¥å‘Šä¸­çš„ "å®‰å…¨ "è®¨è®ºä¸­æœ‰æ‰€æ¶‰åŠã€‚

ğŸ‘‰ â€¼ï¸ The other major feature that _GPT-4_ brings with is anÂ **enlarged token window**Â thatâ€™s doubled in size to 8,192 tokens. The price for this token window is $0.03 per 1,000 prompt tokens and $0.06 per 1,000 completion tokens. This is very cheap compared to just a few months ago, and Iâ€™m sure prices will drop even further as the year progresses.  

ğŸ‘‰â€¼ï¸ _GPT-4_å¸¦æ¥çš„å¦ä¸€ä¸ªä¸»è¦ç‰¹å¾æ˜¯ä¸€ä¸ªæ‰©å¤§çš„ä»£å¸çª—å£ï¼Œå…¶å¤§å°å¢åŠ åˆ°8192ä¸ªä»£å¸ã€‚è¿™ä¸ªä»£å¸çª—å£çš„ä»·æ ¼æ˜¯æ¯1000ä¸ªæç¤ºä»£å¸0.03ç¾å…ƒï¼Œæ¯1000ä¸ªå®Œæˆä»£å¸0.06ç¾å…ƒã€‚ä¸å‡ ä¸ªæœˆå‰ç›¸æ¯”ï¼Œè¿™æ˜¯éå¸¸ä¾¿å®œçš„ï¼Œè€Œä¸”æˆ‘ç›¸ä¿¡éšç€ä»Šå¹´çš„è¿›å±•ï¼Œä»·æ ¼ä¼šè¿›ä¸€æ­¥ä¸‹é™ã€‚

Thereâ€™s also aÂ **much larger window**Â of 32,768 tokens that will eventually be available for everyone at double the price per token of the smaller window.  

è¿˜æœ‰ä¸€ä¸ªæ›´å¤§çš„çª—å£ï¼Œå³32,768ä¸ªä»£å¸ï¼Œæœ€ç»ˆå°†ä»¥æ¯ä¸ªä»£å¸ä¸¤å€äºå°çª—å£çš„ä»·æ ¼æä¾›ç»™æ‰€æœ‰äººã€‚

These larger token window sizes will make it much easier for CHAT stack applications to get relevant context and message history into the token window without blowing out the token budget.  

è¿™äº›è¾ƒå¤§çš„ä»¤ç‰Œçª—å£å°ºå¯¸å°†ä½¿CHATå †æ ˆåº”ç”¨æ›´å®¹æ˜“å°†ç›¸å…³çš„ä¸Šä¸‹æ–‡å’Œæ¶ˆæ¯å†å²çº³å…¥ä»¤ç‰Œçª—å£ï¼Œè€Œä¸ä¼šè€—å°½ä»¤ç‰Œé¢„ç®—ã€‚  

Given the state of current vector databases and the amount of pain involved in trying to only the most relevant parts of context documents into the token window so as not to waste tokens, the larger window will be more forgiving for the current toolchain and will be a big enabler of developer productivity.  

é‰´äºç›®å‰çŸ¢é‡æ•°æ®åº“çš„çŠ¶å†µï¼Œä»¥åŠä¸ºäº†ä¸æµªè´¹ä»¤ç‰Œè€Œè¯•å›¾åªå°†ä¸Šä¸‹æ–‡æ–‡ä»¶ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†æ”¾å…¥ä»¤ç‰Œçª—å£çš„ç—›è‹¦ç¨‹åº¦ï¼Œæ›´å¤§çš„çª—å£å°†å¯¹ç›®å‰çš„å·¥å…·é“¾æ›´åŠ å®½å®¹ï¼Œå¹¶å°†æˆä¸ºå¼€å‘è€…ç”Ÿäº§åŠ›çš„ä¸€å¤§æ¨åŠ¨åŠ›ã€‚

Longer-term, as context databases improve their ability to surface the most relevant collections of tokens, the larger token windows will make the _GPT-4_\-based bots feel more intelligent.  

ä»é•¿è¿œæ¥çœ‹ï¼Œéšç€è¯­å¢ƒæ•°æ®åº“æé«˜å…¶æµ®ç°æœ€ç›¸å…³çš„ä»¤ç‰Œé›†åˆçš„èƒ½åŠ›ï¼Œæ›´å¤§çš„ä»¤ç‰Œçª—å£å°†ä½¿åŸºäº_GPT-4_çš„æœºå™¨äººæ„Ÿè§‰æ›´æ™ºèƒ½ã€‚

-   [The _OpenAI_ announcement  
    
    _OpenAI_çš„å…¬å‘Š](https://openai.com/research/gpt-4)
    
-   [Hacker News thread  
    
    é»‘å®¢æ–°é—»ä¸»é¢˜](https://news.ycombinator.com/item?id=35154527)
    
-   [Reddit thread with many relevant links  
    
    æœ‰è®¸å¤šç›¸å…³é“¾æ¥çš„Redditä¸»é¢˜](https://www.reddit.com/r/MachineLearning/comments/11rc02e/news_openai_announced_gpt4/)
    

<iframe src="https://www.youtube-nocookie.com/embed/outcGtbnMuQ?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" data-immersive-translate-effect="1"></iframe>
