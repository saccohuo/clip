---
title: "äººå·¥æ™ºèƒ½å®‰å…¨ï¼šæŠ€æœ¯ä¸äººç§å­¦æ¦‚è¿°"
date: 2023-03-30T14:03:04+08:00
updated: 2023-03-30T14:03:04+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/ai-safety-a-technical-and-ethnographic
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "AI Safety: A Technical & Ethnographic Overview"
  original_lang: en
---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F92820971-b8e4-449d-8993-e62ecc3cc198_1408x1024.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F92820971-b8e4-449d-8993-e62ecc3cc198_1408x1024.jpeg)

**The story so far:**Â _We need to talk about The Letter. No not [that letter](https://harpers.org/a-letter-on-justice-and-open-debate/), or [the other letter](https://nytletter.com/), or the [other one](https://concerned.tech/)â€¦ **[the AI letter](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)**. The one that calls on all of humanity to, â€œpause for at least 6 months the training of AI systems more powerful than GPT-4.â€_  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šæˆ‘ä»¬éœ€è¦è°ˆè°ˆé‚£å°ä¿¡ã€‚ä¸ï¼Œä¸æ˜¯é‚£å°ä¿¡ï¼Œä¹Ÿä¸æ˜¯å¦ä¸€å°ï¼Œæˆ–è€…å¦ä¸€å°......äººå·¥æ™ºèƒ½çš„ä¿¡ã€‚é‚£å°å‘¼åå…¨äººç±» "æš‚åœè®­ç»ƒæ¯”GPT-4æ›´å¼ºå¤§çš„AIç³»ç»Ÿè‡³å°‘6ä¸ªæœˆ "çš„ä¿¡ã€‚

_For some weeks I had been noodling on a draft entitled, â€œAI Safety: An Ethnography,â€ and today was going to be the day I published it.  

But earlier today that letter dropped, and it so powerfully illuminated the rich, complex contours of the terrain of the â€œAI safetyâ€ issue that I had to fishtail and rework this post to center it.  

å‡ ä¸ªæ˜ŸæœŸä»¥æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨é…é…¿ä¸€ä»½é¢˜ä¸º "äººå·¥æ™ºèƒ½å®‰å…¨ "çš„è‰æ¡ˆï¼šäººç§å­¦"ï¼Œä»Šå¤©æœ¬æ¥æ˜¯æˆ‘å‘è¡¨å®ƒçš„æ—¥å­ã€‚ä½†ä»Šå¤©æ—©äº›æ—¶å€™ï¼Œé‚£å°ä¿¡æ‰äº†ä¸‹æ¥ï¼Œå®ƒå¦‚æ­¤æœ‰åŠ›åœ°é˜æ˜äº† "äººå·¥æ™ºèƒ½å®‰å…¨ "é—®é¢˜çš„ä¸°å¯Œã€å¤æ‚çš„è½®å»“ï¼Œä»¥è‡³äºæˆ‘ä¸å¾—ä¸æŠŠå®ƒæ”¾åœ¨ä¸­å¿ƒä½ç½®ï¼Œé‡æ–°ä¿®æ”¹è¿™ç¯‡æ–‡ç« ã€‚_

_There are two main threads Iâ€™ll follow in this piece:  

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†éµå¾ªä¸¤æ¡ä¸»çº¿ï¼š_

1.  _Technical, covering some key specifics about GPT-4 that make the open letter honestly kind of nonsensical and a bit of a Rorschach test.Â   
    
    æŠ€æœ¯æ€§çš„ï¼Œæ¶µç›–äº†å…³äºGPT-4çš„ä¸€äº›å…³é”®ç»†èŠ‚ï¼Œä½¿å…¬å¼€ä¿¡è¯´å®è¯æœ‰ç‚¹æ— ç¨½ä¹‹è°ˆï¼Œæœ‰ç‚¹åƒç½—å¤å…‹æµ‹è¯•ã€‚_
    
2.  _Ethnographic, covering the cultural and tribal divisions and allegiances that are cropping up around the technical issues.  
    
    äººç§å­¦ï¼Œæ¶µç›–äº†å›´ç»•æŠ€æœ¯é—®é¢˜å‡ºç°çš„æ–‡åŒ–å’Œéƒ¨è½çš„åˆ†æ­§å’Œå¿ è¯šã€‚_
    

_Iâ€™ve crammed these threads into one extremely lengthy post because in order to understand the tribal dynamics you have to understand the technical issues.  

But you also canâ€™t understand how the technical issues are framed without understanding the tribal dynamics.  

These things are intertwined in the â€œAI safetyâ€ issue to a degree that makes them impossible to separate, which is why this topic has fascinated me since the beginning of this newsletter in 2021.  

æˆ‘æŠŠè¿™äº›è¯é¢˜å¡è¿›ä¸€ä¸ªæå…¶å†—é•¿çš„å¸–å­ï¼Œå› ä¸ºä¸ºäº†ç†è§£éƒ¨è½çš„åŠ¨æ€ï¼Œä½ å¿…é¡»ç†è§£æŠ€æœ¯é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä¸äº†è§£éƒ¨è½çš„åŠ¨æ€ï¼Œä½ ä¹Ÿæ— æ³•ç†è§£æŠ€æœ¯é—®é¢˜æ˜¯å¦‚ä½•æ„æˆçš„ã€‚è¿™äº›ä¸œè¥¿åœ¨ "äººå·¥æ™ºèƒ½å®‰å…¨ "é—®é¢˜ä¸Šäº¤ç»‡åœ¨ä¸€èµ·ï¼Œå…¶ç¨‹åº¦ä½¿å®ƒä»¬æ— æ³•åˆ†å¼€ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä»2021å¹´è¿™ä»½é€šè®¯å¼€å§‹ï¼Œè¿™ä¸ªè¯é¢˜å°±å¸å¼•ç€æˆ‘ã€‚_

When it comes to the question of how worried we should all be about AI, everyone initially arrives at this issue with their own culture war baggage.  

å½“æ¶‰åŠåˆ°æˆ‘ä»¬åº”è¯¥å¯¹äººå·¥æ™ºèƒ½æœ‰å¤šæ‹…å¿ƒçš„é—®é¢˜æ—¶ï¼Œæ¯ä¸ªäººæœ€åˆéƒ½å¸¦ç€è‡ªå·±çš„æ–‡åŒ–æˆ˜äº‰åŒ…è¢±æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚  

But what Iâ€™m seeing play out after that initial encounter is a two-step process:  

ä½†æˆ‘çœ‹åˆ°çš„æ˜¯ï¼Œåœ¨æœ€åˆçš„æ¥è§¦ä¹‹åï¼Œå‡ºç°äº†ä¸€ä¸ªä¸¤æ­¥çš„è¿‡ç¨‹ï¼š

1.  You immediately start pattern-matching and sorting to figure out which is the â€œcorrectâ€ side.  
    
    ä½ ç«‹å³å¼€å§‹æ¨¡å¼åŒ¹é…å’Œæ’åºï¼Œä»¥æ‰¾å‡ºå“ªä¸ªæ˜¯ "æ­£ç¡® "çš„ä¸€é¢ã€‚  
    
    This tribal sorting is parallel to and even part of the process of educating yourself about the issue itself.  
    
    è¿™ç§éƒ¨è½æ•´ç†ä¸æ•™è‚²è‡ªå·±äº†è§£é—®é¢˜æœ¬èº«çš„è¿‡ç¨‹æ˜¯å¹³è¡Œçš„ï¼Œç”šè‡³æ˜¯ä¸€éƒ¨åˆ†ã€‚
    
2.  But AI safety doesnâ€™t easily map onto most existing culture war divides, and once you grasp this then you face a choice:  
    
    ä½†æ˜¯ï¼Œäººå·¥æ™ºèƒ½å®‰å…¨å¹¶ä¸å®¹æ˜“æ˜ å°„åˆ°å¤§å¤šæ•°ç°æœ‰çš„æ–‡åŒ–æˆ˜äº‰é¸¿æ²Ÿä¸­ï¼Œä¸€æ—¦ä½ æŒæ¡äº†è¿™ä¸€ç‚¹ï¼Œä½ å°±ä¼šé¢ä¸´ä¸€ä¸ªé€‰æ‹©ï¼š
    
    1.  Re-sort yourself and your network along brand-new AI safety lines.  
        
        æŒ‰ç…§å…¨æ–°çš„äººå·¥æ™ºèƒ½å®‰å…¨çº¿å¯¹è‡ªå·±å’Œè‡ªå·±çš„ç½‘ç»œè¿›è¡Œé‡æ–°æ’åºã€‚
        
    2.  Tweet through it!  
        
        é€šè¿‡å®ƒå‘å¾®åš!
        

In the ethnography portion of this piece, Iâ€™ll cover people whoâ€™ve picked each path of the decision fork above, i.e. a re-sort or just white-knuckled clinging to existing culture war frames.  

åœ¨è¿™ç¯‡æ–‡ç« çš„äººç§å­¦éƒ¨åˆ†ï¼Œæˆ‘å°†æ¶µç›–é‚£äº›é€‰æ‹©äº†ä¸Šè¿°å†³å®šå‰çš„æ¯ä¸€æ¡é“è·¯çš„äººï¼Œå³é‡æ–°åˆ†ç±»æˆ–åªæ˜¯ç™½ç™½åœ°åšæŒç°æœ‰çš„æ–‡åŒ–æˆ˜äº‰æ¡†æ¶ã€‚  

Iâ€™ll also describe some people who are still stuck on step 1.  

æˆ‘ä¹Ÿä¼šæè¿°ä¸€äº›ä»åœç•™åœ¨ç¬¬1æ­¥çš„äººã€‚

But most of the action in AI safety â€” and most of the confusion â€” is taking place in circles that have re-sorted along new and explicitly AI-safety-focused lines.  

ä½†æ˜¯ï¼Œäººå·¥æ™ºèƒ½å®‰å…¨æ–¹é¢çš„å¤§éƒ¨åˆ†è¡ŒåŠ¨--ä»¥åŠå¤§éƒ¨åˆ†æ··ä¹±--éƒ½å‘ç”Ÿåœ¨æ²¿ç€æ–°çš„ã€æ˜ç¡®ä»¥äººå·¥æ™ºèƒ½å®‰å…¨ä¸ºé‡ç‚¹çš„è·¯çº¿é‡æ–°æ’åºçš„åœˆå­é‡Œã€‚  

These re-sorted types are the ones whoâ€™ve signed the letter and are signal-boosting it on Twitter.  

è¿™äº›é‡æ–°åˆ†ç±»çš„äººå°±æ˜¯é‚£äº›ç­¾ç½²äº†è¿™å°ä¿¡å¹¶åœ¨æ¨ç‰¹ä¸Šå‘å‡ºä¿¡å·çš„äººã€‚

âœ‚ï¸ In respect of how many in existing groups are sorting themselves along â€œAI safetyâ€ lines, itâ€™s clear that the whole concept is rapidly emerging as a powerful [scissor](https://slatestarcodex.com/2018/10/30/sort-by-controversial/) â€” a statement or meme that divides a tribe of people into strongly pro and strongly con.  

At its most basic level, a scissor is a spectacle that is taken in by the primitive parts of the human brain â€” specifically the parts that are wired to pre-cognitively sortÂ **friend from foe**.Â   

âœ‚ï¸ å°±ç°æœ‰å›¢ä½“ä¸­çš„è®¸å¤šäººæ­£åœ¨æŒ‰ç…§ "äººå·¥æ™ºèƒ½å®‰å…¨ "çš„è·¯çº¿è¿›è¡Œåˆ†ç±»è€Œè¨€ï¼Œå¾ˆæ˜æ˜¾ï¼Œæ•´ä¸ªæ¦‚å¿µæ­£åœ¨è¿…é€Ÿæˆä¸ºä¸€æŠŠå¼ºå¤§çš„å‰ªåˆ€--ä¸€ç§å°†ä¸€ä¸ªéƒ¨è½çš„äººåˆ†ä¸ºå¼ºçƒˆæ”¯æŒå’Œå¼ºçƒˆåå¯¹çš„å£°æ˜æˆ–å¤‡å¿˜å½•ã€‚åœ¨å…¶æœ€åŸºæœ¬çš„å±‚é¢ä¸Šï¼Œå‰ªåˆ€æ˜¯ä¸€ç§è¢«äººç±»å¤§è„‘çš„åŸå§‹éƒ¨åˆ†æ‰€æ¥å—çš„æ™¯è±¡--ç‰¹åˆ«æ˜¯é‚£äº›è¢«è¿æ¥åˆ°é¢„å…ˆè®¤çŸ¥çš„æ•Œæˆ‘åˆ†ç±»çš„éƒ¨åˆ†ã€‚

So letâ€™s dive into the letter and the technical issues first, so we can understand how this new, post-culture-war, AI-centered, friend-vs-foe division is shaping up.Â   

å› æ­¤ï¼Œè®©æˆ‘ä»¬å…ˆæ·±å…¥äº†è§£è¿™å°ä¿¡å’ŒæŠ€æœ¯é—®é¢˜ï¼Œè¿™æ ·æˆ‘ä»¬å°±èƒ½äº†è§£è¿™ä¸ªæ–°çš„ã€æ–‡åŒ–æˆ˜äº‰åçš„ã€ä»¥äººå·¥æ™ºèƒ½ä¸ºä¸­å¿ƒçš„ã€æœ‹å‹ä¸æ•Œäººçš„åˆ’åˆ†æ˜¯å¦‚ä½•å½¢æˆçš„ã€‚

The AI open letter falls into the same trap as all open letters: the moment you start asking for enough specifics to make the recommendation actionable, the cracks start to show and the whole project starts looking like a publicity stunt.  

äººå·¥æ™ºèƒ½å…¬å¼€ä¿¡å’Œæ‰€æœ‰çš„å…¬å¼€ä¿¡ä¸€æ ·ï¼Œè½å…¥äº†åŒæ ·çš„é™·é˜±ï¼šå½“ä½ å¼€å§‹è¦æ±‚è¶³å¤Ÿå¤šçš„ç»†èŠ‚ä»¥ä½¿å»ºè®®å¯æ“ä½œæ—¶ï¼Œè£‚ç¼å¼€å§‹æ˜¾ç°ï¼Œæ•´ä¸ªé¡¹ç›®å¼€å§‹çœ‹èµ·æ¥åƒä¸€ä¸ªå®£ä¼ å™±å¤´ã€‚

Take a look a the money quote of the AI letter â€” the big ask:  

è¯·çœ‹ä¸€ä¸‹AIä¿¡ä¸­çš„ä¸€å¥è¯--å¤§è¦æ±‚ï¼š

> _Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors.  
> 
> If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.  
> 
> å› æ­¤ï¼Œæˆ‘ä»¬å‘¼åæ‰€æœ‰äººå·¥æ™ºèƒ½å®éªŒå®¤ç«‹å³æš‚åœæ¯”GPT-4æ›´å¼ºå¤§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„è®­ç»ƒï¼Œè‡³å°‘æš‚åœ6ä¸ªæœˆã€‚è¿™ç§æš‚åœåº”è¯¥æ˜¯å…¬å¼€çš„ã€å¯æ ¸æŸ¥çš„ï¼Œå¹¶åŒ…æ‹¬æ‰€æœ‰çš„å…³é”®è¡Œä¸ºè€…ã€‚å¦‚æœè¿™ç§æš‚åœä¸èƒ½è¿…é€Ÿé¢å¸ƒï¼Œæ”¿åºœåº”è¯¥ä»‹å…¥å¹¶åˆ¶å®šæš‚åœä»¤ã€‚_

The whole AI safety problem is contained in the phrase, â€œAI systems more powerful than GPT-4.â€ WhatÂ _exactly_Â does this mean, though?  

æ•´ä¸ªäººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜åŒ…å«åœ¨ "æ¯”GPT-4æ›´å¼ºå¤§çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿ "è¿™å¥è¯ä¸­ã€‚ä¸è¿‡ï¼Œè¿™åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

**âœ‹ Hold up:**Â I know what you think is coming next because itâ€™s a standard culture war pattern. We all do it.  

ç­‰ç­‰ï¼šæˆ‘çŸ¥é“ä½ è®¤ä¸ºæ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ–‡åŒ–æˆ˜äº‰æ¨¡å¼ã€‚æˆ‘ä»¬éƒ½ä¼šè¿™æ ·åšã€‚

-   If you come at me, a 2A supporter, with â€œban assault weapons now!â€ then I am gonna drop a 10-kiloton nuance bomb of deep, thorny definitional problems directly over your position, and watch to see if you scramble for cover.  
    
    å¦‚æœä½ å¯¹æˆ‘è¿™ä¸ª2Aæ”¯æŒè€…è¯´ï¼š"ç°åœ¨ç¦æ­¢æ”»å‡»æ€§æ­¦å™¨ï¼"é‚£ä¹ˆï¼Œæˆ‘å°†åœ¨ä½ çš„ä½ç½®ä¸Šç›´æ¥æŠ•ä¸‹ä¸€é¢—10åƒå¨çº§çš„ç»†å¾®å·®åˆ«ç‚¸å¼¹ï¼Œä»¥è§£å†³æ·±å±‚çš„ã€æ£˜æ‰‹çš„å®šä¹‰é—®é¢˜ï¼Œå¹¶çœ‹ä½ æ˜¯å¦äº‰ç›¸èº²é¿ã€‚
    
-   If I come at you, an LGBTQIA+ ally, with â€œa woman is an adult human female,â€ you are gonna drop a 10-kiloton nuance bomb of deep, thorny definitional problems directly over my position, and watch to see if I scramble for cover.  
    
    å¦‚æœæˆ‘å¯¹ä½ è¿™ä¸ªLGBTQIA+çš„ç›Ÿå‹è¯´ "å¥³äººæ˜¯ä¸€ä¸ªæˆå¹´å¥³æ€§"ï¼Œä½ å°±ä¼šåœ¨æˆ‘çš„ä½ç½®ä¸Šç›´æ¥æŠ•ä¸‹ä¸€é¢—10å¨é‡çš„ç»†å¾®å·®åˆ«ç‚¸å¼¹ï¼Œå…¶ä¸­åŒ…å«æ·±åˆ»ã€æ£˜æ‰‹çš„å®šä¹‰é—®é¢˜ï¼Œç„¶åçœ‹æˆ‘æ˜¯å¦ä¼šæ…Œå¿™èº²é¿ã€‚
    

But reader, I promise you that I am not about to deploy definitional nuance in order to win an argument.  

ä½†è¯»è€…ï¼Œæˆ‘å‘ä½ ä¿è¯ï¼Œæˆ‘ä¸ä¼šä¸ºäº†èµ¢å¾—äº‰è®ºè€Œéƒ¨ç½²å®šä¹‰ä¸Šçš„ç»†å¾®å·®åˆ«ã€‚  

Rather, I will (hopefully) illustrate that the technical, measurable definition of â€œprogressâ€ in AI â€” and even more narrowly, GPT-4â€™s progress over GPT-3 â€” is the whole ballgame.Â   

ç›¸åï¼Œæˆ‘å°†ï¼ˆå¸Œæœ›ï¼‰è¯´æ˜ï¼Œäººå·¥æ™ºèƒ½ä¸­ "è¿›æ­¥ "çš„æŠ€æœ¯æ€§ã€å¯è¡¡é‡çš„å®šä¹‰--ç”šè‡³æ›´ç‹­ä¹‰åœ°è¯´ï¼ŒGPT-4æ¯”GPT-3çš„è¿›æ­¥--æ˜¯æ•´ä¸ªçƒèµ›ã€‚

**To summarize** the points Iâ€™ll make in this section:  

æ€»ç»“ä¸€ä¸‹æˆ‘åœ¨æœ¬èŠ‚ä¸­è¦æå‡ºçš„è§‚ç‚¹ï¼š

1.  There is no â€œI know it when I see itâ€ position for any side of this issue to fall back to becauseÂ **nobody has ever actually seen the â€œitâ€**Â in question!  
    
    è¿™ä¸ªé—®é¢˜çš„ä»»ä½•ä¸€æ–¹éƒ½æ²¡æœ‰ "æˆ‘çœ‹åˆ°å®ƒå°±çŸ¥é“ "çš„ç«‹åœºï¼Œå› ä¸ºæ²¡æœ‰äººçœŸæ­£çœ‹åˆ°è¿‡æœ‰å…³çš„ "å®ƒ"ï¼
    
2.  Sam Altman canâ€™tÂ **even boil down the main intelligence improvements**Â from GPT-3 to GPT-4, apart from pointing to the modelsâ€™ performance on some quizzes and tests that are themselves hotly contested as signifiers of human potential in our current political climate.  
    
    è¨å§†-å¥¥ç‰¹æ›¼ç”šè‡³æ— æ³•å½’çº³å‡ºä»GPT-3åˆ°GPT-4çš„ä¸»è¦æ™ºåŠ›æ”¹è¿›ï¼Œé™¤äº†æŒ‡å‡ºæ¨¡å‹åœ¨ä¸€äº›æµ‹éªŒå’Œæµ‹è¯•ä¸­çš„è¡¨ç°ï¼Œè€Œè¿™äº›æµ‹éªŒå’Œæµ‹è¯•æœ¬èº«åœ¨æˆ‘ä»¬ç›®å‰çš„æ”¿æ²»æ°”å€™ä¸­ä½œä¸ºäººç±»æ½œåŠ›çš„æ ‡å¿—è€Œå—åˆ°æ¿€çƒˆçš„äº‰è®®ã€‚
    
3.  OpenAI as a company has opened up its evaluation tools because theyâ€™re so in the dark on this core question of what â€œmore powerfulâ€ looks like that they desperately need the rest of the world to help them come up with ways toÂ **measure the capability**Â of the systems theyâ€™re developing.  
    
    ä½œä¸ºä¸€å®¶å…¬å¸ï¼ŒOpenAIå·²ç»å¼€æ”¾äº†å®ƒçš„è¯„ä¼°å·¥å…·ï¼Œå› ä¸ºä»–ä»¬å¯¹ "æ›´å¼ºå¤§ "æ˜¯ä»€ä¹ˆæ ·å­è¿™ä¸ªæ ¸å¿ƒé—®é¢˜ä¸€æ— æ‰€çŸ¥ï¼Œä»–ä»¬è¿«åˆ‡éœ€è¦ä¸–ç•Œå…¶ä»–åœ°æ–¹å¸®åŠ©ä»–ä»¬æƒ³å‡ºè¡¡é‡ä»–ä»¬æ­£åœ¨å¼€å‘çš„ç³»ç»Ÿçš„èƒ½åŠ›çš„æ–¹æ³•ã€‚
    

ğŸ‘€ To reframe the first point above (â€œI know it when I see itâ€) using my two culture war examples: assault weapon ban promoters have all seen things they are certain are obviously â€œassault weapons,â€ and gender criticals have all seen people they are certain are obviously â€œwomen,â€ _but not a single soul opining about â€œAI safetyâ€ has ever once encountered the thing theyâ€™re worried about us bringing into existence in the near or medium term_.  

ç”¨æˆ‘çš„ä¸¤ä¸ªæ–‡åŒ–æˆ˜äº‰çš„ä¾‹å­æ¥é‡æ„ä¸Šé¢çš„ç¬¬ä¸€ç‚¹ï¼ˆ"æˆ‘ä¸€çœ‹å°±çŸ¥é“"ï¼‰ï¼šæ”»å‡»æ€§æ­¦å™¨ç¦ä»¤çš„å€¡å¯¼è€…éƒ½è§è¿‡ä»–ä»¬è‚¯å®šæ˜¯ "æ”»å‡»æ€§æ­¦å™¨ "çš„ä¸œè¥¿ï¼Œè€Œæ€§åˆ«æ‰¹è¯„è€…éƒ½è§è¿‡ä»–ä»¬è‚¯å®šæ˜¯ "å¥³æ€§ "çš„äººï¼Œä½†æ²¡æœ‰ä¸€ä¸ªäººåœ¨è¯„è®º "äººå·¥æ™ºèƒ½å®‰å…¨ "æ—¶æ›¾ç»é‡åˆ°è¿‡ä»–ä»¬æ‹…å¿ƒæˆ‘ä»¬åœ¨è¿‘æœŸæˆ–ä¸­æœŸå†…å¸¦æ¥çš„ä¸œè¥¿ã€‚

As a scissor, then, â€œAI safetyâ€ is unique in that it requires two separate cognitive moves:  

é‚£ä¹ˆï¼Œä½œä¸ºä¸€æŠŠå‰ªåˆ€ï¼Œ"AIå®‰å…¨ "çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºï¼Œå®ƒéœ€è¦ä¸¤ä¸ªç‹¬ç«‹çš„è®¤çŸ¥åŠ¨ä½œï¼š

1.  **Imagine in your mind** what â€œAGIâ€ would or possibly could look like.  
    
    åœ¨ä½ çš„è„‘æµ·ä¸­æƒ³è±¡ä¸€ä¸‹ "AGI "ä¼šæˆ–å¯èƒ½ä¼šæ˜¯ä»€ä¹ˆæ ·å­ã€‚
    
2.  Handoff that mental picture to the primitive part of your brain to **pattern match** as â€œfriendâ€ or â€œfoe.â€  
    
    å°†è¿™ä¸€å¿ƒç†å›¾æ™¯ç§»äº¤ç»™ä½ å¤§è„‘çš„åŸå§‹éƒ¨åˆ†ï¼Œä»¥æ¨¡å¼åŒ¹é…ä¸º "æœ‹å‹ "æˆ– "æ•Œäºº"ã€‚
    

Given that this is how it is, I have to confess to quite a bit of sympathy for the anti-â€œtechbroâ€ and anti-â€œAI hypeâ€ tendency to mock those expressing safety concerns as boys sitting around the campfire with flashlights in their faces, creeping each other out with made-up ghost stories.  

é‰´äºæƒ…å†µå°±æ˜¯è¿™æ ·ï¼Œæˆ‘ä¸å¾—ä¸æ‰¿è®¤ï¼Œæˆ‘ç›¸å½“åŒæƒ…å "æŠ€æœ¯å…„å¼Ÿ "å’Œå "äººå·¥æ™ºèƒ½ç‚’ä½œ "çš„å€¾å‘ï¼Œå˜²ç¬‘é‚£äº›è¡¨è¾¾å®‰å…¨å…³åˆ‡çš„äººæ˜¯å›´ååœ¨ç¯ç«æ—ï¼Œç”¨æ‰‹ç”µç­’ç…§ç€è‡ªå·±çš„è„¸ï¼Œç”¨ç¼–é€ çš„é¬¼æ•…äº‹è®©å¯¹æ–¹æ¯›éª¨æ‚šç„¶ã€‚  

It kinda does look like this!  

å®ƒç¡®å®æœ‰ç‚¹åƒè¿™æ ·!

Even worse is the tendency among so many AI safetyists to analogize the threat theyâ€™ve imagined in their minds to the threat posed by nuclear weapons.  

It happens all the time and it never, ever convinces anyone who isnâ€™t already convinced. I almost want to propose **Stokesâ€™s Law:** _as an online discussion of AI safety grows longer, the probability of a comparison to nuclear weapons approaches 1_.  

æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè®¸å¤šäººå·¥æ™ºèƒ½å®‰å…¨ä¸»ä¹‰è€…å€¾å‘äºå°†ä»–ä»¬è„‘æµ·ä¸­æƒ³è±¡çš„å¨èƒç±»æ¯”ä¸ºæ ¸æ­¦å™¨å¸¦æ¥çš„å¨èƒã€‚è¿™ç§æƒ…å†µä¸€ç›´åœ¨å‘ç”Ÿï¼Œè€Œä¸”ä»æ¥æ²¡æœ‰è¯´æœè¿‡ä»»ä½•è¿˜æ²¡æœ‰è¢«è¯´æœçš„äººã€‚æˆ‘å‡ ä¹æƒ³æå‡ºæ–¯æ‰˜å…‹æ–¯å®šå¾‹ï¼šéšç€å…³äºäººå·¥æ™ºèƒ½å®‰å…¨çš„åœ¨çº¿è®¨è®ºæ—¶é—´çš„å»¶é•¿ï¼Œä¸æ ¸æ­¦å™¨è¿›è¡Œæ¯”è¾ƒçš„æ¦‚ç‡æ¥è¿‘1ã€‚

It always goes something like this:  

å®ƒæ€»æ˜¯åƒè¿™æ ·ï¼š

> **X-risker:**Â This jailbroken chatbot just gave me detailed instructions for robbing a bank! Now imagine a more powerful one giving out detailed instructions for making a nuclear bomb.  
> 
> X-risker:è¿™ä¸ªè¶Šç‹±çš„èŠå¤©æœºå™¨äººåˆšåˆšç»™æˆ‘æä¾›äº†æŠ¢åŠ«é“¶è¡Œçš„è¯¦ç»†è¯´æ˜!ç°åœ¨æƒ³è±¡ä¸€ä¸‹ï¼Œä¸€ä¸ªæ›´å¼ºå¤§çš„èŠå¤©æœºå™¨äººç»™å‡ºäº†åˆ¶é€ æ ¸å¼¹çš„è¯¦ç»†è¯´æ˜ã€‚
> 
> **Me:**Â Bruh, those bank robbery instructions are not real. You cannot literally become a successful bank robber by reading those instructions then doing them. Please calm down.  
> 
> æˆ‘ï¼šå…„å¼Ÿï¼Œé‚£äº›é“¶è¡ŒæŠ¢åŠ«çš„è¯´æ˜ä¸æ˜¯çœŸçš„ã€‚ä½ ä¸å¯èƒ½é€šè¿‡é˜…è¯»è¿™äº›è¯´æ˜ç„¶åå»åšå°±çœŸçš„æˆä¸ºä¸€ä¸ªæˆåŠŸçš„é“¶è¡ŒåŠ«åŒªã€‚è¯·å†·é™ä¸‹æ¥ã€‚
> 
> **X-risker:**Â But imagine the instructionsÂ _were_Â real! A super powerful AI would be able to give very a very accurate, actionable plan for a bank robbery! And also for nukes! NU. CLE. AR. WEAPONS!  
> 
> X-risker:ä½†æƒ³è±¡ä¸€ä¸‹è¿™äº›æŒ‡ä»¤æ˜¯çœŸå®çš„ï¼ä¸€ä¸ªè¶…çº§å¼ºå¤§çš„äººå·¥æ™ºèƒ½å°†èƒ½å¤Ÿç»™å‡ºéå¸¸å‡†ç¡®çš„é“¶è¡ŒæŠ¢åŠ«è®¡åˆ’ï¼ä¸€ä¸ªè¶…çº§å¼ºå¤§çš„äººå·¥æ™ºèƒ½å°†èƒ½å¤Ÿä¸ºé“¶è¡ŒæŠ¢åŠ«æä¾›ä¸€ä¸ªéå¸¸å‡†ç¡®çš„ã€å¯æ“ä½œçš„è®¡åˆ’!è¿˜æœ‰æ ¸å¼¹!NU.CLEã€‚AR.æ­¦å™¨ï¼
> 
> **Me:**Â But Brossandra, if weâ€™re just imagining godlike AIs, why donâ€™t we imagine one that loves us and tells us how to live forever? Why imagine Dionysus and not, say, Apollo?  
> 
> æˆ‘ï¼šä½†æ˜¯å¸ƒç½—æ¡‘å¾·æ‹‰ï¼Œå¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³è±¡ç¥ä¸€æ ·çš„äººå·¥æ™ºèƒ½ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸æƒ³è±¡ä¸€ä¸ªçˆ±æˆ‘ä»¬å¹¶å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•é•¿ç”Ÿä¸è€çš„äººå‘¢ï¼Ÿä¸ºä»€ä¹ˆè¦æƒ³è±¡ç‹„ä¿„å°¼ç´¢æ–¯ï¼Œè€Œä¸æ˜¯ï¼Œæ¯”å¦‚è¯´ï¼Œé˜¿æ³¢ç½—ï¼Ÿ
> 
> **X-risker:**Â How do you not understand that this is like nukes.?! Do you just think everyone should have a nuclear bomb?  
> 
> We have blue-ribbon committees and laws for nukes, so we need them for AGI because itâ€™s LIKE NUKES.  
> 
> X-risker:ä½ æ€ä¹ˆä¸æ˜ç™½ï¼Œè¿™å°±åƒæ ¸å¼¹ï¼ä½ åªæ˜¯è®¤ä¸ºæ¯ä¸ªäººéƒ½åº”è¯¥æœ‰ä¸€é¢—æ ¸å¼¹å—ï¼Ÿæˆ‘ä»¬å¯¹æ ¸å¼¹æœ‰è“å¸¦å§”å‘˜ä¼šå’Œæ³•å¾‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹AGIä¹Ÿéœ€è¦ï¼Œå› ä¸ºå®ƒå°±åƒæ ¸å¼¹ã€‚
> 
> **Me:**Â Eh, Iâ€™m not seeing it. Check out this thread of sick Snoop Dogg memes I made with AI. Anything that can do this is a force for good.  
> 
> æˆ‘:å—¯ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ã€‚çœ‹çœ‹æˆ‘ç”¨äººå·¥æ™ºèƒ½åˆ¶ä½œçš„è¿™ä¸ªå˜æ€çš„Snoop Doggå¤‡å¿˜å½•çš„ä¸»é¢˜ã€‚ä»»ä½•èƒ½åšåˆ°è¿™ä¸€ç‚¹çš„ä¸œè¥¿éƒ½æ˜¯ä¸€ç§å–„çš„åŠ›é‡ã€‚

And around it goes, with both sides of this conversation doing friend-or-foe pattern matching on entirely different targets â€” the safetyist is pattern-matching on some viral ChatGPT bank robbery thread, and Iâ€™m pattern-matching on [my Snoop thread](https://twitter.com/jonst0kes/status/1633593896184758272), and weâ€™re each free to pattern match on whatever we like thatâ€™s vaguely AI-related because there is no actually existing AGI to anchor our arguments in.  

ç»•æ¥ç»•å»ï¼Œè¿™æ¬¡è°ˆè¯çš„åŒæ–¹éƒ½åœ¨å¯¹å®Œå…¨ä¸åŒçš„ç›®æ ‡è¿›è¡Œæ•Œæˆ‘æ¨¡å¼åŒ¹é…--å®‰å…¨ä¸»ä¹‰è€…åœ¨å¯¹ä¸€äº›ç—…æ¯’æ€§çš„ChatGPTé“¶è¡ŒæŠ¢åŠ«çº¿ç¨‹è¿›è¡Œæ¨¡å¼åŒ¹é…ï¼Œè€Œæˆ‘åœ¨å¯¹æˆ‘çš„Snoopçº¿ç¨‹è¿›è¡Œæ¨¡å¼åŒ¹é…ï¼Œè€Œæˆ‘ä»¬æ¯ä¸ªäººéƒ½å¯ä»¥è‡ªç”±åœ°å¯¹ä»»ä½•æˆ‘ä»¬å–œæ¬¢çš„ã€ä¸äººå·¥æ™ºèƒ½éšçº¦ç›¸å…³çš„ä¸œè¥¿è¿›è¡Œæ¨¡å¼åŒ¹é…ï¼Œå› ä¸ºæ²¡æœ‰å®é™…å­˜åœ¨çš„AGIæ¥å›ºå®šæˆ‘ä»¬çš„è®ºç‚¹ã€‚

If you listen to Lex Fridmanâ€™s interview with OpenAI CEO Sam Altman, there are a number of exchanges where Lex tries to get Sam to detail the improvements GPT-4 brings to the table over GPT-3 or to characterize how powerful it is.  

å¦‚æœä½ å¬äº†Lex Fridmanå¯¹OpenAIé¦–å¸­æ‰§è¡Œå®˜Sam Altmançš„é‡‡è®¿ï¼Œåœ¨ä¸€äº›äº¤æµä¸­ï¼ŒLexè¯•å›¾è®©Samè¯¦ç»†ä»‹ç»GPT-4æ¯”GPT-3å¸¦æ¥çš„æ”¹è¿›ï¼Œæˆ–æè¿°å®ƒæœ‰å¤šå¼ºå¤§ã€‚

<iframe src="https://www.youtube-nocookie.com/embed/L_Guz73e6fw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" data-immersive-translate-effect="1"></iframe>

Sam says a few things that are interesting and directly relevant to the question of defining what is and is not â€œmore powerful than GPT-4.â€Â   

è¨å§†è¯´äº†å‡ å¥è¯ï¼Œè¿™äº›è¯å¾ˆæœ‰æ„æ€ï¼Œä¸å®šä¹‰ä»€ä¹ˆæ˜¯å’Œä»€ä¹ˆä¸æ˜¯ "æ¯”GPT-4æ›´å¼ºå¤§ "çš„é—®é¢˜ç›´æ¥ç›¸å…³ã€‚

1.  OpenAI did not release the parameter size for GPT-4 because that number, which everyone is fixated on, is not a straightforward **measure of model capability** anymore.  
    
    OpenAIæ²¡æœ‰å…¬å¸ƒGPT-4çš„å‚æ•°å¤§å°ï¼Œå› ä¸ºè¿™ä¸ªå¤§å®¶éƒ½å¾ˆçœ‹é‡çš„æ•°å­—å·²ç»ä¸æ˜¯è¡¡é‡æ¨¡å‹èƒ½åŠ›çš„ç›´æ¥æ ‡å‡†ã€‚
    
2.  OpenAI improved GPT-4 by making **many** **small improvements** in every phase â€” from dataset curation to training to fine-tuning to RLHF to inference â€” that all add up to big gains.  
    
    OpenAIé€šè¿‡åœ¨æ¯ä¸ªé˜¶æ®µè¿›è¡Œè®¸å¤šå°çš„æ”¹è¿›æ¥æ”¹è¿›GPT-4--ä»æ•°æ®é›†ç­–åˆ’åˆ°è®­ç»ƒåˆ°å¾®è°ƒåˆ°RLHFåˆ°æ¨ç†--æ‰€æœ‰è¿™äº›åŠ èµ·æ¥å°±æ˜¯å¤§çš„æ”¶ç›Šã€‚
    
3.  Weâ€™re all still discovering what those â€œbig gainsâ€ are, which is why OpenAI is committed to letting as many people as possible use the model as early as possible.  
    
    Measuring the full impact of all those additive (or multiplicative?) gains is a **large-scale, group effort**.  
    
    æˆ‘ä»¬éƒ½è¿˜åœ¨å‘ç°é‚£äº› "å¤§æ”¶ç›Š "æ˜¯ä»€ä¹ˆï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆOpenAIè‡´åŠ›äºè®©å°½å¯èƒ½å¤šçš„äººå°½æ—©ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚è¡¡é‡æ‰€æœ‰è¿™äº›åŠ æ³•ï¼ˆæˆ–ä¹˜æ³•ï¼Ÿï¼‰æ”¶ç›Šçš„å…¨éƒ¨å½±å“æ˜¯ä¸€é¡¹å¤§è§„æ¨¡çš„ã€é›†ä½“çš„åŠªåŠ›ã€‚
    
4.  In service of #3 above, OpenAI has **opened up its model evaluation tools** and is asking the public to help it develop ways to measure the performance differences between GPT-4 and other models on different types of tasks.  
    
    ä¸ºäº†æœåŠ¡äºä¸Šè¿°ç¬¬3ç‚¹ï¼ŒOpenAIå·²ç»å¼€æ”¾äº†å…¶æ¨¡å‹è¯„ä¼°å·¥å…·ï¼Œå¹¶è¦æ±‚å…¬ä¼—å¸®åŠ©å…¶å¼€å‘æ–¹æ³•æ¥è¡¡é‡GPT-4å’Œå…¶ä»–æ¨¡å‹åœ¨ä¸åŒç±»å‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚ã€‚
    

Do you see what a hot mess this whole thing is for anyone who wants to â€œpress pauseâ€ on a narrow type of AI research â€” so-called â€œgain-of-functionâ€ research aimed at making the models more â€œpowerfulâ€ â€” but not all of it?  

å¯¹äºé‚£äº›æƒ³è¦ "æš‚åœ "ä¸€ç§ç‹­ä¹‰çš„äººå·¥æ™ºèƒ½ç ”ç©¶--æ—¨åœ¨ä½¿æ¨¡å‹æ›´åŠ  "å¼ºå¤§ "çš„æ‰€è°“ "åŠŸèƒ½å¢ç›Š "ç ”ç©¶--è€Œä¸æ˜¯æ‰€æœ‰çš„ç ”ç©¶çš„äººæ¥è¯´ï¼Œä½ æ˜¯å¦çœ‹åˆ°è¿™æ•´ä»¶äº‹æ˜¯å¤šä¹ˆæ£˜æ‰‹çš„éº»çƒ¦ï¼Ÿ

1ï¸âƒ£ We canâ€™t put a straightforward, hard limit on AI research based strictly on the number of parameters and/or GPUs used to train models, because there are other ways to get big jumps in capability apart from scale.  

1ï¸âƒ£æˆ‘ä»¬ä¸èƒ½ä¸¥æ ¼æ ¹æ®ç”¨äºè®­ç»ƒæ¨¡å‹çš„å‚æ•°å’Œ/æˆ–GPUçš„æ•°é‡ï¼Œå¯¹äººå·¥æ™ºèƒ½ç ”ç©¶è¿›è¡Œç›´æ¥çš„ç¡¬æ€§é™åˆ¶ï¼Œå› ä¸ºé™¤äº†è§„æ¨¡ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥è·å¾—èƒ½åŠ›ä¸Šçš„å¤§è·ƒè¿›ã€‚

2ï¸âƒ£ Furthermore, if we did outlaw purely scale-based approaches, then we must be 100 percent sure the law is enforced **globally with zero exceptions**.  

Because in the regions with anti-scaling laws, researchers will uncover more ways to advance without relying on scale, and when those new discoveries are transferred to a region where scaling is still being done they could **contribute to a fast take-off scenario**.  

2ï¸âƒ£æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬çœŸçš„å°†çº¯ç²¹çš„åŸºäºè§„æ¨¡çš„æ–¹æ³•åˆ—ä¸ºéæ³•ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¿…é¡»ç™¾åˆ†ä¹‹ç™¾åœ°ç¡®ä¿è¯¥æ³•å¾‹åœ¨å…¨çƒèŒƒå›´å†…æ‰§è¡Œï¼Œæ²¡æœ‰ä¾‹å¤–ã€‚å› ä¸ºåœ¨æœ‰åè§„æ¨¡æ³•çš„åœ°åŒºï¼Œç ”ç©¶äººå‘˜ä¼šå‘ç°æ›´å¤šä¸ä¾èµ–è§„æ¨¡çš„è¿›æ­¥æ–¹å¼ï¼Œè€Œå½“è¿™äº›æ–°å‘ç°è¢«è½¬ç§»åˆ°ä»åœ¨è¿›è¡Œè§„æ¨¡åŒ–çš„åœ°åŒºæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šä¿ƒè¿›å¿«é€Ÿèµ·é£çš„æƒ…å†µã€‚

**Some definitions**Â for the uninitiated:  

ä¸ºä¸ç†Ÿæ‚‰çš„äººæä¾›ä¸€äº›å®šä¹‰ï¼š

-   **Fast take-off**Â scenarios are where something happens and we suddenly go from the present state of the art to a godlike AGI, with no time to prepare in anyway way.  
    
    å¿«é€Ÿèµ·é£çš„æƒ…å†µæ˜¯ï¼Œå‘ç”Ÿäº†ä¸€äº›äº‹æƒ…ï¼Œæˆ‘ä»¬çªç„¶ä»ç›®å‰çš„æŠ€æœ¯çŠ¶æ€å˜æˆäº†ä¸€ä¸ªç¥ä¸€æ ·çš„AGIï¼Œæ²¡æœ‰æ—¶é—´ä»¥ä»»ä½•æ–¹å¼å‡†å¤‡ã€‚
    
-   **Slow take-off**, in contrast, is where progress is steady and predictable for some lengthy period of time, and weâ€™re able to collectively adjust our society to new AI capabilities as they open up.  
    
    ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ…¢é€Ÿèµ·é£æ˜¯æŒ‡åœ¨ä¸€æ®µè¾ƒé•¿çš„æ—¶é—´å†…ï¼Œè¿›å±•æ˜¯ç¨³å®šå’Œå¯é¢„æµ‹çš„ï¼Œæˆ‘ä»¬èƒ½å¤Ÿé›†ä½“è°ƒæ•´æˆ‘ä»¬çš„ç¤¾ä¼šï¼Œä»¥é€‚åº”æ–°çš„äººå·¥æ™ºèƒ½èƒ½åŠ›ï¼Œå› ä¸ºå®ƒä»¬å¼€æ”¾äº†ã€‚
    

3ï¸âƒ£ And how would we keep researchers from making small, incremental advances in capability, advances of the type that could be combined together to get one big advance?  

3ï¸âƒ£è€Œæˆ‘ä»¬å¦‚ä½•é˜²æ­¢ç ”ç©¶äººå‘˜åœ¨èƒ½åŠ›æ–¹é¢å–å¾—å°çš„ã€æ¸è¿›çš„è¿›å±•ï¼Œè¿™äº›è¿›å±•å¯ä»¥ç»“åˆåœ¨ä¸€èµ·ï¼Œè·å¾—ä¸€ä¸ªå¤§çš„è¿›å±•ï¼Ÿ  

There is no way to regulate this kind of activity, at least that isnâ€™t massively invasive.  

æ²¡æœ‰åŠæ³•ç›‘ç®¡è¿™ç§æ´»åŠ¨ï¼Œè‡³å°‘æ²¡æœ‰å¤§è§„æ¨¡çš„å…¥ä¾µæ€§ã€‚

4ï¸âƒ£ Finally, weâ€™re still trying to understand how much more capable GPT-4 is than GPT-3, and at what kinds of tasks.  

The red-teaming and testing OpenAI has been doing since the model was finished this summer was **just the beginning** of that process.  

4ï¸âƒ£æœ€åï¼Œæˆ‘ä»¬ä»åœ¨è¯•å›¾äº†è§£GPT-4æ¯”GPT-3çš„èƒ½åŠ›æœ‰å¤šå¤§ï¼Œä»¥åŠåœ¨ä»€ä¹ˆæ ·çš„ä»»åŠ¡ä¸Šã€‚è‡ªä»Šå¹´å¤å¤©æ¨¡å‹å®Œæˆä»¥æ¥ï¼ŒOpenAIä¸€ç›´åœ¨è¿›è¡Œçš„çº¢é˜Ÿå’Œæµ‹è¯•åªæ˜¯è¿™ä¸ªè¿‡ç¨‹çš„å¼€å§‹ã€‚

Whatâ€™s happening right now, with hundreds of millions of users poking at the model posting screenshots, or otherwise giving feedback, is the **next phase** of that discovery and measurement process. Thatâ€™s whatâ€™s going on withÂ [OpenAI Evals](https://github.com/openai/evals)Â â€” they want the public to contribute high-quality capability measurements.  

ç°åœ¨æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œéšç€æ•°ä»¥äº¿è®¡çš„ç”¨æˆ·å¯¹æ¨¡å‹çš„æ¢ç©¶ï¼Œå‘å¸ƒæˆªå›¾ï¼Œæˆ–ä»¥å…¶ä»–æ–¹å¼æä¾›åé¦ˆï¼Œæ˜¯è¯¥å‘ç°å’Œæµ‹é‡è¿‡ç¨‹çš„ä¸‹ä¸€ä¸ªé˜¶æ®µã€‚è¿™å°±æ˜¯OpenAI Evalsæ­£åœ¨è¿›è¡Œçš„å·¥ä½œ--ä»–ä»¬å¸Œæœ›å…¬ä¼—èƒ½å¤Ÿè´¡çŒ®é«˜è´¨é‡çš„èƒ½åŠ›æµ‹é‡ã€‚

â¡ï¸ Ultimately, the letterâ€™s central demand boils down to a demand that _we stop making AIs more powerful until we can figure out how to measure if the AIs are becoming more powerful_. Itâ€™s like saying, â€œstop making cars that can go faster until we can figure out what speed is and how to measure it.â€ The only way this demand is not completely absurd is if itâ€™s a demand to pauseÂ _all_Â development.  

â¡ï¸ å½’æ ¹ç»“åº•ï¼Œè¿™å°ä¿¡çš„æ ¸å¿ƒè¦æ±‚å½’ç»“ä¸ºæˆ‘ä»¬åœæ­¢åˆ¶é€ æ›´å¼ºå¤§çš„äººå·¥æ™ºèƒ½ï¼Œç›´åˆ°æˆ‘ä»¬èƒ½å¤Ÿå¼„æ¸…æ¥šå¦‚ä½•è¡¡é‡äººå·¥æ™ºèƒ½æ˜¯å¦å˜å¾—æ›´å¼ºå¤§ã€‚è¿™å°±å¥½æ¯”è¯´ï¼Œ"åœ¨æˆ‘ä»¬èƒ½å¤Ÿå¼„æ¸…æ¥šä»€ä¹ˆæ˜¯é€Ÿåº¦ä»¥åŠå¦‚ä½•æµ‹é‡å®ƒä¹‹å‰ï¼Œåœæ­¢åˆ¶é€ å¯ä»¥è·‘å¾—æ›´å¿«çš„æ±½è½¦ã€‚"è¿™ä¸ªè¦æ±‚ä¸å®Œå…¨è’è°¬çš„å”¯ä¸€æ–¹æ³•æ˜¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªæš‚åœå‘å±•çš„è¦æ±‚ã€‚

You might counter by insisting that itâ€™s just a demand to stop explicitly trying to improve AI capabilities and just focus on explainability and measurement.  

But my brother in technology: explainability and measurement areÂ _critical_Â to gain-of-function work. As the meme says, theyâ€™re the same picture.  

ä½ å¯èƒ½ä¼šåé©³è¯´ï¼Œè¿™åªæ˜¯è¦æ±‚åœæ­¢æ˜ç¡®åœ°è¯•å›¾æé«˜äººå·¥æ™ºèƒ½çš„èƒ½åŠ›ï¼Œè€Œåªæ˜¯ä¸“æ³¨äºå¯è§£é‡Šæ€§å’Œæµ‹é‡ã€‚ä½†æˆ‘çš„æŠ€æœ¯å…„å¼Ÿï¼šå¯è§£é‡Šæ€§å’Œæµ‹é‡å¯¹äºåŠŸèƒ½å¢ç›Šçš„å·¥ä½œæ˜¯è‡³å…³é‡è¦çš„ã€‚æ­£å¦‚å¤‡å¿˜å½•æ‰€è¯´ï¼Œå®ƒä»¬æ˜¯åŒä¸€å¹…ç”»ã€‚

At any rate, the letter is not, in fact, a call to pause all development, and is exactly as self-contradictory as my automotive analogy suggests.  

æ— è®ºå¦‚ä½•ï¼Œè¿™å°ä¿¡äº‹å®ä¸Šå¹¶ä¸æ˜¯å‘¼åæš‚åœæ‰€æœ‰çš„å‘å±•ï¼Œè€Œä¸”æ­£å¦‚æˆ‘çš„æ±½è½¦ç±»æ¯”æ‰€æš—ç¤ºçš„é‚£æ ·ï¼Œå®Œå…¨æ˜¯è‡ªç›¸çŸ›ç›¾çš„ã€‚

ğŸš” After asking for a pause in gain-of-function work, it then goes on to suggest that we institute a regime of inspection, testing, and reporting to verify quantitatively that as everyone is doing their work on the cars nobody is secretly doing the bad thing of making the cars go faster â€” again, all in the absence of a widely agreed upon definition of speed, much less a means of testing it.Â   

ğŸš”åœ¨è¦æ±‚æš‚åœåŠŸèƒ½å¢ç›Šå·¥ä½œä¹‹åï¼Œå®ƒåˆå»ºè®®æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªæ£€æŸ¥ã€æµ‹è¯•å’ŒæŠ¥å‘Šåˆ¶åº¦ï¼Œä»¥å®šé‡åœ°æ ¸å®æ¯ä¸ªäººéƒ½åœ¨å¯¹æ±½è½¦è¿›è¡Œå·¥ä½œï¼Œæ²¡æœ‰äººæš—ä¸­åšåäº‹ï¼Œä½¿æ±½è½¦è·‘å¾—æ›´å¿«--åŒæ ·ï¼Œæ‰€æœ‰è¿™äº›éƒ½ç¼ºä¹ä¸€ä¸ªå¹¿æ³›è®¤åŒçš„é€Ÿåº¦å®šä¹‰ï¼Œæ›´æ²¡æœ‰ä¸€ä¸ªæµ‹è¯•çš„æ‰‹æ®µã€‚

> _In parallel, AI developers must work with policymakers to dramatically accelerate development of robust AI governance systems. These should at a minimum include: new and capable regulatory authorities dedicated to AI; oversight and tracking of highly capable AI systems and large pools of computational capability; provenance and watermarking systems to help distinguish real from synthetic and to track model leaks; a robust auditing and certification ecosystem; liability for AI-caused harm; robust public funding for technical AI safety research; and well-resourced institutions for coping with the dramatic economic and political disruptions (especially to democracy) that AI will cause.  
> 
> åŒæ—¶ï¼Œäººå·¥æ™ºèƒ½å¼€å‘è€…å¿…é¡»ä¸æ”¿ç­–åˆ¶å®šè€…åˆä½œï¼Œå¤§å¹…åŠ å¿«å¼€å‘å¼ºå¤§çš„äººå·¥æ™ºèƒ½æ²»ç†ç³»ç»Ÿã€‚è¿™äº›è‡³å°‘åº”è¯¥åŒ…æ‹¬ï¼šä¸“é—¨é’ˆå¯¹äººå·¥æ™ºèƒ½çš„æ–°çš„å’Œæœ‰èƒ½åŠ›çš„ç›‘ç®¡æœºæ„ï¼›ç›‘ç£å’Œè·Ÿè¸ªé«˜èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ç³»ç»Ÿå’Œå¤§å‹è®¡ç®—èƒ½åŠ›æ± ï¼›å‡ºå¤„å’Œæ°´å°ç³»ç»Ÿï¼Œä»¥å¸®åŠ©åŒºåˆ†çœŸå®å’Œåˆæˆï¼Œå¹¶è·Ÿè¸ªæ¨¡å‹æ³„æ¼ï¼›å¼ºå¤§çš„å®¡è®¡å’Œè®¤è¯ç”Ÿæ€ç³»ç»Ÿï¼›å¯¹äººå·¥æ™ºèƒ½é€ æˆçš„ä¼¤å®³æ‰¿æ‹…è´£ä»»ï¼›ä¸ºäººå·¥æ™ºèƒ½å®‰å…¨æŠ€æœ¯ç ”ç©¶æä¾›å¼ºå¤§çš„å…¬å…±èµ„é‡‘ï¼›ä»¥åŠèµ„æºä¸°å¯Œçš„æœºæ„ï¼Œä»¥åº”å¯¹äººå·¥æ™ºèƒ½å°†å¯¼è‡´çš„å·¨å¤§çš„ç»æµå’Œæ”¿æ²»ç ´åï¼ˆç‰¹åˆ«æ˜¯å¯¹æ°‘ä¸»ï¼‰ã€‚_

To use technical jargon: this is all pretty nuts. Just completely unserious, and Iâ€™m shocked at some of the names that signed onto it.  

ç”¨æŠ€æœ¯æœ¯è¯­æ¥è¯´ï¼šè¿™ä¸€åˆ‡éƒ½å¾ˆç–¯ç‹‚ã€‚åªæ˜¯å®Œå…¨ä¸ä¸¥è‚ƒï¼Œæˆ‘å¯¹ä¸€äº›ç­¾åçš„äººæ„Ÿåˆ°éœ‡æƒŠã€‚

In fact, after thinking this situation through, nowÂ _Iâ€™m_Â getting scared â€” the future of our species is apparently in the hands of a group of people who canâ€™t spot an obvious contradiction of this magnitude.  

Itâ€™s almost enough to make me look at that list of signatories and think, ironically, â€œthere go the techbros playing God, again!â€  

äº‹å®ä¸Šï¼Œåœ¨ä»”ç»†è€ƒè™‘äº†è¿™ç§æƒ…å†µåï¼Œç°åœ¨æˆ‘å¼€å§‹å®³æ€•äº†--æˆ‘ä»¬è¿™ä¸ªç‰©ç§çš„æœªæ¥æ˜¾ç„¶æŒæ¡åœ¨ä¸€ç¾¤äººæ‰‹ä¸­ï¼Œä»–ä»¬ä¸èƒ½å‘ç°è¿™ç§ç¨‹åº¦çš„æ˜æ˜¾çŸ›ç›¾ã€‚è¿™å‡ ä¹è¶³ä»¥è®©æˆ‘çœ‹ç€é‚£ä»½ç­¾ç½²è€…åå•ï¼Œå¹¶è®½åˆºåœ°æƒ³ï¼Œ"ç§‘æŠ€äººå‘˜åˆåœ¨æ‰®æ¼”ä¸Šå¸äº†ï¼"

But that would be a culture war frame, and as such itâ€™s one I think we should reject when grappling with this issue. Itâ€™s best to take AI safety on its own terms.  

ä½†è¿™å°†æ˜¯ä¸€ä¸ªæ–‡åŒ–æˆ˜äº‰çš„æ¡†æ¶ï¼Œå› æ­¤æˆ‘è®¤ä¸ºæˆ‘ä»¬åœ¨å¤„ç†è¿™ä¸ªé—®é¢˜æ—¶åº”è¯¥æ‹’ç»è¿™ä¸ªæ¡†æ¶ã€‚æœ€å¥½çš„åŠæ³•æ˜¯æ ¹æ®è‡ªå·±çš„æ¡ä»¶æ¥çœ‹å¾…äººå·¥æ™ºèƒ½çš„å®‰å…¨ã€‚

When trying to navigate the AI safety debate by mapping the technical issues described above to different tribal groups on my timeline, I find Iâ€™m constantly disoriented.  

å½“æˆ‘è¯•å›¾é€šè¿‡å°†ä¸Šè¿°æŠ€æœ¯é—®é¢˜æ˜ å°„åˆ°æˆ‘æ—¶é—´çº¿ä¸Šçš„ä¸åŒéƒ¨è½ç¾¤ä½“æ¥æµè§ˆäººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºæ—¶ï¼Œæˆ‘å‘ç°æˆ‘ä¸€ç›´åœ¨è¿·å¤±æ–¹å‘ã€‚  

So itâ€™s easy for me to see why outsiders are lost with this whole thing.Â   

æ‰€ä»¥æˆ‘å¾ˆå®¹æ˜“ç†è§£ä¸ºä»€ä¹ˆå¤–äººå¯¹è¿™æ•´ä¸ªäº‹æƒ…æ„Ÿåˆ°è¿·èŒ«ã€‚

A small but telling **recent example:** in this [NY Mag puff piece](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html) on self-styled â€œAI hypeâ€ debunker Emily Bender, the author thinks Sam Altman is an effective altruist but heâ€™s actually effective altruismâ€™s current number one villain.  

Iâ€™ve seen people make this same categorization mistake with Peter Thiel, a man who has [unironically characterized](https://www.youtube.com/watch?v=ibR_ULHYirs) EA guru Nick Bostrom as the antichrist.  

æœ€è¿‘æœ‰ä¸€ä¸ªå¾ˆå°ä½†å¾ˆæœ‰è¯´æœåŠ›çš„ä¾‹å­ï¼šåœ¨ã€Šçº½çº¦æ‚å¿—ã€‹å…³äºè‡ªç§° "äººå·¥æ™ºèƒ½ç‚’ä½œ "æ­ç§˜è€…è‰¾ç±³ä¸½-ç­å¾·çš„è¿™ç¯‡å¹æ§æ–‡ç« ä¸­ï¼Œä½œè€…è®¤ä¸ºè¨å§†-å¥¥ç‰¹æ›¼æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„åˆ©ä»–ä¸»ä¹‰è€…ï¼Œä½†ä»–å®é™…ä¸Šæ˜¯æœ‰æ•ˆåˆ©ä»–ä¸»ä¹‰çš„å¤´å·æ¶æ£ã€‚æˆ‘çœ‹åˆ°äººä»¬åœ¨å½¼å¾—-æ³°å°”èº«ä¸ŠçŠ¯äº†åŒæ ·çš„åˆ†ç±»é”™è¯¯ï¼Œè¿™ä¸ªäººæ›¾ä¸æ— è®½åˆºåœ°å°†EAå¤§å¸ˆå°¼å…‹-æ³¢æ–¯ç‰¹ç½—å§†å®šæ€§ä¸ºååŸºç£è€…ã€‚

ğŸ˜µğŸ’« But I do get why everyone is confused. The tribal signifiers on AI safety Twitter are all over the place.  

Just today, in fact, the rationalist, evopsych, gender critical scholar Geoffrey Miller wasÂ [backing woke AI ethicist Gary Marcus](https://twitter.com/primalpoly/status/1641148286521180173)Â in a thread on the AI letter, and in opposition to both was anÂ [Antifa, tankie, he/him account](https://twitter.com/tante/status/1641175492282793984)Â whoâ€™s worried about the anti-democratic implications of a technocrat-run AI control regimeâ€¦ and off to the side is me, an anti-woke, anti-tankie, pronoun disrespecter cheering he/him on because this aggression cannot stand, man.  

ğŸ˜µğŸ’« ä½†æˆ‘ç¡®å®æ˜ç™½ä¸ºä»€ä¹ˆå¤§å®¶éƒ½å¾ˆå›°æƒ‘ã€‚äººå·¥æ™ºèƒ½å®‰å…¨æ¨ç‰¹ä¸Šçš„éƒ¨è½æ ‡å¿—åˆ°å¤„éƒ½æ˜¯ã€‚äº‹å®ä¸Šï¼Œå°±åœ¨ä»Šå¤©ï¼Œç†æ€§ä¸»ä¹‰ã€è¿›åŒ–å¿ƒç†å­¦ã€æ€§åˆ«æ‰¹åˆ¤å­¦è€…æ°å¼—é‡Œ-ç±³å‹’ï¼ˆGeoffrey Millerï¼‰åœ¨å…³äºäººå·¥æ™ºèƒ½ä¿¡ä»¶çš„ä¸»é¢˜ä¸­æ”¯æŒæ¸…é†’çš„äººå·¥æ™ºèƒ½ä¼¦ç†å­¦å®¶åŠ é‡Œ-é©¬åº“æ–¯ï¼ˆGary Marcusï¼‰ï¼Œè€Œåå¯¹è¿™ä¸¤è€…çš„æ˜¯ä¸€ä¸ªåæ³•è¥¿æ–¯ã€å¦å…‹å…µã€ä»–/ä»–çš„è´¦æˆ·ï¼Œä»–æ‹…å¿ƒæŠ€æœ¯å®˜åƒšç®¡ç†çš„äººå·¥æ™ºèƒ½æ§åˆ¶åˆ¶åº¦çš„åæ°‘ä¸»å½±å“...è€Œåœ¨ä¸€è¾¹çš„æ˜¯æˆ‘ï¼Œä¸€ä¸ªåæ¸…é†’ã€åå¦å…‹å…µã€ä¸å°Šé‡äººç§°çš„äººï¼Œåœ¨ä¸ºä»–/ä»–åŠ æ²¹ï¼Œå› ä¸ºè¿™ç§ä¾µç•¥è¡Œä¸ºä¸èƒ½å¿å—ï¼Œä¼™è®¡ã€‚

**Another example:** If I were at a party with e/acc and EA types on opposite sides of the AI x-risk issue, I wouldnâ€™t be able to tell who was who just by looking at normal exterior signals or even asking non-AI-related questions.  

The two groups are very similar along most tribal vectors. But In an effective accelerationism (shortened to â€œe/accâ€) Twitter space a while back, the opening questions were, _why havenâ€™t the EA/rationalist x-riskers killed any AI researchers or accelerationists yet_, and _when are they going to start doing that?_  

å¦ä¸€ä¸ªä¾‹å­ï¼šå¦‚æœæˆ‘åœ¨ä¸€ä¸ªæœ‰e/accå’ŒEAç±»å‹çš„äººå‚åŠ çš„èšä¼šä¸Šï¼Œå¤„äºäººå·¥æ™ºèƒ½Xé£é™©é—®é¢˜çš„å¯¹ç«‹é¢ï¼Œå…‰çœ‹æ­£å¸¸çš„å¤–éƒ¨ä¿¡å·ï¼Œç”šè‡³é—®ä¸äººå·¥æ™ºèƒ½æ— å…³çš„é—®é¢˜ï¼Œæˆ‘éƒ½æ— æ³•åˆ†è¾¨è°æ˜¯è°ã€‚è¿™ä¸¤ä¸ªç¾¤ä½“æ²¿ç€å¤§å¤šæ•°éƒ¨è½çš„å‘é‡éå¸¸ç›¸ä¼¼ã€‚ä½†åœ¨å‰ä¸€é˜µå­çš„æœ‰æ•ˆåŠ é€Ÿä¸»ä¹‰ï¼ˆç®€ç§° "e/acc"ï¼‰æ¨ç‰¹ç©ºé—´é‡Œï¼Œå¼€ç¯‡çš„é—®é¢˜æ˜¯ï¼Œä¸ºä»€ä¹ˆEA/ç†æ€§ä¸»ä¹‰çš„xé£é™©è€…è¿˜æ²¡æœ‰æ€æ­»ä»»ä½•AIç ”ç©¶è€…æˆ–åŠ é€Ÿè€…ï¼Œä»–ä»¬ä»€ä¹ˆæ—¶å€™å¼€å§‹è¿™ä¹ˆåšï¼Ÿ

I mean, we now have the leader of one whole wing of x-risk discourse calling for airstrikes on datacenters:  

æˆ‘çš„æ„æ€æ˜¯ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€æ•´ç¿¼çš„X-é£é™©è¨€è®ºçš„é¢†å¯¼äººå‘¼åå¯¹æ•°æ®ä¸­å¿ƒè¿›è¡Œç©ºè¢­ï¼š

So itâ€™s a free-for-all out there. A veritable state of nature. But there are a few broad camps emerging.  

å› æ­¤ï¼Œé‚£é‡Œæ˜¯ä¸€ä¸ªè‡ªç”±ç«äº‰çš„åœ°æ–¹ã€‚ä¸€ä¸ªåå‰¯å…¶å®çš„è‡ªç„¶çŠ¶æ€ã€‚ä½†æœ‰å‡ ä¸ªå¤§çš„é˜µè¥æ­£åœ¨å‡ºç°ã€‚

The Safetyists are people who express some degree of worry about â€œAI safety,â€ even though they have very different ways of framing the issue. Such people fall into roughly three camps:  

å®‰å…¨ä¸»ä¹‰è€…æ˜¯å¯¹ "äººå·¥æ™ºèƒ½å®‰å…¨ "è¡¨ç¤ºæŸç§ç¨‹åº¦æ‹…å¿§çš„äººï¼Œå°½ç®¡ä»–ä»¬å¯¹è¿™ä¸ªé—®é¢˜æœ‰éå¸¸ä¸åŒçš„è¡¨è¿°æ–¹å¼ã€‚è¿™ç±»äººå¤§è‡´åˆ†ä¸ºä¸‰ä¸ªé˜µè¥ï¼š

1.  **The language police:**Â Worried that LLMs will say mean words, be used to spread disinformation, or be used for phishing attempts or other social manipulation on a large scale. AI ethicist [Gary Marcus](https://twitter.com/GaryMarcus) is in this camp, as are most â€œdisinfoâ€ and DEI advocacy types in the media and academia who are not deep into AI professionally but are opining about it.  
    
    è¯­è¨€è­¦å¯Ÿ:æ‹…å¿ƒLLMä¼šè¯´ä¸€äº›åˆ»è–„çš„è¯ï¼Œè¢«ç”¨æ¥ä¼ æ’­è™šå‡ä¿¡æ¯ï¼Œæˆ–è€…è¢«ç”¨æ¥è¿›è¡Œç½‘ç»œé’“é±¼å°è¯•æˆ–å…¶ä»–å¤§è§„æ¨¡çš„ç¤¾ä¼šæ“çºµã€‚äººå·¥æ™ºèƒ½ä¼¦ç†å­¦å®¶åŠ é‡Œ-é©¬åº“æ–¯å±äºè¿™ä¸ªé˜µè¥ï¼Œåª’ä½“å’Œå­¦æœ¯ç•Œå¤§å¤šæ•° "å‡ä¿¡æ¯ "å’ŒDEIå€¡å¯¼è€…ä¹Ÿå±äºè¿™ä¸ªé˜µè¥ï¼Œä»–ä»¬åœ¨ä¸“ä¸šä¸Šæ²¡æœ‰æ·±å…¥ç ”ç©¶äººå·¥æ™ºèƒ½ï¼Œä½†å´åœ¨å¯¹å…¶å‘è¡¨è¯„è®ºã€‚
    
2.  **The Chernobylists:**Â Worried about what will happen if we hook ML models we donâ€™t fully understand to real-life systems, especially critical ones or ones with weapons on them. [David Chapman](https://betterwithout.ai/) is in this camp, as am I.Â   
    
    åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…ï¼šæ‹…å¿ƒå¦‚æœæˆ‘ä»¬å°†æˆ‘ä»¬ä¸å®Œå…¨ç†è§£çš„MLæ¨¡å‹ä¸ç°å®ç”Ÿæ´»ä¸­çš„ç³»ç»ŸæŒ‚é’©ï¼Œç‰¹åˆ«æ˜¯å…³é”®ç³»ç»Ÿæˆ–æœ‰æ­¦å™¨çš„ç³»ç»Ÿä¼šå‘ç”Ÿä»€ä¹ˆã€‚ å¤§å«-æŸ¥æ™®æ›¼å’Œæˆ‘ä¸€æ ·ï¼Œéƒ½å±äºè¿™ä¸ªé˜µè¥ã€‚
    
3.  **The x-riskers:**Â Absolutely convinced that the moment an AGI comes on the scene, humanity is doomed. [Eliezer Yudkowsky](https://twitter.com/ESYudkowsky) is the most prominent person in this camp, but there are many others in [rationalist and EA circles](https://lesswrong.com/) who fall into it.  
    
    å†’é£é™©è€…ï¼šç»å¯¹ç›¸ä¿¡AGIå‡ºç°çš„é‚£ä¸€åˆ»ï¼Œäººç±»å°±æ³¨å®šè¦å¤±è´¥ã€‚ åŸƒåˆ©æ³½-å°¤å¾·è€ƒå¤«æ–¯åŸºï¼ˆEliezer Yudkowskyï¼‰æ˜¯è¿™ä¸ªé˜µè¥ä¸­æœ€çªå‡ºçš„äººï¼Œä½†åœ¨ç†æ€§ä¸»ä¹‰å’ŒEAåœˆå­é‡Œè¿˜æœ‰å¾ˆå¤šäººå±äºè¿™ä¸ªé˜µè¥ã€‚
    

ğŸ‘®â™€ï¸ The language police and the x-riskers are longstanding culture war enemies.  

ğŸ‘®â™€ï¸ è¯­è¨€è­¦å¯Ÿå’ŒX-Riskersæ˜¯é•¿æœŸçš„æ–‡åŒ–æˆ˜äº‰æ•Œäººã€‚  

Neither has much of a care for the otherâ€™s specific concerns about AI â€” language police think arguments about AI killing us all via nanobots are dumb, and x-riskers think worries about LLMs being coaxed into printing a racial stereotype are dumb.  

ä¸¤è€…éƒ½ä¸å¤ªå…³å¿ƒå¯¹æ–¹å¯¹äººå·¥æ™ºèƒ½çš„å…·ä½“æ‹…å¿§--è¯­è¨€è­¦å¯Ÿè®¤ä¸ºå…³äºäººå·¥æ™ºèƒ½é€šè¿‡çº³ç±³æœºå™¨äººæ€æ­»æˆ‘ä»¬æ‰€æœ‰äººçš„è®ºç‚¹æ˜¯æ„šè ¢çš„ï¼Œè€ŒX-é£é™©è€…è®¤ä¸ºå¯¹LLMè¢«å“„éª—ç€æ‰“å°ç§æ—åˆ»æ¿å°è±¡çš„æ‹…å¿§æ˜¯æ„šè ¢çš„ã€‚  

Nonetheless, these two rival camps are temporarily finding common ground on the cause of regulating or slowing AI.  

å°½ç®¡å¦‚æ­¤ï¼Œè¿™ä¸¤ä¸ªå¯¹ç«‹çš„é˜µè¥åœ¨ç›‘ç®¡æˆ–å‡ç¼“äººå·¥æ™ºèƒ½çš„äº‹ä¸šä¸Šæš‚æ—¶æ‰¾åˆ°äº†å…±åŒç‚¹ã€‚

â›‘ï¸ The Chernobylists donâ€™t have much interest in either of the concerns of the other two camps â€” for us (as I said, Iâ€™m one), toxic language concerns are picayune, fears of mass disinfo campaigns are overblown, and the x-risk stuff is just sci-fi nonsense.  

â›‘ï¸ åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…å¯¹å…¶ä»–ä¸¤ä¸ªé˜µè¥çš„æ‹…å¿§éƒ½æ²¡æœ‰å¤ªå¤§çš„å…´è¶£--å¯¹æˆ‘ä»¬æ¥è¯´ï¼ˆæ­£å¦‚æˆ‘è¯´çš„ï¼Œæˆ‘æ˜¯å…¶ä¸­ä¹‹ä¸€ï¼‰ï¼Œå¯¹æœ‰æ¯’è¯­è¨€çš„æ‹…å¿§æ˜¯è½»ææ·¡å†™çš„ï¼Œå¯¹å¤§è§„æ¨¡é€ è°£è¿åŠ¨çš„æ‹…å¿§æ˜¯å¤¸å¤§å…¶è¯çš„ï¼Œè€ŒX-é£é™©çš„ä¸œè¥¿åªæ˜¯ç§‘å¹»çš„èƒ¡è¯´å…«é“ã€‚  

No, weâ€™re worried that somebody is going to deploy a system in some production context where it can cause an accident that kills a lot of people â€” either a spectacular industrial accident or a slow-rolling, mostly invisible catastrophe of the type that might be downstream of a medical LLM that screws up certain types of drug interactions.  

ä¸ï¼Œæˆ‘ä»¬æ‹…å¿ƒçš„æ˜¯æœ‰äººä¼šåœ¨æŸäº›ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ä¸€ä¸ªç³»ç»Ÿï¼Œåœ¨é‚£é‡Œå®ƒå¯èƒ½ä¼šå¯¼è‡´ä¸€ä¸ªæ€æ­»å¾ˆå¤šäººçš„äº‹æ•…--è¦ä¹ˆæ˜¯ä¸€ä¸ªå£®è§‚çš„å·¥ä¸šäº‹æ•…ï¼Œè¦ä¹ˆæ˜¯ä¸€ä¸ªç¼“æ…¢æ»šåŠ¨çš„ã€å¤§éƒ¨åˆ†çœ‹ä¸è§çš„ç¾éš¾ï¼Œè¿™ç§ç±»å‹çš„ç¾éš¾å¯èƒ½æ˜¯ä¸€ä¸ªåŒ»å­¦æ³•å­¦ç¡•å£«çš„ä¸‹æ¸¸ï¼Œæç ¸äº†æŸäº›ç±»å‹çš„è¯ç‰©ç›¸äº’ä½œç”¨ã€‚

Note that we Chernobylists differ widely on how to address the scenarios weâ€™re worried about.  

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…åœ¨å¦‚ä½•è§£å†³æˆ‘ä»¬æ‰€æ‹…å¿ƒçš„æƒ…å†µä¸Šæœ‰å¾ˆå¤§åˆ†æ­§ã€‚  

Chapman wants to see AI development slow down, whereas Iâ€™m an accelerationist who thinks the answer is to keep building and use AI to find ways to mitigate the chaos that AI progress itself creates.  

æŸ¥æ™®æ›¼å¸Œæœ›çœ‹åˆ°äººå·¥æ™ºèƒ½å‘å±•æ”¾ç¼“ï¼Œè€Œæˆ‘æ˜¯ä¸€ä¸ªåŠ é€Ÿæ´¾ï¼Œè®¤ä¸ºç­”æ¡ˆæ˜¯ç»§ç»­å»ºè®¾ï¼Œå¹¶åˆ©ç”¨äººå·¥æ™ºèƒ½æ‰¾åˆ°ç¼“è§£äººå·¥æ™ºèƒ½è¿›æ­¥æœ¬èº«æ‰€é€ æˆçš„æ··ä¹±çš„æ–¹æ³•ã€‚  

This is a debate for another day, though.  

ä¸è¿‡è¿™æ˜¯å¦ä¸€å¤©çš„è¾©è®ºã€‚

ğŸ‘µğŸ» The boomers are not technically baby boomers â€” theyâ€™re just people who are still in the very first step of the two-step process I described in the prelude, i.e., theyâ€™re cramming all this AI stuff into existing culture war frames based on the tribal signals the different participants in the brawl are throwing off as they go by in the melee.  

ğŸ‘µğŸ» ä¸¥æ ¼æ¥è¯´ï¼Œå©´å„¿æ½®ä¸€ä»£å¹¶ä¸æ˜¯å©´å„¿æ½®--ä»–ä»¬åªæ˜¯ä»å¤„äºæˆ‘åœ¨åºè¨€ä¸­æè¿°çš„ä¸¤æ­¥è¿›ç¨‹çš„ç¬¬ä¸€æ­¥çš„äººï¼Œå³ä»–ä»¬æ ¹æ®äº‰åµä¸­ä¸åŒå‚ä¸è€…åœ¨æ··æˆ˜ä¸­ç»è¿‡æ—¶æŠ›å‡ºçš„éƒ¨è½ä¿¡å·ï¼Œå°†æ‰€æœ‰è¿™äº›äººå·¥æ™ºèƒ½çš„ä¸œè¥¿å¡è¿›ç°æœ‰çš„æ–‡åŒ–æˆ˜äº‰æ¡†æ¶ã€‚

In my own experience, there are two subgroups of AI Boomers: the red tribe and the blue tribe. Their views on AI break down roughly as follows:  

æ ¹æ®æˆ‘è‡ªå·±çš„ç»éªŒï¼Œäººå·¥æ™ºèƒ½æ½®äººæœ‰ä¸¤ä¸ªäºšç¾¤ï¼šçº¢è‰²éƒ¨è½å’Œè“è‰²éƒ¨è½ã€‚ä»–ä»¬å¯¹äººå·¥æ™ºèƒ½çš„çœ‹æ³•å¤§è‡´ç»†åˆ†å¦‚ä¸‹ï¼š

**ğŸ”µ Blue tribe:  

ğŸ”µè“è‰²éƒ¨è½ï¼š**

-   Sam Altman, Peter Thiel, and Elon Musk are capitalists and longtermists, therefore theyâ€™re bad because capitalism bad and longtermism bad.Â   
    
    è¨å§†-å¥¥ç‰¹æ›¼ã€å½¼å¾—-è’‚å°”å’ŒåŸƒéš†-é©¬æ–¯å…‹æ˜¯èµ„æœ¬å®¶å’Œé•¿æœŸä¸»ä¹‰è€…ï¼Œå› æ­¤ä»–ä»¬æ˜¯åäººï¼Œå› ä¸ºèµ„æœ¬ä¸»ä¹‰ä¸å¥½ï¼Œé•¿æœŸä¸»ä¹‰ä¸å¥½ã€‚
    
-   The AIs that have been unleashed by capitalists and BigCos are screwing up all of plans to sell our art and short fiction, which is bad because indie artists are good and capitalist BigCos are bad.  
    
    è¢«èµ„æœ¬å®¶å’ŒBigCosé‡Šæ”¾å‡ºæ¥çš„AIæ­£åœ¨ç ´åæ‰€æœ‰é”€å”®æˆ‘ä»¬çš„è‰ºæœ¯å’ŒçŸ­ç¯‡å°è¯´çš„è®¡åˆ’ï¼Œè¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸ºç‹¬ç«‹è‰ºæœ¯å®¶æ˜¯å¥½çš„ï¼Œè€Œèµ„æœ¬å®¶BigCosæ˜¯åçš„ã€‚
    
-   AI smells like a new tool of right-wing capitalist power for controlling the masses.  
    
    äººå·¥æ™ºèƒ½é—»èµ·æ¥åƒæ˜¯å³ç¿¼èµ„æœ¬ä¸»ä¹‰æƒåŠ›æ§åˆ¶å¤§ä¼—çš„æ–°å·¥å…·ã€‚
    

**ğŸ”´ Red tribe:  

ğŸ”´ çº¢è‰²éƒ¨è½ï¼š**

-   Sam Altman, Peter Thiel, and Elon Musk are sexual degenerates and longtermists, therefore theyâ€™re bad because sexual degeneracy bad and longtermism bad. (Everyone hates the longtermists!)  
    
    è¨å§†-å¥¥ç‰¹æ›¼ã€å½¼å¾—-è’‚å°”å’ŒåŸƒéš†-é©¬æ–¯å…‹æ˜¯æ€§å •è½è€…å’Œé•¿æœŸä¸»ä¹‰è€…ï¼Œå› æ­¤ä»–ä»¬æ˜¯åçš„ï¼Œå› ä¸ºæ€§å •è½è€…åï¼Œé•¿æœŸä¸»ä¹‰è€…åã€‚ æ¯ä¸ªäººéƒ½è®¨åŒé•¿æœŸä¸»ä¹‰è€…ï¼ï¼‰ã€‚
    
-   AI is a creation of BigTech, BigTech is woke, ChatGPT is woke, woke is bad, therefore AI is bad by transitivity.  
    
    äººå·¥æ™ºèƒ½æ˜¯BigTechçš„åˆ›é€ ï¼ŒBigTechæ˜¯æ¸…é†’çš„ï¼ŒChatGPTæ˜¯æ¸…é†’çš„ï¼Œæ¸…é†’çš„æ˜¯åçš„ï¼Œå› æ­¤æ ¹æ®åè¯æ³•ï¼Œäººå·¥æ™ºèƒ½æ˜¯åçš„ã€‚
    
-   So much boobs and waifu and porn in every AI art FB group.  
    
    æ¯ä¸€ä¸ªäººå·¥æ™ºèƒ½è‰ºæœ¯FBå°ç»„ä¸­éƒ½æœ‰è¿™ä¹ˆå¤šçš„èƒ¸éƒ¨ã€è…°éƒ¨å’Œè‰²æƒ…ã€‚
    
-   It ainâ€™t human, which means itâ€™s either angelic or demonic, and it ainâ€™t angelic (see above re: sexual degeneracy), soâ€¦  
    
    å®ƒä¸æ˜¯äººç±»ï¼Œè¿™æ„å‘³ç€å®ƒè¦ä¹ˆæ˜¯å¤©ä½¿ï¼Œè¦ä¹ˆæ˜¯æ¶é­”ï¼Œè€Œå®ƒä¸æ˜¯å¤©ä½¿ï¼ˆè§ä¸Šæ–‡å…³äºæ€§å •è½ï¼‰ï¼Œæ‰€ä»¥......
    
-   AI smells like a new tool of woke capitalist power.  
    
    äººå·¥æ™ºèƒ½é—»èµ·æ¥åƒä¸€ä¸ªé†’ç›®çš„èµ„æœ¬ä¸»ä¹‰æƒåŠ›çš„æ–°å·¥å…·ã€‚
    

There is a ton of overlap between Blue Tribe Boomers and Language Police, but I consider the Language Police a separate group because theyâ€™re situated in media and academia and have sinecures where theyâ€™re paid to tell everyone to stop using certain words and phrases.  

è“è‰²éƒ¨è½æ½®äººå’Œè¯­è¨€è­¦å¯Ÿä¹‹é—´æœ‰å¤§é‡çš„é‡å ï¼Œä½†æˆ‘è®¤ä¸ºè¯­è¨€è­¦å¯Ÿæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ç¾¤ä½“ï¼Œå› ä¸ºä»–ä»¬ä½äºåª’ä½“å’Œå­¦æœ¯ç•Œï¼Œå¹¶æœ‰ä¸€ä»½å·¥èµ„ï¼Œå‘Šè¯‰å¤§å®¶åœæ­¢ä½¿ç”¨æŸäº›è¯æ±‡å’ŒçŸ­è¯­ã€‚

Iâ€™ve saved the most interesting (to me) camp for last. These people are way off in their own, largely isolated wing of the AI wars. This camp is also centered on a specific clique.  

æˆ‘æŠŠæœ€æœ‰è¶£çš„ï¼ˆå¯¹æˆ‘æ¥è¯´ï¼‰é˜µè¥ç•™åˆ°æœ€åã€‚è¿™äº›äººåœ¨ä»–ä»¬è‡ªå·±çš„ã€åŸºæœ¬ä¸Šæ˜¯å­¤ç«‹çš„äººå·¥æ™ºèƒ½æˆ˜äº‰çš„ä¸€ç¿¼ä¸­èµ°å¾—å¾ˆè¿œã€‚è¿™ä¸ªé˜µè¥ä¹Ÿä»¥ä¸€ä¸ªç‰¹å®šçš„å°é›†å›¢ä¸ºä¸­å¿ƒã€‚  

I will name two of the names in that clique in a moment, but first I have to lay out their position.  

æˆ‘ä¸€ä¼šå„¿ä¼šè¯´å‡ºè¿™ä¸ªé›†å›¢ä¸­çš„ä¸¤ä¸ªåå­—ï¼Œä½†é¦–å…ˆæˆ‘å¿…é¡»é˜è¿°ä»–ä»¬çš„ç«‹åœºã€‚

ğŸ§  Letâ€™s say youâ€™re someone who believes the concept of **IQ is a racist, eugenicist** **fiction**.  

In your view, any attempt to sort people into groups based on some measurement of intelligence â€” an IQ test, the SAT or ACT, the MCAT, LSAT, GRE, whatever â€” is definitionally part of a colonialist, eugenicist project that marginalizes indigenous ways of knowing and being, and seeks to reify a white supremacist ideology that views human intellectual capacity as measurable and quantifiable.  

å‡è®¾ä½ æ˜¯ä¸€ä¸ªè®¤ä¸ºæ™ºå•†çš„æ¦‚å¿µæ˜¯ç§æ—ä¸»ä¹‰çš„ã€ä¼˜ç”Ÿä¸»ä¹‰çš„è™šæ„ã€‚åœ¨ä½ çœ‹æ¥ï¼Œä»»ä½•è¯•å›¾æ ¹æ®æŸç§æ™ºåŠ›æµ‹é‡æ–¹æ³•å°†äººä»¬åˆ†æˆä¸åŒçš„ç¾¤ä½“--æ™ºå•†æµ‹è¯•ã€SATæˆ–ACTã€MCATã€LSATã€GREç­‰--ä»å®šä¹‰ä¸Šè®²éƒ½æ˜¯æ®–æ°‘ä¸»ä¹‰ã€ä¼˜ç”Ÿä¸»ä¹‰é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒå°†æœ¬åœŸçš„è®¤è¯†å’Œå­˜åœ¨æ–¹å¼è¾¹ç¼˜åŒ–ï¼Œå¹¶è¯•å›¾é‡æ–°è¯æ˜ç™½äººè‡³ä¸Šä¸»ä¹‰çš„æ„è¯†å½¢æ€ï¼Œå°†äººç±»æ™ºåŠ›èƒ½åŠ›è§†ä¸ºå¯è¡¡é‡å’Œå¯é‡åŒ–çš„ã€‚

If you are one of these people, then how are you going to relate constructively to any important human endeavor that advertises itself as having â€œintelligenceâ€ in the name?  

å¦‚æœä½ æ˜¯è¿™äº›äººä¸­çš„ä¸€å‘˜ï¼Œé‚£ä¹ˆä½ å°†å¦‚ä½•å»ºè®¾æ€§åœ°ä¸ä»»ä½•æ ‡æ¦œè‡ªå·±æœ‰ "æ™ºæ…§ "çš„é‡è¦äººç±»åŠªåŠ›è”ç³»èµ·æ¥ï¼Ÿ  

How can you contribute to the kind of high-stakes, polarizing, collective problem-solving efforts around how to quantify machine intelligence described in the technical section?  

ä½ å¦‚ä½•ä¸ºæŠ€æœ¯éƒ¨åˆ†ä¸­æè¿°çš„é‚£ç§å›´ç»•å¦‚ä½•é‡åŒ–æœºå™¨æ™ºèƒ½çš„é«˜é£é™©ã€ä¸¤æåŒ–ã€é›†ä½“è§£å†³é—®é¢˜çš„åŠªåŠ›åšå‡ºè´¡çŒ®ï¼Ÿ

ğŸ«¢ The answer is that itâ€™s going to be quite difficult to position yourself in any conversation that moves beyond very basic Language Police concerns.  

ç­”æ¡ˆæ˜¯ï¼Œåœ¨ä»»ä½•è¶…è¶Šè¯­è¨€è­¦å¯ŸåŸºæœ¬å…³åˆ‡çš„å¯¹è¯ä¸­ï¼Œè¦ç»™è‡ªå·±å®šä½æ˜¯ç›¸å½“å›°éš¾çš„ã€‚  

The minute the discussion moves out of the realm of naughty words and into the realm of ML models with dangerous, superhuman capabilities that we donâ€™t understand and canâ€™t control, you have to hit eject.  

ä¸€æ—¦è®¨è®ºè„±ç¦»äº†è°ƒçš®è¯çš„èŒƒç•´ï¼Œè¿›å…¥äº†æˆ‘ä»¬ä¸äº†è§£ä¹Ÿæ— æ³•æ§åˆ¶çš„å…·æœ‰å±é™©çš„è¶…äººèƒ½åŠ›çš„MLæ¨¡å‹çš„é¢†åŸŸï¼Œä½ å°±å¿…é¡»ç‚¹å‡»å¼¹å‡ºã€‚  

This is why the Intelligence Deniers are isolated from the rest of the safety discourse.  

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ "æƒ…æŠ¥å¦è®¤è€… "è¢«å­¤ç«‹äºå…¶ä»–å®‰å…¨è®ºè¿°ä¹‹å¤–ã€‚

The fundamental premises of the Intelligence Denier clique and their hangers-on in the media and academia are something like the following:  

æƒ…æŠ¥å¦è®¤è€…é›†å›¢åŠå…¶åœ¨åª’ä½“å’Œå­¦æœ¯ç•Œçš„è¿½éšè€…çš„åŸºæœ¬å‰ææ˜¯å¦‚ä¸‹å†…å®¹ï¼š

1.  â€œArtificial intelligenceâ€ is a capitalist marketing term for statistical smoke-and-mirrors. There is no real innovation there and itâ€™s **mostly just a scam**.  
    
    "äººå·¥æ™ºèƒ½ "æ˜¯ä¸€ä¸ªèµ„æœ¬ä¸»ä¹‰è¥é”€æœ¯è¯­ï¼Œæ˜¯ç»Ÿè®¡å­¦ä¸Šçš„çƒŸå¹•å¼¹ã€‚é‚£é‡Œæ²¡æœ‰çœŸæ­£çš„åˆ›æ–°ï¼Œå®ƒä¸»è¦æ˜¯ä¸€ä¸ªéª—å±€ã€‚
    
2.  The **models are just math** and are not â€œintelligentâ€ because â€œintelligenceâ€ is a colonialist cisheteropatriarchal white supremacist eugenicist construct.  
    
    è¿™äº›æ¨¡å‹åªæ˜¯æ•°å­¦ï¼Œå¹¶ä¸æ˜¯ "æ™ºèƒ½"ï¼Œå› ä¸º "æ™ºèƒ½ "æ˜¯æ®–æ°‘ä¸»ä¹‰çš„é¡ºå‘å¼‚æ€§æ‹çš„ç™½äººè‡³ä¸Šä¸»ä¹‰çš„ä¼˜ç”Ÿå­¦æ„é€ ã€‚
    
3.  To the extent that the models are made to act in some human-like fashion, this is a **dangerous, dehumanizing** effort by our enemies to effect the aforementioned capitalist smoke-and-mirrors strategy for profit at the expense of the marginalized and minoritized.Â   
    
    å¦‚æœè®©è¿™äº›æ¨¡ç‰¹å„¿ä»¥æŸç§ç±»ä¼¼äººç±»çš„æ–¹å¼è¡ŒåŠ¨ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„æ•Œäººä¸ºå®ç°ä¸Šè¿°èµ„æœ¬ä¸»ä¹‰çš„çƒŸå¹•å¼¹æˆ˜ç•¥è€Œåšå‡ºçš„å±é™©çš„ã€éäººåŒ–çš„åŠªåŠ›ï¼Œä»¥ç‰ºç‰²è¾¹ç¼˜åŒ–å’Œå°‘æ•°äººçš„åˆ©ç›Šä¸ºä»£ä»·ã€‚
    
4.  **Gary Marcus** and other white dudes who are trying to be allies and do â€œAI ethicsâ€ alongside us need to shut up and just center us and signal boost us.  
    
    (That Gary Marcus should shut his trap is surely the only thing AI-related that Emily Bender and Yann Lecun can agree on.)  
    
    åŠ é‡Œ-é©¬åº“æ–¯å’Œå…¶ä»–è¯•å›¾æˆä¸ºç›Ÿå‹å¹¶ä¸æˆ‘ä»¬ä¸€èµ·åš "äººå·¥æ™ºèƒ½ä¼¦ç† "çš„ç™½äººå¸…å“¥éœ€è¦é—­å˜´ï¼Œåªéœ€ä»¥æˆ‘ä»¬ä¸ºä¸­å¿ƒï¼Œç»™æˆ‘ä»¬å‘ä¿¡å·ã€‚(åŠ é‡Œ-é©¬åº“æ–¯åº”è¯¥é—­ä¸Šä»–çš„å˜´ï¼Œè¿™è‚¯å®šæ˜¯è‰¾ç±³ä¸½-æœ¬å¾·å’Œæ‰¬-å‹’åº“æ©å”¯ä¸€èƒ½åŒæ„çš„ä¸äººå·¥æ™ºèƒ½æœ‰å…³çš„äº‹æƒ…ã€‚ï¼‰
    
5.  If youâ€™re proposing technical solutions to the ML problems weâ€™ve identified (e.g., it spits out stereotypes, it doesnâ€™t reliably recognize non-white faces, it is used by the police in any capacity for any reason, it does any kind of profiling or evaluating of humans in a way that might disadvantage some who are marginalized), then you are doing a â€œ**techno-solutionism**â€ and that is absolutely not allowed. The only correct answer is to completely reform all of society and dismantle all interlocking systems of oppression.  
    
    Your GitHub pull requests and technical proposals are attempts to maintain the status quo. Much more on this, [here](https://www.jonstokes.com/p/understanding-the-role-of-racist).  
    
    å¦‚æœä½ å¯¹æˆ‘ä»¬å‘ç°çš„MLé—®é¢˜æå‡ºæŠ€æœ¯è§£å†³æ–¹æ¡ˆï¼ˆä¾‹å¦‚ï¼Œå®ƒåå‡ºçš„æ˜¯åˆ»æ¿å°è±¡ï¼Œå®ƒä¸èƒ½å¯é åœ°è¯†åˆ«éç™½äººé¢å­”ï¼Œå®ƒè¢«è­¦å¯Ÿä»¥ä»»ä½•èº«ä»½ä»¥ä»»ä½•ç†ç”±ä½¿ç”¨ï¼Œå®ƒä»¥ä¸€ç§å¯èƒ½ä½¿ä¸€äº›è¢«è¾¹ç¼˜åŒ–çš„äººå¤„äºä¸åˆ©åœ°ä½çš„æ–¹å¼å¯¹äººç±»è¿›è¡Œä»»ä½•å½¢å¼çš„å®šæ€§æˆ–è¯„ä¼°ï¼‰ï¼Œé‚£ä¹ˆä½ æ­£åœ¨åšä¸€ä¸ª "æŠ€æœ¯è§£å†³æ–¹æ¡ˆ"ï¼Œè¿™æ˜¯ç»å¯¹ä¸å…è®¸çš„ã€‚å”¯ä¸€æ­£ç¡®çš„ç­”æ¡ˆæ˜¯å½»åº•æ”¹é©æ•´ä¸ªç¤¾ä¼šï¼Œæ‹†é™¤æ‰€æœ‰ç¯ç¯ç›¸æ‰£çš„å‹è¿«ç³»ç»Ÿã€‚ä½ çš„GitHubæ‹‰åŠ¨è¯·æ±‚å’ŒæŠ€æœ¯å»ºè®®æ˜¯ç»´æŒç°çŠ¶çš„å°è¯•ã€‚å…³äºè¿™ä¸€ç‚¹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·çœ‹ ã€‚
    
6.  Thereâ€™s a bunch of stuff about **ableism** that I donâ€™t have the time or energy to get into, but basically, if you say the models â€œhallucinateâ€ then that is ableist language that marginalizes the mentally ill (I am not making this up).  
    
    They make these and similar arguments that anyone whoâ€™s been on Twitter long enough can GPT given the right prompt.  
    
    æœ‰ä¸€å †å…³äºèƒ½åŠ›ä¸»ä¹‰çš„ä¸œè¥¿ï¼Œæˆ‘æ²¡æœ‰æ—¶é—´æˆ–ç²¾åŠ›å»ç ”ç©¶ï¼Œä½†åŸºæœ¬ä¸Šï¼Œå¦‚æœä½ è¯´æ¨¡å‹ "äº§ç”Ÿå¹»è§‰"ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯èƒ½åŠ›ä¸»ä¹‰çš„è¯­è¨€ï¼Œå°†ç²¾ç¥ç—…æ‚£è€…è¾¹ç¼˜åŒ–ï¼ˆè¿™ä¸æ˜¯æˆ‘ç¼–çš„ï¼‰ã€‚ä»–ä»¬æå‡ºäº†è¿™äº›å’Œç±»ä¼¼çš„è®ºç‚¹ï¼Œåªè¦æœ‰æ­£ç¡®çš„æç¤ºï¼Œä»»ä½•åœ¨Twitterä¸Šå‘†å¾—å¤Ÿä¹…çš„äººéƒ½å¯ä»¥GPTã€‚
    

(When this newsletter first started in 2021, I spent a bunch of time studying these people and writing about them, so if you scroll back in [my archives](https://www.jonstokes.com/archive) you can find as many of my takes on them as you have time to read.)  

(å½“è¿™ä»½é€šè®¯åœ¨2021å¹´é¦–æ¬¡å¼€å§‹æ—¶ï¼Œæˆ‘èŠ±äº†å¤§é‡æ—¶é—´ç ”ç©¶è¿™äº›äººï¼Œå¹¶å†™äº†å…³äºä»–ä»¬çš„æ–‡ç« ï¼Œæ‰€ä»¥å¦‚æœä½ åœ¨æˆ‘çš„æ¡£æ¡ˆä¸­æ»šåŠ¨ï¼Œä½ å¯ä»¥æ‰¾åˆ°æˆ‘å¯¹ä»–ä»¬çš„çœ‹æ³•ï¼Œåªè¦ä½ æœ‰æ—¶é—´é˜…è¯»ã€‚ï¼‰

If the above list is the way youâ€™re approaching the AI debate, then what is your reaction going to be to an open letter promoted by Gary Effing Marcus that suggests AIs are becoming too intelligent, too quickly?  

Well, letâ€™s see what one member of the Intelligence Denier clique,Â **Emily Bender**, has to say:  

å¦‚æœä¸Šè¿°æ¸…å•æ˜¯ä½ å¯¹å¾…äººå·¥æ™ºèƒ½è¾©è®ºçš„æ–¹å¼ï¼Œé‚£ä¹ˆä½ å¯¹Gary Effing Marcusæ¨åŠ¨çš„ä¸€å°å…¬å¼€ä¿¡ä¼šæœ‰ä»€ä¹ˆååº”ï¼Œè¯¥ä¿¡è®¤ä¸ºäººå·¥æ™ºèƒ½æ­£åœ¨å˜å¾—è¿‡äºæ™ºèƒ½ï¼Œé€Ÿåº¦å¤ªå¿«ï¼Ÿå¥½å§ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ "æ™ºåŠ›å¦è®¤è€… "é›†å›¢çš„ä¸€ä¸ªæˆå‘˜ï¼Œè‰¾ç±³ä¸½-æœ¬å¾·ï¼Œæ˜¯æ€ä¹ˆè¯´çš„ï¼š

ğŸ¤¬ So yeah, she _hated_ the letter.  

She hates Marcus (the two of them beef with each other on Twitter), she hates the capitalist â€œtechbrosâ€ who are investing in and building in AI and whoâ€™ve signed that letter, she hates GPT-4 and thinks itâ€™s a scam, she hates Sam Altmanâ€¦ sheâ€™s just extremely mad all the time about anything that looks like AI progress.  

What can I say, tho, other than that Godâ€™s punishment on such people is that they have to be who they are?  

æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œå¥¹è®¨åŒé‚£å°ä¿¡ã€‚å¥¹è®¨åŒé©¬åº“æ–¯ï¼ˆä»–ä»¬ä¸¤äººåœ¨æ¨ç‰¹ä¸Šäº’ç›¸äº‰åµï¼‰ï¼Œå¥¹è®¨åŒé‚£äº›æŠ•èµ„å’Œå»ºè®¾äººå·¥æ™ºèƒ½å¹¶åœ¨ä¿¡ä¸Šç­¾åçš„èµ„æœ¬ä¸»ä¹‰ "æŠ€æœ¯ç”·"ï¼Œå¥¹è®¨åŒGPT-4ï¼Œè®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªéª—å±€ï¼Œå¥¹è®¨åŒå±±å§†-å¥¥ç‰¹æ›¼......å¥¹åªæ˜¯å¯¹ä»»ä½•çœ‹èµ·æ¥åƒäººå·¥æ™ºèƒ½è¿›æ­¥çš„ä¸œè¥¿ä¸€ç›´éƒ½éå¸¸ç”Ÿæ°”ã€‚æˆ‘è¿˜èƒ½è¯´ä»€ä¹ˆå‘¢ï¼Œé™¤äº†ä¸Šå¸å¯¹è¿™ç§äººçš„æƒ©ç½šå°±æ˜¯ä»–ä»¬å¿…é¡»æˆä¸ºä»–ä»¬è‡ªå·±ï¼Ÿ

**Timnit Gebru**Â is another senior member of this clique. I have [written about her before](https://www.jonstokes.com/p/googles-colosseum) in 2021. If youâ€™ve followed my coverage of her, you know I credit her with raising some important issues in AI ethics, but lately, sheâ€™s completely gone off the deep end with elaborate, genealogical conspiracy theories about how the quest for AGI is a literal actual eugenics project.  

è’‚å§†å°¼ç‰¹-æ ¼å¸ƒé²æ–¯æ˜¯è¿™ä¸ªé›†å›¢çš„å¦ä¸€ä¸ªé«˜çº§æˆå‘˜ã€‚æˆ‘ä»¥å‰åœ¨2021å¹´å†™è¿‡å…³äºå¥¹çš„æ–‡ç« ã€‚å¦‚æœä½ å…³æ³¨è¿‡æˆ‘å¯¹å¥¹çš„æŠ¥é“ï¼Œä½ çŸ¥é“æˆ‘è®¤ä¸ºå¥¹æå‡ºäº†ä¸€äº›äººå·¥æ™ºèƒ½ä¼¦ç†æ–¹é¢çš„é‡è¦é—®é¢˜ï¼Œä½†æœ€è¿‘ï¼Œå¥¹å®Œå…¨èµ°å…¥äº†æ·±æ¸Šï¼Œæå‡ºäº†ç²¾å¿ƒè®¾è®¡çš„å®¶è°±å¼é˜´è°‹è®ºï¼Œè¯´å¯¹AGIçš„è¿½æ±‚æ˜¯ä¸€ä¸ªå­—é¢ä¸Šçš„å®é™…ä¼˜ç”Ÿé¡¹ç›®ã€‚

<iframe src="https://www.youtube-nocookie.com/embed/P7XT4TWLzJw?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" frameborder="0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" data-immersive-translate-effect="1"></iframe>

Iâ€™m not going to get into any of that, because if that stuff is your bag then why on earth are you reading my newsletter?Â   

æˆ‘ä¸æ‰“ç®—è®¨è®ºè¿™äº›ï¼Œå› ä¸ºå¦‚æœè¿™äº›ä¸œè¥¿æ˜¯ä½ çš„å›Šä¸­ä¹‹ç‰©ï¼Œé‚£ä¹ˆä½ åˆ°åº•ä¸ºä»€ä¹ˆè¦è¯»æˆ‘çš„é€šè®¯ï¼Ÿ

There are other members of this clique â€” a few more senior members, and a few junior members â€” but Iâ€™m restricting my coverage here to Bender and Gebru because theyâ€™re the most prominent and have consistently put themselves out there publicly.  

è¿™ä¸ªé›†å›¢è¿˜æœ‰å…¶ä»–æˆå‘˜--ä¸€äº›æ›´é«˜çº§çš„æˆå‘˜ï¼Œä»¥åŠä¸€äº›åˆçº§æˆå‘˜--ä½†æˆ‘åœ¨è¿™é‡Œçš„æŠ¥é“ä»…é™äºæœ¬å¾·å’Œæ ¼å¸ƒé²ï¼Œå› ä¸ºä»–ä»¬æ˜¯æœ€çªå‡ºçš„ï¼Œè€Œä¸”ä¸€ç›´å…¬å¼€åœ°æŠŠè‡ªå·±æ”¾åœ¨é‚£é‡Œã€‚

If you havenâ€™t guessed by now, the Intelligence Deniers are people whoâ€™ve taken the â€œtweet through it!â€ path after grappling with AI safety concerns.  

å¦‚æœä½ ç°åœ¨è¿˜æ²¡æœ‰çŒœåˆ°ï¼Œæ™ºèƒ½å¦è®¤è€…æ˜¯é‚£äº›åœ¨åŠªåŠ›è§£å†³äººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜åé‡‡å– "é€šè¿‡æ¨ç‰¹ï¼"é“è·¯çš„äººã€‚  

Theyâ€™re close enough to the issue to understand the debate, but they are just not going to budge on any of their existing idpol positions to make room for a new set of allies, some of whom (the rationalists) are decidedly â€œproblematic.â€  

ä»–ä»¬ç¦»è¿™ä¸ªé—®é¢˜å¾ˆè¿‘ï¼Œèƒ½å¤Ÿç†è§£è¾©è®ºï¼Œä½†ä»–ä»¬å°±æ˜¯ä¸æ‰“ç®—åœ¨ä»–ä»¬ç°æœ‰çš„å¶åƒæ´¾ç«‹åœºä¸Šè®©æ­¥ï¼Œä¸ºä¸€ç»„æ–°çš„ç›Ÿå‹è…¾å‡ºç©ºé—´ï¼Œå…¶ä¸­ä¸€äº›äººï¼ˆç†æ€§ä¸»ä¹‰è€…ï¼‰æ˜¾ç„¶æ˜¯ "æœ‰é—®é¢˜çš„"ã€‚

At any rate, this camp seems destined to shrink through infighting and general toxicity until they drive their allies into the Language Police camp. You hate to see it.  

æ— è®ºå¦‚ä½•ï¼Œè¿™ä¸ªé˜µè¥ä¼¼ä¹æ³¨å®šè¦é€šè¿‡å†…è®§å’Œæ™®éçš„æ¯’å®³è€Œèç¼©ï¼Œç›´åˆ°ä»–ä»¬æŠŠä»–ä»¬çš„ç›Ÿå‹èµ¶åˆ°è¯­è¨€è­¦å¯Ÿé˜µè¥ã€‚ä½ è®¨åŒçœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

ğŸ“ˆ The other thing working against this crew is that **the models just keep getting more capable** and, well, intelligent.  

So if youâ€™re committed on principle to the stance that there is no real innovation going on here, itâ€™s all capitalist flimflammery, intelligence is a racist lie, and so on, youâ€™re destined to migrate further to the fringes as AI keeps posting dramatic wins.  

å¦ä¸€ä»¶ä¸åˆ©äºè¿™æ‰¹äººçš„äº‹æƒ…æ˜¯ï¼Œè¿™äº›æ¨¡å‹çš„èƒ½åŠ›è¶Šæ¥è¶Šå¼ºï¼Œè€Œä¸”ï¼Œè¶Šæ¥è¶Šèªæ˜ã€‚å› æ­¤ï¼Œå¦‚æœä½ åŸåˆ™ä¸ŠåšæŒè®¤ä¸ºè¿™é‡Œæ²¡æœ‰çœŸæ­£çš„åˆ›æ–°ï¼Œéƒ½æ˜¯èµ„æœ¬ä¸»ä¹‰çš„èŠ±æ¶å­ï¼Œæ™ºèƒ½æ˜¯ç§æ—ä¸»ä¹‰çš„è°è¨€ï¼Œç­‰ç­‰ï¼Œé‚£ä¹ˆéšç€äººå·¥æ™ºèƒ½ä¸æ–­å–å¾—æˆå‰§æ€§çš„èƒœåˆ©ï¼Œä½ å°±æ³¨å®šè¦è¿›ä¸€æ­¥è¿ç§»åˆ°è¾¹ç¼˜åœ°å¸¦ã€‚

For those who havenâ€™t read Dune, the Butlerian Jihad was a crusade against any kind of computer or machine whose function approximates what the human mind can do â€” this included everything from artificial intelligence to pocket calculators.Â   

å¯¹äºé‚£äº›æ²¡æœ‰è¯»è¿‡ã€Šæ²™ä¸˜ã€‹çš„äººæ¥è¯´ï¼Œ"å·´ç‰¹å‹’åœ£æˆ˜ "æ˜¯å¯¹ä»»ä½•ä¸€ç§åŠŸèƒ½ä¸äººç±»æ€ç»´ç›¸è¿‘çš„è®¡ç®—æœºæˆ–æœºå™¨çš„è®¨ä¼--è¿™åŒ…æ‹¬ä»äººå·¥æ™ºèƒ½åˆ°è¢–çè®¡ç®—å™¨çš„ä¸€åˆ‡ã€‚

ğŸ”« I think most of the camps and subgroups described in this article are going to converge into a type of real-life Butlerianism.Â   

æˆ‘è®¤ä¸ºè¿™ç¯‡æ–‡ç« ä¸­æè¿°çš„å¤§å¤šæ•°é˜µè¥å’Œåˆ†ç»„éƒ½å°†æ±‡èšæˆä¸€ç§ç°å®ç”Ÿæ´»ä¸­çš„å·´ç‰¹å‹’ä¸»ä¹‰ã€‚

The Intelligence Deniers are already fully Butlerian.  

They donâ€™t take any of the nonlinguistic AI safety concerns seriously, but they still want to crusade against AI, so theyâ€™ve adopted a quasi-mystical of ineffable humaneness that you can read all about in the [NY Magâ€™s Bender fanfic](https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html).  

æ‹’ç»æ™ºèƒ½çš„äººå·²ç»å®Œå…¨æ˜¯å·´ç‰¹å‹’å¼çš„äº†ã€‚ä»–ä»¬ä¸æŠŠä»»ä½•éè¯­è¨€çš„äººå·¥æ™ºèƒ½å®‰å…¨é—®é¢˜å½“å›äº‹ï¼Œä½†ä»–ä»¬ä»ç„¶æƒ³è®¨ä¼äººå·¥æ™ºèƒ½ï¼Œæ‰€ä»¥ä»–ä»¬é‡‡ç”¨äº†ä¸€ç§ä¸å¯è¨€å–»çš„äººæ€§çš„å‡†ç¥ç§˜ä¸»ä¹‰ï¼Œä½ å¯ä»¥åœ¨ã€Šçº½çº¦æ‚å¿—ã€‹çš„ç­å¾·ç²‰ä¸å°è¯´ä¸­è¯»åˆ°è¿™ä¸€åˆ‡ ã€‚

The Boomers and most of the AI Safetyists are going to join the Intelligence Deniers in their Butlerianism either because they have their own metaphysical critiques of AI, or out of convenience because they agree with the ultimate goal of slowing or stopping AI progress.  

Boomerså’Œå¤§å¤šæ•°äººå·¥æ™ºèƒ½å®‰å…¨ä¸»ä¹‰è€…å°†åŠ å…¥æ™ºèƒ½å¦è®¤è€…çš„ç®¡å®¶ä¸»ä¹‰ï¼Œè¦ä¹ˆæ˜¯å› ä¸ºä»–ä»¬å¯¹äººå·¥æ™ºèƒ½æœ‰è‡ªå·±çš„å½¢è€Œä¸Šå­¦æ‰¹åˆ¤ï¼Œè¦ä¹ˆæ˜¯å‡ºäºæ–¹ä¾¿ï¼Œå› ä¸ºä»–ä»¬åŒæ„å‡ç¼“æˆ–é˜»æ­¢äººå·¥æ™ºèƒ½è¿›å±•çš„æœ€ç»ˆç›®æ ‡ã€‚

ğŸï¸ If youâ€™re not a Butlerian, then youâ€™ll be an effective accelerationist (e/acc). This latter camp is the one Iâ€™m in, and we are increasingly (it seems) outnumbered.Â   

ğŸï¸ å¦‚æœä½ ä¸æ˜¯ä¸€ä¸ªå·´ç‰¹å‹’ä¸»ä¹‰è€…ï¼Œé‚£ä¹ˆä½ å°†æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„åŠ é€Ÿä¸»ä¹‰è€…ï¼ˆe/accï¼‰ã€‚è¿™åä¸€ä¸ªé˜µè¥å°±æ˜¯æˆ‘æ‰€åœ¨çš„é˜µè¥ï¼Œè€Œæˆ‘ä»¬çš„äººæ•°è¶Šæ¥è¶Šå¤šï¼ˆä¼¼ä¹ï¼‰ã€‚

You may disagree with me that Butlerianism and e/acc are the only two minima that everyone is going to sort into, but youâ€™d be wrong. So you might as well get to know the sides so you can pick one.  

ä½ å¯èƒ½ä¸åŒæ„æˆ‘çš„è§‚ç‚¹ï¼Œè®¤ä¸ºå·´ç‰¹å‹’ä¸»ä¹‰å’Œe/accæ˜¯æ¯ä¸ªäººéƒ½è¦åˆ†ç±»çš„å”¯ä¸€ä¸¤ä¸ªæœ€å°å€¼ï¼Œä½†ä½ ä¼šé”™çš„ã€‚æ‰€ä»¥ä½ ä¸å¦¨äº†è§£ä¸€ä¸‹åŒæ–¹çš„æƒ…å†µï¼Œè¿™æ ·ä½ å°±å¯ä»¥é€‰ä¸€ä¸ªã€‚

On the topic of scaling laws and GPT-4â€™s parameter count, Anton Troynikov speculated in a recent interview with me that OpenAI did not release GPT-4â€™s parameter count because it may be the same or even smaller than GPT-3â€™s parameter count, and if so that would be a clue that theyâ€™ve made proprietary advances (i.e., something beyond just turning up the scaling knob another notch) and donâ€™t want competitors to know that information.  

å…³äºç¼©æ”¾æ³•åˆ™å’ŒGPT-4çš„å‚æ•°æ•°çš„è¯é¢˜ï¼ŒAnton Troynikovåœ¨æœ€è¿‘æ¥å—æˆ‘çš„é‡‡è®¿æ—¶æ¨æµ‹ï¼ŒOpenAIæ²¡æœ‰å…¬å¸ƒGPT-4çš„å‚æ•°æ•°ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¸GPT-3çš„å‚æ•°æ•°ç›¸åŒç”šè‡³æ›´å°‘ï¼Œå¦‚æœæ˜¯è¿™æ ·ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªçº¿ç´¢ï¼Œä»–ä»¬å·²ç»å–å¾—äº†ä¸“æœ‰çš„è¿›å±•ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ä»…ä»…æ˜¯æŠŠç¼©æ”¾æ—‹é’®å†è°ƒé«˜ä¸€ä¸ªæ¡£æ¬¡ï¼‰ï¼Œå¹¶ä¸”ä¸å¸Œæœ›ç«äº‰å¯¹æ‰‹çŸ¥é“è¿™äº›ä¿¡æ¯ã€‚

The more I consider Antonâ€™s speculation, the more Iâ€™m convinced itâ€™s correct.  

æˆ‘è¶Šæ˜¯è€ƒè™‘å®‰ä¸œçš„æ¨æµ‹ï¼Œå°±è¶Šæ˜¯ç›¸ä¿¡å®ƒæ˜¯æ­£ç¡®çš„ã€‚  

GPT-4 is probably a smaller model than GPT-3 or GPT-3.5, and the point in releasing it this way is to understand the impact of the other advances theyâ€™ve made before they go back to turning up the scaling knob with GPT-5 and possibly rocket all the way into an entirely new regime that looks a lot more like a superhuman AGI.  

GPT-4å¯èƒ½æ˜¯ä¸€ä¸ªæ¯”GPT-3æˆ–GPT-3.5æ›´å°çš„æ¨¡å‹ï¼Œä»¥è¿™ç§æ–¹å¼å‘å¸ƒçš„æ„ä¹‰åœ¨äºäº†è§£ä»–ä»¬æ‰€å–å¾—çš„å…¶ä»–è¿›å±•çš„å½±å“ï¼Œç„¶åä»–ä»¬å†å»è½¬åŠ¨GPT-5çš„ç¼©æ”¾æ—‹é’®ï¼Œå¹¶å¯èƒ½ä¸€è·¯ç«ç®­èˆ¬åœ°è¿›å…¥ä¸€ä¸ªå…¨æ–°çš„åˆ¶åº¦ï¼Œçœ‹èµ·æ¥æ›´åƒä¸€ä¸ªè¶…äººçš„AGIã€‚

Sam has consistently stressed the need for slow takeoff and incrementalism so that humanity can progressively come to grips with more capable models as they develop.  

He says explicitly in the Lex interview that he **fears fast takeoff** and wants to avoid that if at all possible.  

è¨å§†ä¸€ç›´å¼ºè°ƒéœ€è¦ç¼“æ…¢çš„èµ·é£å’Œæ¸è¿›ä¸»ä¹‰ï¼Œä»¥ä¾¿äººç±»èƒ½å¤Ÿåœ¨å‘å±•è¿‡ç¨‹ä¸­é€æ­¥æŒæ¡èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹ã€‚ä»–åœ¨Lexé‡‡è®¿ä¸­æ˜ç¡®è¡¨ç¤ºï¼Œä»–å®³æ€•å¿«é€Ÿèµ·é£ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œå¸Œæœ›é¿å…è¿™ç§æƒ…å†µã€‚

And as an **addendum to this addendum**, I feel compelled to point out a subtle contradiction in the GPT-4 performance picture Sam presents on that podcast.  

On the one hand, he tells Lex he was not surprised by GPT-4â€™s performance because it obeys certain laws; on the other, he says all that stuff about tons of incremental gains adding up to one big gain,Â _and_Â he stresses that theyâ€™re not merely following the regular scaling law with GPT-4,Â _and_Â he says they need help characterizing how much more powerful it is.  

ä½œä¸ºè¿™ä¸ªé™„å½•çš„è¡¥å……ï¼Œæˆ‘è§‰å¾—æœ‰å¿…è¦æŒ‡å‡ºSamåœ¨è¯¥æ’­å®¢ä¸­å±•ç¤ºçš„GPT-4æ€§èƒ½å›¾ç‰‡ä¸­çš„ä¸€ä¸ªå¾®å¦™çš„çŸ›ç›¾ã€‚ä¸€æ–¹é¢ï¼Œä»–å‘Šè¯‰Lexä»–å¯¹GPT-4çš„æ€§èƒ½å¹¶ä¸æ„Ÿåˆ°æƒŠè®¶ï¼Œå› ä¸ºå®ƒéµå®ˆäº†æŸäº›è§„å¾‹ï¼›å¦ä¸€æ–¹é¢ï¼Œä»–è¯´äº†æ‰€æœ‰é‚£äº›å…³äºæˆå¨çš„é€’å¢æ”¶ç›ŠåŠ èµ·æ¥å°±æ˜¯ä¸€ä¸ªå¤§æ”¶ç›Šçš„ä¸œè¥¿ï¼Œä»–å¼ºè°ƒä»–ä»¬å¯¹GPT-4ä¸ä»…ä»…æ˜¯éµå¾ªå¸¸è§„çš„ç¼©æ”¾è§„å¾‹ï¼Œè€Œä¸”ä»–è¯´ä»–ä»¬éœ€è¦å¸®åŠ©æè¿°å®ƒæœ‰å¤šå¼ºå¤§ã€‚

Thereâ€™s a part of this picture missing, and I donâ€™t know what it is.  

è¿™å¼ ç…§ç‰‡å°‘äº†ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä¸çŸ¥é“å®ƒæ˜¯ä»€ä¹ˆã€‚  

GPT-4â€™s â€œintelligenceâ€ is either neatly characterized by some â€œnumber go upâ€ type law, or itâ€™s hard to measure and is the product of a bunch of small optimizations. But it seems like it canâ€™t be both.  

GPT-4çš„ "æ™ºèƒ½ "è¦ä¹ˆè¢«ä¸€äº› "æ•°å­—ä¸Šå‡ "ç±»å‹çš„è§„å¾‹æ•´é½åœ°æè¿°å‡ºæ¥ï¼Œè¦ä¹ˆå®ƒå¾ˆéš¾æµ‹é‡ï¼Œæ˜¯ä¸€å †å°ä¼˜åŒ–çš„äº§ç‰©ã€‚ä½†ä¼¼ä¹å®ƒä¸å¯èƒ½ä¸¤è€…å…¼å¾—ã€‚

If you figure this one out, let me know in the comments.  

å¦‚æœä½ æƒ³å‡ºäº†è¿™ä¸ªé—®é¢˜ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ã€‚
