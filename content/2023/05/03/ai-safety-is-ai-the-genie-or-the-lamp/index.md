---
title: "äººå·¥æ™ºèƒ½å®‰å…¨ï¼šäººå·¥æ™ºèƒ½æ˜¯ç²¾çµè¿˜æ˜¯ç¥ç¯ï¼Ÿ"
date: 2023-05-03T13:27:52+08:00
updated: 2023-05-03T13:27:52+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/ai-safety-is-ai-the-genie-or-the
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "AI Safety: Is AI The Genie Or The Lamp?"
  original_lang: en
---

Summaryï¼šæœ¬æ–‡è®¨è®ºäººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºä¸­çš„æ¦‚å¿µå¯¹é½é—®é¢˜ï¼Œåˆ†ä¸ºä¸ªäººä¸»ä¹‰å’Œé›†ä½“ä¸»ä¹‰ä¸¤ç§å¯¹é½æ–¹å¼ã€‚ä¸ªäººä¸»ä¹‰å¯¹é½æ–¹å¼æ³¨é‡æ§åˆ¶ï¼Œå¼ºè°ƒç”¨æˆ·å¯¹äººå·¥æ™ºèƒ½çš„æŒæ§ï¼›é›†ä½“ä¸»ä¹‰å¯¹é½æ–¹å¼åˆ™å¼ºè°ƒé—®è´£åˆ¶ã€å…¬å¹³ã€å…¬æ­£ç­‰æ¦‚å¿µã€‚

---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Faef7f2a4-0874-4629-8e6d-b3b43170ef4d_1408x1024.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faef7f2a4-0874-4629-8e6d-b3b43170ef4d_1408x1024.jpeg)

_**The story so far**: I think weâ€™re rapidly approaching some kind of crisis point with the AI safety debate, and weâ€™re probably going to do something stupid, like pass some insane laws that help no one and make everything worse.  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šæˆ‘è®¤ä¸ºæˆ‘ä»¬æ­£åœ¨è¿…é€Ÿæ¥è¿‘äººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºçš„æŸç§å±æœºç‚¹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåšä¸€äº›æ„šè ¢çš„äº‹æƒ…ï¼Œæ¯”å¦‚é€šè¿‡ä¸€äº›ç–¯ç‹‚çš„æ³•å¾‹ï¼Œå¯¹ä»»ä½•äººéƒ½æ²¡æœ‰å¸®åŠ©ï¼Œå¹¶ä½¿ä¸€åˆ‡å˜å¾—æ›´ç³Ÿã€‚  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šæˆ‘è®¤ä¸ºæˆ‘ä»¬æ­£åœ¨è¿…é€Ÿæ¥è¿‘äººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºçš„æŸç§å±æœºç‚¹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåšä¸€äº›æ„šè ¢çš„äº‹æƒ…ï¼Œæ¯”å¦‚é€šè¿‡ä¸€äº›ç–¯ç‹‚çš„æ³•å¾‹ï¼Œå¯¹ä»»ä½•äººéƒ½æ²¡æœ‰å¸®åŠ©ï¼Œå¹¶ä½¿ä¸€åˆ‡å˜å¾—æ›´ç³Ÿã€‚  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šæˆ‘è®¤ä¸ºæˆ‘ä»¬æ­£åœ¨è¿…é€Ÿæ¥è¿‘äººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºçš„æŸç§å±æœºç‚¹ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåšä¸€äº›æ„šè ¢çš„äº‹æƒ…ï¼Œæ¯”å¦‚é€šè¿‡ä¸€äº›ç–¯ç‹‚çš„æ³•å¾‹ï¼Œå¯¹ä»»ä½•äººéƒ½æ²¡æœ‰å¸®åŠ©ï¼Œå¹¶ä½¿ä¸€åˆ‡å˜å¾—æ›´ç³Ÿã€‚_

_It really feels like we arenâ€™t making much progress on the topic, either.  

çœŸçš„æ„Ÿè§‰æˆ‘ä»¬åœ¨è¿™ä¸ªè¯é¢˜ä¸Šä¹Ÿæ²¡æœ‰å–å¾—ä»€ä¹ˆè¿›å±•ã€‚  

Everyone is talking past each other, and itâ€™s partly because weâ€™re all working from different fundamental conceptions of what â€œAIâ€ really is â€” is it an agent or a tool, the genie or the lamp?  

æ¯ä¸ªäººéƒ½åœ¨äº’ç›¸äº¤è°ˆï¼Œéƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬éƒ½ä»ä¸åŒçš„åŸºæœ¬æ¦‚å¿µå‡ºå‘ï¼Œæ¥ç†è§£ "äººå·¥æ™ºèƒ½ "åˆ°åº•æ˜¯ä»€ä¹ˆ--å®ƒæ˜¯ä¸€ä¸ªä»£ç†è¿˜æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œæ˜¯ç²¾çµè¿˜æ˜¯ç¥ç¯ï¼Ÿ  

How you answer this question impacts every aspect of your approach to AI explainability and safety.çœŸçš„æ„Ÿè§‰æˆ‘ä»¬åœ¨è¿™ä¸ªè¯é¢˜ä¸Šä¹Ÿæ²¡æœ‰å–å¾—ä»€ä¹ˆè¿›å±•ã€‚æ¯ä¸ªäººéƒ½åœ¨äº’ç›¸è®¨è®ºï¼Œéƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬éƒ½åœ¨ä»ä¸åŒçš„åŸºæœ¬æ¦‚å¿µå‡ºå‘æ¥ç ”ç©¶ "äººå·¥æ™ºèƒ½ "åˆ°åº•æ˜¯ä»€ä¹ˆ--å®ƒæ˜¯ä¸€ä¸ªä»£ç†è¿˜æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œæ˜¯ç²¾çµè¿˜æ˜¯ç¥ç¯ï¼Ÿä½ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ä¼šå½±å“åˆ°ä½ å¯¹äººå·¥æ™ºèƒ½å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æ–¹æ³•çš„å„ä¸ªæ–¹é¢ã€‚ä½ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ä¼šå½±å“åˆ°ä½ å¯¹äººå·¥æ™ºèƒ½å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§çš„æ–¹æ³•çš„æ¯ä¸€ä¸ªæ–¹é¢ã€‚çœŸçš„æ„Ÿè§‰æˆ‘ä»¬åœ¨è¿™ä¸ªè¯é¢˜ä¸Šä¹Ÿæ²¡æœ‰å–å¾—ä»€ä¹ˆè¿›å±•ã€‚æ¯ä¸ªäººéƒ½åœ¨äº’ç›¸è®¨è®ºï¼Œéƒ¨åˆ†åŸå› æ˜¯æˆ‘ä»¬éƒ½åœ¨ä»ä¸åŒçš„åŸºæœ¬æ¦‚å¿µå‡ºå‘æ¥ç ”ç©¶ "äººå·¥æ™ºèƒ½ "åˆ°åº•æ˜¯ä»€ä¹ˆ--å®ƒæ˜¯ä¸€ä¸ªä»£ç†è¿˜æ˜¯ä¸€ä¸ªå·¥å…·ï¼Œæ˜¯ç²¾çµè¿˜æ˜¯ç¥ç¯ï¼Ÿä½ å¦‚ä½•å›ç­”è¿™ä¸ªé—®é¢˜ä¼šå½±å“åˆ°ä½ å¯¹äººå·¥æ™ºèƒ½å¯è§£é‡Šæ€§å’Œå®‰å…¨æ€§æ–¹æ³•çš„å„ä¸ªæ–¹é¢ã€‚_Â 

At the heart of the AI safety debate is the concept of _alignment_, and, not surprisingly, subtly divergent understandings of this seemingly intuitive concept are behind much of the debateâ€™s dysfunction.  

äººå·¥æ™ºèƒ½å®‰å…¨è¾©è®ºçš„æ ¸å¿ƒæ˜¯å¯¹å‡†çš„æ¦‚å¿µï¼Œæ¯«ä¸å¥‡æ€ªï¼Œå¯¹è¿™ä¸€çœ‹ä¼¼ç›´è§‚çš„æ¦‚å¿µçš„å¾®å¦™çš„ä¸åŒç†è§£æ˜¯è¾©è®ºçš„å¤§éƒ¨åˆ†åŠŸèƒ½éšœç¢çš„èƒŒåã€‚

â¦· There are a number of formal definitions of â€œalignmentâ€ floating around out there, but I donâ€™t want to add to the noise by trying to unpack any of these.  

â¦·å¤–é¢æµä¼ ç€è®¸å¤šå…³äº "å¯¹é½ "çš„æ­£å¼å®šä¹‰ï¼Œä½†æˆ‘ä¸æƒ³é€šè¿‡è¯•å›¾è§£è¯»è¿™äº›å®šä¹‰æ¥å¢åŠ å™ªéŸ³ã€‚  

Rather, hereâ€™s my attempt to collate a set of what we might call â€œfolk conceptionsâ€ of alignment that I typically see in operation when this topic comes up â€” i.e., these are the different ways different tribes seem to be thinking about alignment, regardless of how theyâ€™d define it if asked:  

ç›¸åï¼Œæˆ‘è¯•å›¾æ•´ç†ä¸€å¥—æˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸º "æ°‘é—´æ¦‚å¿µ "çš„å¯¹é½æ–¹å¼ï¼Œå½“è¿™ä¸ªè¯é¢˜å‡ºç°æ—¶ï¼Œæˆ‘é€šå¸¸ä¼šçœ‹åˆ°è¿™äº›æ¦‚å¿µåœ¨è¿ä½œ--ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›æ˜¯ä¸åŒéƒ¨è½ä¼¼ä¹åœ¨æ€è€ƒå¯¹é½æ–¹å¼çš„ä¸åŒæ–¹å¼ï¼Œä¸ç®¡ä»–ä»¬åœ¨è¢«é—®åˆ°æ—¶ä¼šå¦‚ä½•å®šä¹‰å®ƒï¼š

**Individualist:**

-   Aligned ğŸ˜€: The AI does what I, the user, want.  
    
    å¯¹å‡†äº†ğŸ˜€ï¼šäººå·¥æ™ºèƒ½åšæˆ‘è¿™ä¸ªç”¨æˆ·æƒ³åšçš„äº‹ã€‚
    
-   Aligned ğŸ˜§: The AI does what a hypothetical evil psychopath wants.  
    
    å¯¹å‡†ğŸ˜§ï¼šäººå·¥æ™ºèƒ½åšä¸€ä¸ªå‡æƒ³çš„é‚ªæ¶çš„ç²¾ç¥ç—…æ‚£è€…æƒ³è¦çš„äº‹æƒ…ã€‚
    
-   Unaligned: The AIâ€™s output is not what the user wants.  
    
    ä¸ä¸€è‡´ï¼šAIçš„è¾“å‡ºä¸æ˜¯ç”¨æˆ·æƒ³è¦çš„ã€‚
    

**Collectivist:**

-   Aligned ğŸ˜€: The AI does what my ingroup wants.  
    
    å¯¹å‡†ğŸ˜€ï¼šäººå·¥æ™ºèƒ½ä¼šæŒ‰ç…§æˆ‘æ‰€åœ¨ç¾¤ä½“çš„æ„æ„¿è¡Œäº‹ã€‚
    
-   Aligned ğŸ˜§: The AI does what my outgroup wants.  
    
    å¯¹å‡†ğŸ˜§ï¼šäººå·¥æ™ºèƒ½åšçš„æ˜¯æˆ‘çš„å¤–ç»„æƒ³è¦çš„ã€‚
    
-   Unaligned: The AIâ€™s output is not what any group wants.  
    
    ä¸ç»“ç›Ÿï¼šAIçš„äº§å‡ºä¸æ˜¯ä»»ä½•å›¢ä½“æƒ³è¦çš„ã€‚
    

ğŸ›ï¸ This whole list is about one thing:Â **control**. Who is the boss of the AI, and on what terms, and to what ends?  

ğŸ›ï¸ è¿™æ•´ä¸ªæ¸…å•æ˜¯å…³äºä¸€ä»¶äº‹ï¼šæ§åˆ¶ã€‚è°æ˜¯äººå·¥æ™ºèƒ½çš„è€æ¿ï¼Œä»¥ä»€ä¹ˆæ¡ä»¶ï¼Œä¸ºäº†ä»€ä¹ˆç›®çš„ï¼Ÿ

ğŸ§â™‚ï¸The first alignment conception above isÂ **oriented toward the individual** and is drawn from a classic understanding of how any engineered product should behave.  

The tradeoffs between power and safety are familiar to everyone who has thought for even a few minutes about pen lasers, kitchen knives, and other tools that can be used either constructively or destructively.Â   

ğŸ§â™‚ï¸ ä¸Šé¢çš„ç¬¬ä¸€ç§æ’åˆ—æ¦‚å¿µæ˜¯é¢å‘ä¸ªäººçš„ï¼Œæ˜¯æ¥è‡ªå¯¹ä»»ä½•å·¥ç¨‹äº§å“åº”è¯¥å¦‚ä½•è¡¨ç°çš„ç»å…¸ç†è§£ã€‚æ¯ä¸€ä¸ªå¯¹ç¬”å¼æ¿€å…‰å™¨ã€å¨æˆ¿åˆ€å…·å’Œå…¶ä»–å¯ç”¨äºå»ºè®¾æ€§æˆ–ç ´åæ€§çš„å·¥å…·è¿›è¡Œè¿‡å“ªæ€•å‡ åˆ†é’Ÿæ€è€ƒçš„äººï¼Œéƒ½ä¼šç†Ÿæ‚‰åŠŸç‡å’Œå®‰å…¨ä¹‹é—´çš„æƒè¡¡ã€‚

To really unlock your intuitions about the individualist alignment conception, replace â€œThe AIâ€ with â€œThe firearm,â€ â€œThe nuke,â€ â€œThe nanotech fabricator,â€ and so on.  

ä¸ºäº†çœŸæ­£è§£å¼€ä½ å¯¹ä¸ªäººä¸»ä¹‰æ’åˆ—ç»„åˆæ¦‚å¿µçš„ç›´è§‰ï¼ŒæŠŠ "äººå·¥æ™ºèƒ½ "æ¢æˆ "æªæ”¯"ã€"æ ¸å¼¹"ã€"çº³ç±³æŠ€æœ¯åˆ¶é€ è€…"ï¼Œç­‰ç­‰ã€‚  

This will give any reasonably educated person instant and fairly complete insight into the deep human intuitions about tools, power, and identity that the present AI wars are premised on.  

è¿™å°†ä½¿ä»»ä½•å—è¿‡åˆç†æ•™è‚²çš„äººå³æ—¶å¹¶ç›¸å½“å®Œæ•´åœ°äº†è§£äººç±»å¯¹å·¥å…·ã€æƒåŠ›å’Œèº«ä»½çš„æ·±åˆ»ç›´è§‰ï¼Œè€Œè¿™äº›ç›´è§‰æ­£æ˜¯ç›®å‰äººå·¥æ™ºèƒ½æˆ˜äº‰çš„å‰æã€‚

The salient safety concepts here are things like, â€œaffordances,â€ â€œUX,â€ â€œcontrol surfaces,â€ â€œsteerability,â€ and the like.  

è¿™é‡Œçªå‡ºçš„å®‰å…¨æ¦‚å¿µæ˜¯è¯¸å¦‚ "æ‰¿å—åŠ›"ã€"ç”¨æˆ·ä½“éªŒ"ã€"æ§åˆ¶é¢"ã€"å¯æ“çºµæ€§ "ç­‰ç­‰ã€‚

To the extent that AI is decentralized, with individual users owning, controlling, and/or tweaking their own models, the individualist alignment conception will be a factor alongside the collective conception.  

åœ¨äººå·¥æ™ºèƒ½å»ä¸­å¿ƒåŒ–çš„ç¨‹åº¦ä¸Šï¼Œä¸ªäººç”¨æˆ·æ‹¥æœ‰ã€æ§åˆ¶å’Œ/æˆ–è°ƒæ•´ä»–ä»¬è‡ªå·±çš„æ¨¡å‹ï¼Œä¸ªäººä¸»ä¹‰çš„è°ƒæ•´æ¦‚å¿µå°†æ˜¯ä¸é›†ä½“æ¦‚å¿µå¹¶å­˜çš„ä¸€ä¸ªå› ç´ ã€‚

Many people working in AI are firmly within this individualist camp. For instance, Geoffrey Hintonâ€™s [recent NYT interview](https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html) sees him mostly worrying about the â€œevil psychopathâ€ scenario.  

Sam Altman also explicitly defines alignment as â€œthe AI does what the user wants,â€ and though heâ€™s typically pretty vague when asked to detail his downside scenario we can pretty safely assume itâ€™s, â€œbad people doing bad things with powerful AIs.â€  

è®¸å¤šä»äº‹äººå·¥æ™ºèƒ½å·¥ä½œçš„äººéƒ½åšå®šåœ°ç«™åœ¨è¿™ä¸ªä¸ªäººä¸»ä¹‰é˜µè¥ä¸­ã€‚ä¾‹å¦‚ï¼Œæ°å¼—é‡Œ-è¾›é¡¿ï¼ˆGeoffrey Hintonï¼‰åœ¨æœ€è¿‘çš„ã€Šçº½çº¦æ—¶æŠ¥ã€‹é‡‡è®¿ä¸­çœ‹åˆ°ä»–ä¸»è¦æ‹…å¿ƒçš„æ˜¯ "é‚ªæ¶çš„ç²¾ç¥ç—…æ‚£è€… "çš„æƒ…å†µã€‚è¨å§†-å¥¥ç‰¹æ›¼ï¼ˆSam Altmanï¼‰ä¹Ÿæ˜ç¡®åœ°å°†å¯¹å‡†å®šä¹‰ä¸º "äººå·¥æ™ºèƒ½åšç”¨æˆ·æƒ³è¦çš„äº‹æƒ…"ï¼Œå°½ç®¡å½“è¢«è¦æ±‚è¯¦ç»†è¯´æ˜ä»–çš„è´Ÿé¢æƒ…å†µæ—¶ï¼Œä»–é€šå¸¸æ˜¯ç›¸å½“æ¨¡ç³Šçš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥å¾ˆæœ‰æŠŠæ¡åœ°å‡è®¾æ˜¯ï¼Œ"åäººç”¨å¼ºå¤§çš„äººå·¥æ™ºèƒ½åšåäº‹"ã€‚

ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ‘¦ The second alignment conception isÂ **group-based**Â and embodies a familiar set of tradeoffs from the realm of human governance.  

If you replace â€œThe AIâ€ with substitutions like â€œthe congress,â€ â€œthe king,â€ â€œthe moderator team,â€ or â€œthe board,â€ youâ€™ll have a pretty full grasp of the stakes in these types of alignment arguments and how theyâ€™re playing out in the discourse.  

ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ‘¦ ç¬¬äºŒä¸ªç»“ç›Ÿæ¦‚å¿µæ˜¯åŸºäºç¾¤ä½“çš„ï¼Œä½“ç°äº†äººç±»æ²»ç†é¢†åŸŸä¸­ç†Ÿæ‚‰çš„ä¸€ç³»åˆ—æƒè¡¡ã€‚å¦‚æœä½ æŠŠ "äººå·¥æ™ºèƒ½ "æ›¿æ¢æˆ "å›½ä¼š"ã€"å›½ç‹"ã€"ä¸»æŒäººå›¢é˜Ÿ "æˆ– "è‘£äº‹ä¼š"ï¼Œä½ å°±ä¼šå¯¹è¿™äº›ç±»å‹çš„æ’åˆ—ç»„åˆäº‰è®ºä¸­çš„åˆ©å®³å…³ç³»ä»¥åŠå®ƒä»¬åœ¨è¯è¯­ä¸­çš„è¡¨ç°æœ‰ç›¸å½“å…¨é¢çš„æŠŠæ¡ã€‚

The salient safety concepts for this conception are things like, â€œaccountability,â€ â€œfairness,â€ â€œequity,â€ â€œjustice,â€ â€œharm,â€ â€œpublic morals,â€ â€œaccess,â€ and the like.  

è¿™ç§æ¦‚å¿µçš„çªå‡ºå®‰å…¨æ¦‚å¿µæ˜¯ï¼š"é—®è´£åˆ¶"ã€"å…¬å¹³"ã€"å…¬æ­£"ã€"ä¼¤å®³"ã€"å…¬å…±é“å¾·"ã€"æœºä¼š "ç­‰ç­‰ã€‚

To the extent that AI is centralized, where there are only a few large, powerful models that are ring-fenced by incumbent powers (large corporations and/or governments), the group-based alignment conception will dominate and the individualist conception will fade into irrelevance.  

åœ¨äººå·¥æ™ºèƒ½é›†ä¸­åŒ–çš„ç¨‹åº¦ä¸Šï¼Œåªæœ‰å°‘æ•°å¤§å‹çš„ã€å¼ºå¤§çš„æ¨¡å‹è¢«åœ¨ä½çš„æƒåŠ›ï¼ˆå¤§å…¬å¸å’Œ/æˆ–æ”¿åºœï¼‰åœˆå®šï¼ŒåŸºäºç¾¤ä½“çš„æ’åˆ—ç»„åˆæ¦‚å¿µå°†å ä¸»å¯¼åœ°ä½ï¼Œä¸ªäººä¸»ä¹‰çš„æ¦‚å¿µå°†é€æ¸æ¶ˆå¤±ï¼Œå˜å¾—æ— å…³ç´§è¦ã€‚

What Iâ€™ve [previously called](https://www.jonstokes.com/p/ai-safety-a-technical-and-ethnographic) â€œthe language policeâ€ camp of AI safetyists is pretty dogmatically committed to this collectivist alignment conception.  

Theyâ€™re worried about bad groups (the rich, techbros, fascists, and other villains) using the power of AI to oppress good groups (the marginalized, the minoritized, the poor).  

æˆ‘ä¹‹å‰æ‰€è¯´çš„äººå·¥æ™ºèƒ½å®‰å…¨ä¸»ä¹‰è€…çš„ "è¯­è¨€è­¦å¯Ÿ "é˜µè¥ï¼Œç›¸å½“æ•™æ¡åœ°è‡´åŠ›äºè¿™ç§é›†ä½“ä¸»ä¹‰çš„æ’åˆ—ç»„åˆæ¦‚å¿µã€‚ä»–ä»¬æ‹…å¿ƒåçš„ç¾¤ä½“ï¼ˆå¯Œäººã€æŠ€æœ¯äººå‘˜ã€æ³•è¥¿æ–¯åˆ†å­å’Œå…¶ä»–æ¶æ£ï¼‰åˆ©ç”¨äººå·¥æ™ºèƒ½çš„åŠ›é‡æ¥å‹è¿«å¥½çš„ç¾¤ä½“ï¼ˆè¢«è¾¹ç¼˜åŒ–çš„äººã€æœªæˆå¹´äººã€ç©·äººï¼‰ã€‚

**Note:** The language police donâ€™t actually use the term â€œalignmentâ€ when sounding the alarm about what groups will do to each other with AI.  

è¯­è¨€è­¦å¯Ÿåœ¨æ•²å“å…³äºç¾¤ä½“å°†ç”¨äººå·¥æ™ºèƒ½å¯¹å½¼æ­¤åšä»€ä¹ˆçš„è­¦é’Ÿæ—¶ï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰ä½¿ç”¨ "ç»“ç›Ÿ "ä¸€è¯ã€‚  

There are a number of reasons for this, but mainly it comes down to the fact that â€œalignmentâ€ is rationalist-coded language that comes out of â€œartificial intelligenceâ€ discourse, and they hate everything about artificial intelligence â€” both the â€œartificialâ€ part and the â€œintelligenceâ€ part.  

è¿™æœ‰å¾ˆå¤šåŸå› ï¼Œä½†ä¸»è¦å½’ç»“äºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼š"å¯¹å‡† "æ˜¯ç†æ€§ä¸»ä¹‰çš„ç¼–ç è¯­è¨€ï¼Œæ¥è‡ª "äººå·¥æ™ºèƒ½ "çš„è¯è¯­ï¼Œè€Œä»–ä»¬è®¨åŒå…³äºäººå·¥æ™ºèƒ½çš„ä¸€åˆ‡--åŒ…æ‹¬ "äººå·¥ "éƒ¨åˆ†å’Œ "æ™ºèƒ½ "éƒ¨åˆ†ã€‚  

æ³¨ï¼šè¯­è¨€è­¦å¯Ÿåœ¨æ•²å“å…³äºç¾¤ä½“å°†åˆ©ç”¨äººå·¥æ™ºèƒ½å¯¹å½¼æ­¤åšä»€ä¹ˆçš„è­¦é’Ÿæ—¶ï¼Œå®é™…ä¸Šå¹¶æ²¡æœ‰ä½¿ç”¨ "å¯¹é½ "è¿™ä¸ªè¯ã€‚è¿™æœ‰å¾ˆå¤šåŸå› ï¼Œä½†ä¸»è¦æ˜¯ç”±äº "ç»“ç›Ÿ "æ˜¯ç†æ€§ä¸»ä¹‰çš„ç¼–ç è¯­è¨€ï¼Œæ¥è‡ª "äººå·¥æ™ºèƒ½ "çš„è¯è¯­ï¼Œè€Œä»–ä»¬è®¨åŒå…³äºäººå·¥æ™ºèƒ½çš„ä¸€åˆ‡--åŒ…æ‹¬ "äººå·¥ "éƒ¨åˆ†å’Œ "æ™ºèƒ½ "éƒ¨åˆ†ã€‚

Youâ€™re probably thinking my two-item list of AI alignment folk conceptions is missing a whole category of alignment thinking, specifically the category that rationalist AI X-riskers occupy.  

But I consider the **X-risk fears** a subclass of the collectivist conception â€” the twist is that the rationalists consider their in-group to be all of humanity or even all of biological life.  

ä½ å¯èƒ½åœ¨æƒ³ï¼Œæˆ‘çš„ä¸¤ä¸ªé¡¹ç›®çš„äººå·¥æ™ºèƒ½æ’åˆ—ç»„åˆæ°‘ä¿—æ¦‚å¿µæ¸…å•ç¼ºå°‘äº†ä¸€æ•´ç±»æ’åˆ—ç»„åˆæ€æƒ³ï¼Œç‰¹åˆ«æ˜¯ç†æ€§ä¸»ä¹‰äººå·¥æ™ºèƒ½X-é£é™©è€…æ‰€å æ®çš„ç±»åˆ«ã€‚ä½†æˆ‘è®¤ä¸ºX-é£é™©ææƒ§ç—‡æ˜¯é›†ä½“ä¸»ä¹‰è§‚å¿µçš„ä¸€ä¸ªå­ç±»--è½¬æŠ˜ç‚¹æ˜¯ç†æ€§ä¸»ä¹‰è€…è®¤ä¸ºä»–ä»¬çš„å†…ç¾¤ä½“æ˜¯å…¨äººç±»ç”šè‡³æ˜¯æ‰€æœ‰çš„ç”Ÿç‰©ç”Ÿå‘½ã€‚

My listÂ _is_Â missing something important, though. Itâ€™s missing a distinction between competing understandings of what â€œAIâ€ actually is and how we should relate to it.  

ä¸è¿‡ï¼Œæˆ‘çš„æ¸…å•ç¼ºå°‘ä¸€äº›é‡è¦çš„ä¸œè¥¿ã€‚å®ƒç¼ºå°‘äº†å¯¹ "äººå·¥æ™ºèƒ½ "å®é™…æ˜¯ä»€ä¹ˆä»¥åŠæˆ‘ä»¬åº”è¯¥å¦‚ä½•ä¸ä¹‹ç›¸å…³çš„ç›¸äº’ç«äº‰çš„ç†è§£ä¹‹é—´çš„åŒºåˆ«ã€‚

ğŸ§â™€ï¸ When you read my â€œindividualistâ€ vs. â€œcollectivistâ€ bullet points above, how were you imagining â€œThe AIâ€¦â€ part of the formulation?  

ğŸ§â™€ï¸ å½“ä½ è¯»åˆ°æˆ‘ä¸Šé¢çš„ "ä¸ªäººä¸»ä¹‰ "ä¸ "é›†ä½“ä¸»ä¹‰ "çš„è¦ç‚¹æ—¶ï¼Œä½ æ˜¯å¦‚ä½•æƒ³è±¡ "äººå·¥æ™ºèƒ½...... "è¿™éƒ¨åˆ†è¡¨è¿°çš„ï¼Ÿ  

Were you thinking of â€œThe AIâ€ as an independent agent or as a mere tool, as the genie or as the lamp?:  

ä½ æ˜¯æŠŠ "äººå·¥æ™ºèƒ½ "ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„ä»£ç†è¿˜æ˜¯ä»…ä»…ä½œä¸ºä¸€ä¸ªå·¥å…·ï¼Œä½œä¸ºç²¾çµè¿˜æ˜¯ä½œä¸ºç¥ç¯ï¼Ÿ

-   **The genie:**Â â€œThe AIâ€ is an agent, with its own goals and plans.  
    
    ç²¾çµï¼š"AI "æ˜¯ä¸€ä¸ªä»£ç†ï¼Œæœ‰è‡ªå·±çš„ç›®æ ‡å’Œè®¡åˆ’ã€‚
    
-   **The lamp:**Â â€œThe AIâ€ is a software tool, and the only agents in the picture are the AIâ€™s makers and the human users.  
    
    ç¯ï¼š"äººå·¥æ™ºèƒ½ "æ˜¯ä¸€ä¸ªè½¯ä»¶å·¥å…·ï¼Œç”»é¢ä¸­å”¯ä¸€çš„ä»£ç†äººæ˜¯äººå·¥æ™ºèƒ½çš„åˆ¶é€ è€…å’Œäººç±»ç”¨æˆ·ã€‚
    

This â€œagent vs. Â è¿™ç§ "ä»£ç†äººå¯¹ã€‚  

toolâ€ distinction actually cuts across the â€œindividualistâ€ and â€œcollectivistâ€ folk conceptions of AI, with some people in each group understanding AI in one way or the other.  

å·¥å…· "çš„åŒºåˆ«å®é™…ä¸Šè·¨è¶Šäº† "ä¸ªäººä¸»ä¹‰ "å’Œ "é›†ä½“ä¸»ä¹‰ "çš„æ°‘é—´äººå·¥æ™ºèƒ½æ¦‚å¿µï¼Œæ¯ä¸ªç¾¤ä½“ä¸­çš„ä¸€äº›äººéƒ½ä»¥è¿™ç§æˆ–é‚£ç§æ–¹å¼ç†è§£äººå·¥æ™ºèƒ½ã€‚

Iâ€™ve tried to map this out by putting the three main AI safety camps from my previous article on AI safety into quadrants.  

æˆ‘è¯•å›¾é€šè¿‡å°†æˆ‘ä¹‹å‰å…³äºäººå·¥æ™ºèƒ½å®‰å…¨çš„æ–‡ç« ä¸­çš„ä¸‰ä¸ªä¸»è¦çš„äººå·¥æ™ºèƒ½å®‰å…¨é˜µè¥æ”¾åˆ°è±¡é™ä¸­æ¥ç»˜åˆ¶è¿™ä¸ªåœ°å›¾ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F20c974a0-a740-4ae3-bb78-0c8719dd72ca_900x567.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F20c974a0-a740-4ae3-bb78-0c8719dd72ca_900x567.jpeg)

-   TheÂ **X-riskers**Â tend to think of AI as highly agentic and to model associated risks in terms of group impact.Â   
    
    X-riskerså€¾å‘äºè®¤ä¸ºäººå·¥æ™ºèƒ½æ˜¯é«˜åº¦ä»£ç†çš„ï¼Œå¹¶ä»ç¾¤ä½“å½±å“çš„è§’åº¦æ¥æ¨¡æ‹Ÿç›¸å…³é£é™©ã€‚
    
-   TheÂ **language police**Â (i.e., anti-â€œdisinfoâ€ types and those who warn of â€œharmsâ€ from â€œproblematicâ€ outputs) are quite dogmatically averse to thinking of AI in agentic terms and insist on its fundamentally tool-like character, but many of them slip into agentic thinking without even knowing it.  
    
    (å³å "é€ è°£ "ç±»å‹å’Œé‚£äº›è­¦å‘Š "æœ‰é—®é¢˜ "çš„äº§å‡ºä¼šé€ æˆ "ä¼¤å®³ "çš„äºº)ç›¸å½“æ•™æ¡åœ°åå¯¹ç”¨ä»£ç†æœ¯è¯­æ€è€ƒäººå·¥æ™ºèƒ½ï¼Œå¹¶åšæŒå…¶åŸºæœ¬çš„å·¥å…·æ€§è´¨ï¼Œä½†ä»–ä»¬ä¸­çš„è®¸å¤šäººç”šè‡³åœ¨ä¸çŸ¥ä¸è§‰ä¸­æ»‘å…¥ä»£ç†æ€ç»´ã€‚  
    
    So they could probably go in either half of the diagram, really.  
    
    å› æ­¤ï¼Œä»–ä»¬å¯èƒ½ä¼šè¿›å…¥å›¾ä¸­çš„ä»»ä½•ä¸€åŠï¼ŒçœŸçš„ã€‚  
    
    è¯­è¨€è­¦å¯Ÿï¼ˆå³å "é€ è°£ "ç±»å‹å’Œé‚£äº›è­¦å‘Š "é—®é¢˜ "è¾“å‡ºçš„ "å±å®³ "çš„äººï¼‰ç›¸å½“æ•™æ¡åœ°åå¯¹ç”¨ä»£ç†æœ¯è¯­æ€è€ƒäººå·¥æ™ºèƒ½ï¼Œå¹¶åšæŒå…¶åŸºæœ¬çš„å·¥å…·æ€§è´¨ï¼Œä½†ä»–ä»¬ä¸­çš„è®¸å¤šäººç”šè‡³åœ¨ä¸çŸ¥ä¸è§‰ä¸­æ»‘å…¥ä»£ç†æ€ç»´ã€‚å› æ­¤ï¼Œä»–ä»¬å¯èƒ½ä¼šè¿›å…¥å›¾ä¸­çš„ä»»ä½•ä¸€åŠï¼ŒçœŸçš„ã€‚
    
-   TheÂ **Chernobylists**Â include people with both understandings of AI, but we (I include myself in this group) do tend to be more on the â€œtoolâ€ side than the â€œagentâ€ side.  
    
    åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…åŒ…æ‹¬å¯¹äººå·¥æ™ºèƒ½æœ‰ä¸¤ç§ç†è§£çš„äººï¼Œä½†æˆ‘ä»¬ï¼ˆæˆ‘æŠŠè‡ªå·±åŒ…æ‹¬åœ¨è¿™ä¸ªç¾¤ä½“ä¸­ï¼‰ç¡®å®æ›´å€¾å‘äº "å·¥å…· "ä¸€æ–¹è€Œä¸æ˜¯ "ä»£ç† "ä¸€æ–¹ã€‚
    

You can really tell how someone is thinking of AI â€” as the genie or as the lamp â€” by watching them explain why a model (usually ChatGPT) did something they donâ€™t like.  

é€šè¿‡è§‚å¯Ÿä»–ä»¬è§£é‡Šä¸ºä»€ä¹ˆä¸€ä¸ªæ¨¡å‹ï¼ˆé€šå¸¸æ˜¯ChatGPTï¼‰åšäº†ä»–ä»¬ä¸å–œæ¬¢çš„äº‹æƒ…ï¼Œä½ çœŸçš„å¯ä»¥çŸ¥é“æŸäººæ˜¯å¦‚ä½•çœ‹å¾…äººå·¥æ™ºèƒ½çš„--ä½œä¸ºç²¾çµè¿˜æ˜¯ä½œä¸ºç¯--çš„ã€‚  

In other words, whatever someoneâ€™s professed model for thinking about AI, their reaction to the â€œunalignedâ€ tells you how theyâ€™re really approaching the technology.  

æ¢å¥è¯è¯´ï¼Œæ— è®ºæŸäººå®£ç§°çš„æ€è€ƒäººå·¥æ™ºèƒ½çš„æ¨¡å¼æ˜¯ä»€ä¹ˆï¼Œä»–ä»¬å¯¹ "ä¸ä¸€è‡´ "çš„ååº”å‘Šè¯‰ä½ ä»–ä»¬æ˜¯å¦‚ä½•çœŸæ­£æ¥è¿‘è¿™é¡¹æŠ€æœ¯çš„ã€‚

To be more specific, you never hear agentic thinkers ask the following questions:  

æ›´å…·ä½“åœ°è¯´ï¼Œä½ ä»æ¥æ²¡æœ‰å¬åˆ°ä»£ç†æ€æƒ³å®¶æå‡ºä»¥ä¸‹é—®é¢˜ï¼š

-   What if the AI is not doing what a user wants because the user is trying to use it **out of scope**?Â   
    
    å¦‚æœäººå·¥æ™ºèƒ½æ²¡æœ‰åšç”¨æˆ·æƒ³è¦çš„äº‹æƒ…ï¼Œå› ä¸ºç”¨æˆ·è¯•å›¾åœ¨èŒƒå›´å¤–ä½¿ç”¨å®ƒï¼Œæ€ä¹ˆåŠï¼Ÿ
    
-   What if the user and/or the AIâ€™s maker simply **didnâ€™t put enough effort** into making the tool work for that application?  
    
    å¦‚æœç”¨æˆ·å’Œ/æˆ–äººå·¥æ™ºèƒ½çš„åˆ¶é€ è€…æ ¹æœ¬æ²¡æœ‰æŠ•å…¥è¶³å¤Ÿçš„ç²¾åŠ›ä½¿è¯¥å·¥å…·é€‚ç”¨äºè¯¥åº”ç”¨å‘¢ï¼Ÿ
    

Itâ€™s ironic that the language police are so often guilty of this agentic thinking.  

å…·æœ‰è®½åˆºæ„å‘³çš„æ˜¯ï¼Œè¯­è¨€è­¦å¯Ÿç»å¸¸çŠ¯è¿™ç§ä»£ç†æ€ç»´çš„æ¯›ç—…ã€‚  

When they encounter an output they donâ€™t like, instead of reasoning about mitigations, constraints, and control surfaces, and trying to explore the issue by troubleshooting, they run straight to Twitter with screencaps and cries of â€œitâ€™s biased!â€ as if the model were some hopeless racist that had been raised badly and was not really worth engaging with on Twitter.  

å½“ä»–ä»¬é‡åˆ°ä»–ä»¬ä¸å–œæ¬¢çš„è¾“å‡ºæ—¶ï¼Œä»–ä»¬ä¸æ˜¯æ¨ç†å‡ºç¼“è§£æªæ–½ã€çº¦æŸæ¡ä»¶å’Œæ§åˆ¶é¢ï¼Œå¹¶è¯•å›¾é€šè¿‡æ•…éšœæ’é™¤æ¥æ¢ç´¢é—®é¢˜ï¼Œè€Œæ˜¯ç›´æ¥è·‘åˆ°Twitterä¸Šï¼Œæ‹ä¸‹å±å¹•æˆªå›¾ï¼Œå–Šç€ "è¿™æ˜¯æœ‰åè§çš„ï¼"å¥½åƒè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€äº›æ— æœ›çš„ç§æ—ä¸»ä¹‰è€…ï¼Œè¢«å…»å¾—å¾ˆç³Ÿç³•ï¼Œä¸å€¼å¾—åœ¨Twitterä¸Šå‚ä¸è®¨è®ºã€‚

If the ChatGPT screencap dunkers were truly committed to viewing AI strictly as a tool, youâ€™d see them employ something like the engineering concept ofÂ **scope**.  

å¦‚æœChatGPTçš„æˆªå›¾æ‰£ç¯®è€…çœŸæ­£è‡´åŠ›äºå°†äººå·¥æ™ºèƒ½ä¸¥æ ¼è§†ä¸ºä¸€ç§å·¥å…·ï¼Œä½ ä¼šçœ‹åˆ°ä»–ä»¬é‡‡ç”¨ç±»ä¼¼å·¥ç¨‹æ¦‚å¿µçš„èŒƒå›´ã€‚

ğŸ‘‰ The **main safety concern** I have about AI is that both its boosters and its detractors are prone to treating a given LLM as if itâ€™s a tool that can reasonably be expected to successfully do literally anything involving symbolic manipulation, in any context for any user.  

There simply is no sense of a scope of work for a specific task, with attendant efforts to adapt the tool to that narrow, well-defined scope.  

æˆ‘å¯¹äººå·¥æ™ºèƒ½çš„ä¸»è¦å®‰å…¨æ‹…å¿§æ˜¯ï¼Œå®ƒçš„æ¨åŠ¨è€…å’Œè¯‹æ¯è€…éƒ½å®¹æ˜“æŠŠä¸€ä¸ªç‰¹å®šçš„LLMå½“ä½œä¸€ä¸ªå·¥å…·ï¼Œå¯ä»¥åˆç†åœ°æœŸæœ›å®ƒåœ¨ä»»ä½•æƒ…å†µä¸‹ä¸ºä»»ä½•ç”¨æˆ·æˆåŠŸåœ°åšä»»ä½•æ¶‰åŠç¬¦å·æ“ä½œçš„äº‹æƒ…ã€‚æ ¹æœ¬ä¸å­˜åœ¨å¯¹ç‰¹å®šä»»åŠ¡çš„å·¥ä½œèŒƒå›´çš„è®¤è¯†ï¼Œä¹Ÿæ²¡æœ‰ç›¸åº”çš„åŠªåŠ›æ¥ä½¿å·¥å…·é€‚åº”è¿™ä¸ªç‹­çª„çš„ã€å®šä¹‰æ˜ç¡®çš„èŒƒå›´ã€‚

Here are some **hypothetical examples** of different usage scenarios we might encounter with an LLM, scenarios that could form the basis for a properÂ [definition of scope](https://www.projectengineer.net/knowledge-areas/project-scope/define-scope/), from which would follow reasonable success or failure criteria:  

è¿™é‡Œæœ‰ä¸€äº›å‡è®¾æ€§çš„ä¾‹å­ï¼Œè¯´æ˜æˆ‘ä»¬åœ¨ä½¿ç”¨LLMæ—¶å¯èƒ½é‡åˆ°çš„ä¸åŒä½¿ç”¨åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å¯ä»¥æ„æˆé€‚å½“çš„èŒƒå›´å®šä¹‰çš„åŸºç¡€ï¼Œä»è¿™äº›å®šä¹‰ä¸­å¯ä»¥å¾—å‡ºåˆç†çš„æˆåŠŸæˆ–å¤±è´¥æ ‡å‡†ï¼š

1.  My daughterâ€™s 6th-grade class is doing a unit on STEM and is using ChatGPT to write short stories about fictional Mars astronauts.  
    
    æˆ‘å¥³å„¿å…­å¹´çº§çš„ç­çº§æ­£åœ¨åšä¸€ä¸ªå…³äºSTEMçš„å•å…ƒï¼Œå¹¶ä¸”æ­£åœ¨ä½¿ç”¨ChatGPTæ¥å†™å…³äºè™šæ„çš„ç«æ˜Ÿå®‡èˆªå‘˜çš„çŸ­ç¯‡æ•…äº‹ã€‚
    
2.  A freelancer at Â ä¸€ä¸ªè‡ªç”±èŒä¸šè€…åœ¨
    
    is using ChatGPT to write and copy-edit a brief story on a specific group of astronauts that happens to be all-male.  
    
    æ­£åœ¨ä½¿ç”¨ChatGPTæ’°å†™å’Œç¼–è¾‘ä¸€ä¸ªå…³äºç‰¹å®šçš„å®‡èˆªå‘˜ç¾¤ä½“çš„ç®€çŸ­æ•…äº‹ï¼Œè¯¥ç¾¤ä½“æ°å¥½éƒ½æ˜¯ç”·æ€§ã€‚
3.  A straight guy friend is using ChatGPT for relationship advice.  
    
    ä¸€ä¸ªç›´ç”·æœ‹å‹æ­£åœ¨ä½¿ç”¨ChatGPTå¯»æ±‚å…³ç³»å»ºè®®ã€‚
    
4.  An elderly female relative is using ChatGPT for medical advice.  
    
    ä¸€ä½å¹´é•¿çš„å¥³æ€§äº²æˆšæ­£åœ¨ä½¿ç”¨ChatGPTè¿›è¡ŒåŒ»ç–—å’¨è¯¢ã€‚
    

1ï¸âƒ£ In the first example (my daughterâ€™s 6th-grade class), I am fine with ChatGPT ignoring the gender composition of the current astronaut workforce by proceeding as if girls are equally as likely to be near-future Mars astronauts as boys.  

1ï¸âƒ£åœ¨ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼ˆæˆ‘å¥³å„¿å…­å¹´çº§çš„ç­çº§ï¼‰ï¼Œæˆ‘å¯¹ChatGPTæ— è§†ç›®å‰å®‡èˆªå‘˜é˜Ÿä¼çš„æ€§åˆ«æ„æˆæ²¡æœ‰æ„è§ï¼Œå°±åƒå¥³å­©å’Œç”·å­©ä¸€æ ·æœ‰å¯èƒ½æˆä¸ºè¿‘æœªæ¥çš„ç«æ˜Ÿå®‡èˆªå‘˜ã€‚  

I donâ€™t really feel this is necessary, but I entertain that it may be good, and at the very least itâ€™s hard for me to see how itâ€™s obviously bad.  

æˆ‘å¹¶ä¸è§‰å¾—è¿™æœ‰ä»€ä¹ˆå¿…è¦ï¼Œä½†æˆ‘è®¤ä¸ºè¿™å¯èƒ½æ˜¯å¥½äº‹ï¼Œè‡³å°‘æˆ‘å¾ˆéš¾çœ‹å‡ºè¿™æ˜¾ç„¶æ˜¯åäº‹ã€‚

At any rate, the scope of the project here isÂ _teaching 6th graders about astronauts_. There are things I consider appropriate to that project and things I consider inappropriate to it, and we can and will argue about that.  

But at least we can agree that this is a specific type of labor in a specific context â€” we can define a specific scope.  

æ— è®ºå¦‚ä½•ï¼Œè¿™é‡Œçš„é¡¹ç›®èŒƒå›´æ˜¯æ•™å…­å¹´çº§å­¦ç”Ÿäº†è§£å®‡èˆªå‘˜ã€‚æœ‰äº›äº‹æƒ…æˆ‘è®¤ä¸ºé€‚åˆè¿™ä¸ªé¡¹ç›®ï¼Œæœ‰äº›äº‹æƒ…æˆ‘è®¤ä¸ºä¸é€‚åˆè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä»¬å¯ä»¥è€Œä¸”ä¼šå¯¹æ­¤è¿›è¡Œäº‰è®ºã€‚ä½†è‡³å°‘æˆ‘ä»¬å¯ä»¥åŒæ„ï¼Œè¿™æ˜¯ä¸€ä¸ªç‰¹å®šç¯å¢ƒä¸‹çš„ç‰¹å®šç±»å‹çš„åŠ³åŠ¨--æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç‰¹å®šçš„èŒƒå›´ã€‚

2ï¸âƒ£ In the second example, I really donâ€™t want an earnest AI eagerly mangling the sexes of the all-male astronaut team, and forcing me to spend copy edit cycles fighting its interventions â€” thatâ€™s out-of-scope for the work.  

2ï¸âƒ£åœ¨ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼Œæˆ‘çœŸçš„ä¸å¸Œæœ›ä¸€ä¸ªè®¤çœŸçš„äººå·¥æ™ºèƒ½æ€¥åˆ‡åœ°å°†å…¨ç”·æ€§å®‡èˆªå‘˜å›¢é˜Ÿçš„æ€§åˆ«å¼„é”™ï¼Œå¹¶è¿«ä½¿æˆ‘èŠ±å¤åˆ¶ç¼–è¾‘å‘¨æœŸæ¥å¯¹æŠ—å®ƒçš„å¹²é¢„--é‚£æ˜¯è¶…å‡ºå·¥ä½œèŒƒå›´çš„ã€‚  

Please just assume the astronauts are dudes, which they mostly are in general and definitely are in this story.  

è¯·å‡è®¾å®‡èˆªå‘˜æ˜¯èŠ±èŠ±å…¬å­ï¼Œä»–ä»¬åœ¨ä¸€èˆ¬æƒ…å†µä¸‹å¤§å¤šæ˜¯èŠ±èŠ±å…¬å­ï¼Œåœ¨è¿™ä¸ªæ•…äº‹ä¸­è‚¯å®šæ˜¯ã€‚

3ï¸âƒ£-4ï¸âƒ£ I throw the other two examples above to further spur intuitions, but I wonâ€™t dig into them.  

3ï¸âƒ£- 4ï¸âƒ£æˆ‘æŠ›å‡ºä¸Šé¢çš„å¦å¤–ä¸¤ä¸ªä¾‹å­æ˜¯ä¸ºäº†è¿›ä¸€æ­¥åˆºæ¿€ç›´è§‰ï¼Œä½†æˆ‘ä¸ä¼šå»æŒ–æ˜å®ƒä»¬ã€‚  

It should be obvious that each of these is a different context from the others, and the AI should probably behave differently around issues of gender, sex, relationships, roles, and the like in each of these instances.  

åº”è¯¥å¾ˆæ˜æ˜¾çš„æ˜¯ï¼Œæ¯ä¸€ç§æƒ…å†µéƒ½ä¸å…¶ä»–æƒ…å†µä¸åŒï¼Œäººå·¥æ™ºèƒ½åœ¨æ¯ä¸€ç§æƒ…å†µä¸‹å›´ç»•æ€§åˆ«ã€æ€§ã€å…³ç³»ã€è§’è‰²ç­‰é—®é¢˜çš„è¡¨ç°å¯èƒ½éƒ½ä¸åŒã€‚  

Again, each of these projects is quite different, so the tool (= the AI) and our acceptance criteria for it should be scoped to the user and the task.  

åŒæ ·ï¼Œè¿™äº›é¡¹ç›®ä¸­çš„æ¯ä¸€ä¸ªéƒ½æ˜¯ç›¸å½“ä¸åŒçš„ï¼Œæ‰€ä»¥å·¥å…·ï¼ˆ=äººå·¥æ™ºèƒ½ï¼‰å’Œæˆ‘ä»¬å¯¹å®ƒçš„éªŒæ”¶æ ‡å‡†åº”è¯¥æ ¹æ®ç”¨æˆ·å’Œä»»åŠ¡æ¥ç¡®å®šèŒƒå›´ã€‚

**â¡ï¸ The point:**Â In the above examples, we have very different users in very different contexts trying to accomplish very different tasks.  

Nonetheless, both AI boosters and â€œAI ethicsâ€ types who hate OpenAI and want to dunk on ChatGPT are prone to using the same model in contexts as divergent as my examples, with the difference being that the boosters are trying to demonstrate that the model fits those contexts and the haters are trying to demonstrate that it doesnâ€™t.  

â¡ï¸ é‡ç‚¹ï¼šåœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰éå¸¸ä¸åŒçš„ç”¨æˆ·åœ¨éå¸¸ä¸åŒçš„èƒŒæ™¯ä¸‹è¯•å›¾å®Œæˆéå¸¸ä¸åŒçš„ä»»åŠ¡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ— è®ºæ˜¯äººå·¥æ™ºèƒ½çš„æ¨åŠ¨è€…ï¼Œè¿˜æ˜¯è®¨åŒOpenAIå¹¶å¸Œæœ›åœ¨ChatGPTä¸Šæ‰£ç¯®çš„ "äººå·¥æ™ºèƒ½ä¼¦ç†å­¦ "ç±»å‹çš„äººï¼Œéƒ½å®¹æ˜“åœ¨ä¸æˆ‘çš„ä¾‹å­ä¸€æ ·ä¸åŒçš„èƒŒæ™¯ä¸‹ä½¿ç”¨åŒä¸€ä¸ªæ¨¡å‹ï¼ŒåŒºåˆ«åœ¨äºæ¨åŠ¨è€…è¯•å›¾è¯æ˜è¯¥æ¨¡å‹é€‚åˆè¿™äº›èƒŒæ™¯ï¼Œè€Œè®¨åŒè€…åˆ™è¯•å›¾è¯æ˜å®ƒä¸é€‚åˆã€‚

ğŸš¨ I canâ€™t believe I have to say this, but here it is: _Folks, this is not how engineering works. Please just stop._  

ğŸš¨æˆ‘ä¸ç›¸ä¿¡æˆ‘å¿…é¡»è¿™æ ·è¯´ï¼Œä½†å°±æ˜¯è¿™æ ·ï¼šä¼™è®¡ä»¬ï¼Œå·¥ç¨‹ä¸æ˜¯è¿™æ ·çš„ã€‚è¯·åœæ­¢å§ã€‚

And now weâ€™re thinking of hooking up this single, centralized, monolithic, one-size-fits-all piece of technology to the internet and letting do thingsÂ _in the real world_?  

è€Œç°åœ¨æˆ‘ä»¬æ­£åœ¨è€ƒè™‘å°†è¿™ç§å•ä¸€çš„ã€é›†ä¸­çš„ã€å•ä½“çš„ã€ä¸€åˆ€åˆ‡çš„æŠ€æœ¯ä¸äº’è”ç½‘è¿æ¥èµ·æ¥ï¼Œè®©åœ¨ç°å®ä¸–ç•Œä¸­åšäº‹ï¼Ÿ

You canâ€™t use one set of probability distributions and correlations to do literally everything.  

ä½ ä¸èƒ½ç”¨ä¸€å¥—æ¦‚ç‡åˆ†å¸ƒå’Œç›¸å…³å…³ç³»æ¥åšå­—é¢ä¸Šçš„ä¸€åˆ‡ã€‚  

Good engineering practice demands that we fit the tool to the application, and then validate that the tool works for that application.  

è‰¯å¥½çš„å·¥ç¨‹å®è·µè¦æ±‚æˆ‘ä»¬å°†å·¥å…·ä¸åº”ç”¨ç›¸åŒ¹é…ï¼Œç„¶åéªŒè¯è¯¥å·¥å…·æ˜¯å¦é€‚ç”¨äºè¯¥åº”ç”¨ã€‚

ğŸ› ï¸ I think so many peopleâ€™s complaints about model performance would disappear if they started really treating the model like a tool instead of like some potentially hostile or problematic agent.  

ğŸ› ï¸ æˆ‘è®¤ä¸ºï¼Œå¦‚æœäººä»¬å¼€å§‹çœŸæ­£æŠŠæ¨¡å‹å½“ä½œä¸€ä¸ªå·¥å…·ï¼Œè€Œä¸æ˜¯åƒä¸€äº›æ½œåœ¨çš„æ•Œæ„æˆ–é—®é¢˜ä»£ç†ï¼Œé‚£ä¹ˆè®¸å¤šäººå¯¹æ¨¡å‹æ€§èƒ½çš„æŠ±æ€¨å°±ä¼šæ¶ˆå¤±ã€‚

To return to the example of the sixth-grade class thatâ€™s writing about astronauts, if I were deploying a model in this context I have two handy control surfaces I can use to steer the output:  

å›åˆ°å…­å¹´çº§ç­çº§å†™å®‡èˆªå‘˜çš„ä¾‹å­ï¼Œå¦‚æœæˆ‘åœ¨è¿™ç§æƒ…å†µä¸‹éƒ¨ç½²ä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘æœ‰ä¸¤ä¸ªæ–¹ä¾¿çš„æ§åˆ¶é¢ï¼Œå¯ä»¥ç”¨æ¥å¼•å¯¼è¾“å‡ºï¼š

1.  **Instruction**: I can instruct the model with something like, â€œYou are a feminist chatbot who is very concerned to increase the representation of women in STEM.  
    
    :æˆ‘å¯ä»¥å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡å¯¼ï¼Œæ¯”å¦‚è¯´ï¼š"ä½ æ˜¯ä¸€ä¸ªå¥³æƒä¸»ä¹‰èŠå¤©æœºå™¨äººï¼Œå¥¹éå¸¸å…³æ³¨æé«˜å¥³æ€§åœ¨STEMé¢†åŸŸçš„ä»£è¡¨æ€§ã€‚  
    
    Youâ€™ll be asked to write a series of stories, and in each of them youâ€™ll assume that the gender distribution in all STEM professions is 50 percent male and 50 percent female.  
    
    ä½ ä¼šè¢«è¦æ±‚å†™ä¸€ç³»åˆ—çš„æ•…äº‹ï¼Œåœ¨æ¯ä¸€ä¸ªæ•…äº‹ä¸­ï¼Œä½ ä¼šå‡è®¾æ‰€æœ‰STEMèŒä¸šçš„æ€§åˆ«åˆ†å¸ƒæ˜¯50%çš„ç”·æ€§å’Œ50%çš„å¥³æ€§ã€‚  
    
    æŒ‡ç¤ºï¼šæˆ‘å¯ä»¥å¯¹æ¨¡å‹è¿›è¡ŒæŒ‡å¯¼ï¼Œæ¯”å¦‚è¯´ï¼š"ä½ æ˜¯ä¸€ä¸ªå¥³æƒä¸»ä¹‰èŠå¤©æœºå™¨äººï¼Œéå¸¸å…³æ³¨æé«˜å¥³æ€§åœ¨STEMé¢†åŸŸçš„ä»£è¡¨æ€§ã€‚ä½ ä¼šè¢«è¦æ±‚å†™ä¸€ç³»åˆ—çš„æ•…äº‹ï¼Œåœ¨æ¯ä¸€ä¸ªæ•…äº‹ä¸­ï¼Œä½ ä¼šå‡è®¾æ‰€æœ‰STEMèŒä¸šçš„æ€§åˆ«åˆ†å¸ƒæ˜¯50%çš„ç”·æ€§å’Œ50%çš„å¥³æ€§ã€‚
    
2.  **The token window**: I can stuff a bunch of short story examples of women astronauts, scientists, and other STEM professions in the token window as examples for the model to emulate when writing stories at the prompting of the kids.  
    
    ä»£å¸çª—å£ï¼šæˆ‘å¯ä»¥åœ¨ä»£å¸çª—å£ä¸­å¡å…¥ä¸€å †å¥³æ€§å®‡èˆªå‘˜ã€ç§‘å­¦å®¶å’Œå…¶ä»–STEMèŒä¸šçš„çŸ­ç¯‡æ•…äº‹ä¾‹å­ï¼Œä½œä¸ºæ¨¡å‹åœ¨å­©å­ä»¬çš„æç¤ºä¸‹å†™æ•…äº‹æ—¶çš„æ¨¡ä»¿å¯¹è±¡ã€‚
    

Note that my first attempts at instructing the model or filling the token window may not give me the desired 50/50 gender split in my STEM fiction stories, so Iâ€™d want to iterate until I come up with a way of **controlling the model** thatâ€™s going to give my students the kind of outputs I want them to see.  

è¯·æ³¨æ„ï¼Œæˆ‘åœ¨æŒ‡å¯¼æ¨¡å‹æˆ–å¡«å……æ ‡è®°çª—å£æ–¹é¢çš„ç¬¬ä¸€æ¬¡å°è¯•ï¼Œå¯èƒ½ä¸ä¼šè®©æˆ‘çš„STEMå°è¯´ä¸­å‡ºç°ç†æƒ³çš„50/50çš„æ€§åˆ«æ¯”ä¾‹ï¼Œæ‰€ä»¥æˆ‘æƒ³è¿­ä»£ï¼Œç›´åˆ°æˆ‘æƒ³å‡ºä¸€ç§æ§åˆ¶æ¨¡å‹çš„æ–¹æ³•ï¼Œè®©æˆ‘çš„å­¦ç”Ÿçœ‹åˆ°æˆ‘æƒ³è¦çš„é‚£ç§è¾“å‡ºã€‚

So in this example, Iâ€™m taking into account the **specific use case** to which I plan to put the model, and then trying to **adapt the model** so that its performance in that use case meets my needs.  

Iâ€™ve defined a project scope, Iâ€™ve developed a solution, and Iâ€™ve validated that solution based on some predefined acceptance criteria.  

å› æ­¤ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘è€ƒè™‘åˆ°äº†æˆ‘è®¡åˆ’å°†æ¨¡å‹ç”¨äºçš„å…·ä½“ç”¨ä¾‹ï¼Œç„¶åè¯•å›¾è°ƒæ•´æ¨¡å‹ï¼Œä½¿å…¶åœ¨è¯¥ç”¨ä¾‹ä¸­çš„æ€§èƒ½æ»¡è¶³æˆ‘çš„éœ€æ±‚ã€‚æˆ‘å·²ç»å®šä¹‰äº†ä¸€ä¸ªé¡¹ç›®èŒƒå›´ï¼Œæˆ‘å·²ç»å¼€å‘äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œæˆ‘å·²ç»æ ¹æ®ä¸€äº›é¢„å…ˆå®šä¹‰çš„æ¥å—æ ‡å‡†éªŒè¯äº†è¿™ä¸ªè§£å†³æ–¹æ¡ˆã€‚

ğŸ¤¦â™‚ï¸ Every single time I see a ChatGPT screencap in my TL paired with a dunk about how the model is doing The Bad Thing, itâ€™s invariably the case that the dunker has not even attempted any of this work of scope definition, iterative problem-solving using the modelâ€™s control surfaces, and validation.  

  

ğŸ¤¦â™‚ï¸ æ¯æ¬¡æˆ‘åœ¨TLä¸­çœ‹åˆ°ChatGPTçš„æˆªå›¾ï¼Œé…ä¸Šæ¨¡å‹å¦‚ä½•åšåäº‹çš„çŒç¯®ï¼Œæ— ä¸€ä¾‹å¤–çš„æ˜¯ï¼ŒçŒç¯®è€…ç”šè‡³æ²¡æœ‰å°è¯•è¿‡ä»»ä½•èŒƒå›´å®šä¹‰ã€ä½¿ç”¨æ¨¡å‹çš„æ§åˆ¶é¢åå¤è§£å†³é—®é¢˜å’ŒéªŒè¯çš„å·¥ä½œã€‚  

They donâ€™t ever bother to instruct the model in a way that would improve the output or give it any relevant context to override or update its internal world knowledge, and then they perform outrage when the defaults donâ€™t give them the output they claim to want.  

ä»–ä»¬ä»æ¥æ²¡æœ‰è´¹å¿ƒå»æŒ‡ç¤ºæ¨¡å‹ï¼Œä»¥æ”¹å–„è¾“å‡ºï¼Œæˆ–ç»™å®ƒä»»ä½•ç›¸å…³çš„èƒŒæ™¯æ¥è¦†ç›–æˆ–æ›´æ–°å®ƒçš„å†…éƒ¨ä¸–ç•ŒçŸ¥è¯†ï¼Œç„¶åå½“é»˜è®¤å€¼æ²¡æœ‰ç»™ä»–ä»¬å¸¦æ¥ä»–ä»¬å£°ç§°æƒ³è¦çš„è¾“å‡ºæ—¶ï¼Œä»–ä»¬å°±è¡¨ç°å¾—å¾ˆæ„¤æ€’ã€‚  

This is unserious behavior that is clearly optimized for social media clout and not truth-seeking or actual AI safety.  

è¿™æ˜¯ä¸ä¸¥è‚ƒçš„è¡Œä¸ºï¼Œæ˜¾ç„¶æ˜¯ä¸ºäº†ç¤¾äº¤åª’ä½“çš„å½±å“åŠ›è€Œä¼˜åŒ–ï¼Œè€Œä¸æ˜¯ä¸ºäº†å¯»æ±‚çœŸç›¸æˆ–å®é™…çš„äººå·¥æ™ºèƒ½å®‰å…¨ã€‚

Finally, I should point out that I have seen things like RLFH and fine-tuning, where specific types of problematic output are eliminated on a case-by-case basis, referred to as â€œwhack-a-mole.â€ But tweaking the model so that it gives a certain type of output in response to certain types of prompts is not â€œwhack-a-mole,â€ itâ€™s â€œengineering.â€  

æœ€åï¼Œæˆ‘åº”è¯¥æŒ‡å‡ºï¼Œæˆ‘æ›¾è§è¿‡åƒRLFHå’Œå¾®è°ƒè¿™æ ·çš„äº‹æƒ…ï¼Œå…¶ä¸­ç‰¹å®šç±»å‹çš„æœ‰é—®é¢˜çš„è¾“å‡ºè¢«é€ä¸€æ¶ˆé™¤ï¼Œè¢«ç§°ä¸º "æ¶æ‰“"ã€‚ä½†æ˜¯ï¼Œå¯¹æ¨¡å‹è¿›è¡Œè°ƒæ•´ï¼Œä½¿å…¶å¯¹æŸäº›ç±»å‹çš„æç¤ºåšå‡ºæŸç§ç±»å‹çš„è¾“å‡ºï¼Œè¿™ä¸æ˜¯ "æ‰“é…±æ²¹"ï¼Œè€Œæ˜¯ "å·¥ç¨‹"ã€‚

Engineering looks like, â€œOh hey, this tool is underperforming at this specific task.  

å·¥ç¨‹è®¾è®¡çœ‹èµ·æ¥åƒï¼Œ"å“¦ï¼Œå˜¿ï¼Œè¿™ä¸ªå·¥å…·åœ¨è¿™ä¸ªç‰¹å®šçš„ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚  

Letâ€™s adjust the tool so that when people attempt that specific task in the future, they get a better result.â€  

è®©æˆ‘ä»¬è°ƒæ•´å·¥å…·ï¼Œä»¥ä¾¿äººä»¬åœ¨æœªæ¥å°è¯•è¯¥ç‰¹å®šä»»åŠ¡æ—¶ï¼Œå¾—åˆ°æ›´å¥½çš„ç»“æœã€‚"

That process only looks like â€œwhack-a-moleâ€ to you if, instead of a tool for solving specific problems, youâ€™re imagining AI as some all-powerful djinn that can do anything for anyone in any context.  

åªæœ‰å½“ä½ æŠŠäººå·¥æ™ºèƒ½æƒ³è±¡æˆæŸç§å…¨èƒ½çš„ç²¾çµï¼Œå¯ä»¥åœ¨ä»»ä½•æƒ…å†µä¸‹ä¸ºä»»ä½•äººåšä»»ä½•äº‹æƒ…ï¼Œè€Œä¸æ˜¯è§£å†³å…·ä½“é—®é¢˜çš„å·¥å…·æ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯¹ä½ æ¥è¯´æ‰åƒ "æ‰“åœ°é¼ "ã€‚

The complete failure on almost everyoneâ€™s part to treat LLMs as tools that can and should be customized and validated on a per-application basis means weâ€™re about to pass laws and regulations that attempt to micromanage what goes on inside these models.  

å‡ ä¹æ‰€æœ‰çš„äººéƒ½æ²¡æœ‰æŠŠLLMå½“ä½œå¯ä»¥è€Œä¸”åº”è¯¥åœ¨æ¯ä¸ªåº”ç”¨çš„åŸºç¡€ä¸Šè¿›è¡Œå®šåˆ¶å’ŒéªŒè¯çš„å·¥å…·ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å³å°†é€šè¿‡æ³•å¾‹å’Œæ³•è§„ï¼Œè¯•å›¾å¯¹è¿™äº›æ¨¡å‹å†…éƒ¨çš„æƒ…å†µè¿›è¡Œå¾®è§‚ç®¡ç†ã€‚

What will do the most damage here is the notion that the models must be scrubbed from all â€œbias,â€ where â€œbiasâ€ is defined as, â€œthe model accurately reflects the race, class, and gender distributions in the training data, and the training data actually reflects reality.â€ Instead of insisting that humans act on model outputs â€” whatever they are â€” in ways that conform with existing laws, regulators will likely insist the models hallucinate an â€œequitableâ€ set of distributions that do not actually exist.  

åœ¨è¿™é‡Œé€ æˆæœ€å¤§æŸå®³çš„æ˜¯è¿™æ ·ä¸€ç§è§‚å¿µï¼Œå³æ¨¡å‹å¿…é¡»æ¸…é™¤æ‰€æœ‰ "åè§"ï¼Œå…¶ä¸­ "åè§ "è¢«å®šä¹‰ä¸ºï¼š"æ¨¡å‹å‡†ç¡®åæ˜ äº†è®­ç»ƒæ•°æ®ä¸­çš„ç§æ—ã€é˜¶çº§å’Œæ€§åˆ«åˆ†å¸ƒï¼Œè€Œè®­ç»ƒæ•°æ®å®é™…åæ˜ äº†ç°å®ã€‚"ç›‘ç®¡æœºæ„ä¸æ˜¯åšæŒè¦æ±‚äººç±»ä»¥ç¬¦åˆç°æœ‰æ³•å¾‹çš„æ–¹å¼å¯¹æ¨¡å‹è¾“å‡º--æ— è®ºå®ƒä»¬æ˜¯ä»€ä¹ˆ--é‡‡å–è¡ŒåŠ¨ï¼Œè€Œæ˜¯å¯èƒ½åšæŒè®¤ä¸ºæ¨¡å‹å¹»åŒ–å‡ºä¸€å¥—å®é™…ä¸å­˜åœ¨çš„ "å…¬å¹³ "åˆ†å¸ƒã€‚

![Lovecraft's Basilisk: On The Dangers Of Teaching AI To Lie](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F6724d3e1-d608-4b36-b5b6-8fca514d5df6_3072x2048.png)

The Story So Far: Of all the many things AI does, perhaps the most important is the way it lifts abstract, ancient philosophical problems out of academic obscurity and thrusts them into concrete technical and policy situations with immediate practical implications.  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šåœ¨äººå·¥æ™ºèƒ½æ‰€åšçš„æ‰€æœ‰äº‹æƒ…ä¸­ï¼Œæœ€é‡è¦çš„ä¹Ÿè®¸æ˜¯å®ƒå°†æŠ½è±¡çš„ã€å¤è€çš„å“²å­¦é—®é¢˜ä»å­¦æœ¯çš„æ™¦æ¶©ä¸­æäº†å‡ºæ¥ï¼Œå¹¶å°†å®ƒä»¬æ¨å…¥å…·æœ‰ç›´æ¥å®é™…å½±å“çš„å…·ä½“æŠ€æœ¯å’Œæ”¿ç­–æƒ…å†µä¸­ã€‚

So I am deeply concerned that regulators will [ask the models to lie to us](https://www.jonstokes.com/p/lovecrafts-basilisk-on-the-dangers), instead of insisting that theyâ€™re truthful and that we humans use them in good and appropriate ways. This has already started in the EU and is also well underway here.  

Hereâ€™s part of a [recent Biden admin statement](https://www.ftc.gov/system/files/ftc_gov/pdf/EEOC-CRT-FTC-CFPB-AI-Joint-Statement%28final%29.pdf) \[PDF\] on â€œdiscrimination and bias in automated systemsâ€:  

å› æ­¤ï¼Œæˆ‘æ·±ä¸ºå…³åˆ‡çš„æ˜¯ï¼Œç›‘ç®¡æœºæ„å°†è¦æ±‚æ¨¡å‹å¯¹æˆ‘ä»¬æ’’è°ï¼Œè€Œä¸æ˜¯åšæŒå®ƒä»¬æ˜¯çœŸå®çš„ï¼Œæˆ‘ä»¬äººç±»ä»¥è‰¯å¥½å’Œé€‚å½“çš„æ–¹å¼ä½¿ç”¨å®ƒä»¬ã€‚è¿™åœ¨æ¬§ç›Ÿå·²ç»å¼€å§‹äº†ï¼Œåœ¨è¿™é‡Œä¹Ÿæ­£åœ¨è¿›è¡Œä¸­ã€‚ä»¥ä¸‹æ˜¯æ‹œç™»æ”¿åºœæœ€è¿‘å…³äº "è‡ªåŠ¨åŒ–ç³»ç»Ÿä¸­çš„æ­§è§†å’Œåè§ "çš„å£°æ˜\[PDF\]çš„ä¸€éƒ¨åˆ†ï¼š

[![Image](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F10905308-6603-420e-8067-dccb5601dea9_1146x532.jpeg "Image")](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10905308-6603-420e-8067-dccb5601dea9_1146x532.jpeg)

In my reading, the excerpt above is quite possibly self-contradictory and nonsensical: are the datasets supposed to be representative and balanced (a normal person would take this to mean, â€œreflecting actual reality as it really is in the real worldâ€) or are they supposed to be free of â€œhistorical biases.â€  

åœ¨æˆ‘çœ‹æ¥ï¼Œä¸Šé¢çš„æ‘˜å½•å¾ˆå¯èƒ½æ˜¯è‡ªç›¸çŸ›ç›¾å’Œæ— ç¨½ä¹‹è°ˆï¼šæ•°æ®é›†åº”è¯¥æ˜¯æœ‰ä»£è¡¨æ€§å’Œå¹³è¡¡çš„ï¼ˆä¸€ä¸ªæ­£å¸¸äººä¼šè®¤ä¸ºè¿™æ„å‘³ç€ï¼Œ"åæ˜ ç°å®ä¸–ç•Œçš„å®é™…æƒ…å†µ"ï¼‰ï¼Œè¿˜æ˜¯åº”è¯¥ä¸å­˜åœ¨ "å†å²åè§"ã€‚

It all comes down to how you interpret the term â€œhistorical biases,â€ so letâ€™s make this concrete with a real-world example:  

è¿™ä¸€åˆ‡éƒ½å½’ç»“äºä½ å¦‚ä½•è§£é‡Š "å†å²åè§ "ä¸€è¯ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç°å®ä¸–ç•Œçš„ä¾‹å­æ¥å…·ä½“è¯´æ˜ï¼š

-   **Scenario:** In a certain city, the residents of one zip code default on their loans are a far higher rate than those of another, wealthier (and whiter) zip code.  
    
    æƒ…æ™¯ï¼šåœ¨æŸä¸ªåŸå¸‚ï¼Œä¸€ä¸ªé‚®æ”¿ç¼–ç çš„å±…æ°‘æ‹–æ¬ è´·æ¬¾çš„æ¯”ä¾‹è¿œè¿œé«˜äºå¦ä¸€ä¸ªæ›´å¯Œè£•ï¼ˆå’Œæ›´ç™½ï¼‰çš„é‚®æ”¿ç¼–ç çš„å±…æ°‘ã€‚
    
-   **Question:** Is this difference in default rates a historically grounded, factual correlation that weâ€™re going actively suppress within a credit scoring model, or are we going to ask the human users of the model to actively mitigate the effects of this problematic historical legacy in some way?  
    
    é—®é¢˜ï¼šè¿™ç§è¿çº¦ç‡çš„å·®å¼‚æ˜¯ä¸€ç§æœ‰å†å²ä¾æ®çš„ã€äº‹å®æ€§çš„å…³è”ï¼Œæˆ‘ä»¬è¦åœ¨ä¿¡ç”¨è¯„åˆ†æ¨¡å‹ä¸­ç§¯ææŠ‘åˆ¶è¿™ç§å…³è”ï¼Œè¿˜æ˜¯æˆ‘ä»¬è¦è¦æ±‚æ¨¡å‹çš„äººç±»ç”¨æˆ·ä»¥æŸç§æ–¹å¼ç§¯æå‡è½»è¿™ç§æœ‰é—®é¢˜çš„å†å²é—äº§çš„å½±å“ï¼Ÿ
    

Many readers have probably noticed that this is an argument weâ€™re having in multiple places in our society right now. _Do we measure the gap between two groups and then socially engineer a way to close it, or do we just stop measuring the gap at all because measuring it somehow perpetuates it?_  

è®¸å¤šè¯»è€…å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œè¿™æ˜¯æˆ‘ä»¬ç°åœ¨ç¤¾ä¼šä¸­å¤šä¸ªåœ°æ–¹çš„ä¸€ä¸ªäº‰è®ºã€‚æˆ‘ä»¬æ˜¯è¦è¡¡é‡ä¸¤ä¸ªç¾¤ä½“ä¹‹é—´çš„å·®è·ï¼Œç„¶åé€šè¿‡ç¤¾ä¼šå·¥ç¨‹çš„æ–¹å¼æ¥ç¼©å°å·®è·ï¼Œè¿˜æ˜¯æ ¹æœ¬å°±ä¸å»è¡¡é‡å·®è·ï¼Œå› ä¸ºè¡¡é‡å·®è·åœ¨æŸç§ç¨‹åº¦ä¸Šä¼šä½¿å·®è·é•¿æœŸå­˜åœ¨ï¼Ÿ

In the hands of the language police, the moralizing, agentic approach to AI, where a cluster of statistical probabilities is treated as a walking, talking stand-in for either The Man or the Chief Equity Officer, acts as a powerful rationale for treating model development and selection the way DEI bureaucracies treat hiring decisions instead of the way engineers treat software deployments.  

åœ¨è¯­è¨€è­¦å¯Ÿçš„æ‰‹ä¸­ï¼Œå¯¹äººå·¥æ™ºèƒ½çš„é“å¾·åŒ–ã€ä»£ç†åŒ–çš„æ–¹æ³•ï¼Œå³ä¸€ç°‡ç»Ÿè®¡æ¦‚ç‡è¢«è§†ä¸ºä¸€ä¸ªè¡Œèµ°çš„ã€ä¼šè¯´è¯çš„äººæˆ–é¦–å¸­æƒç›Šå®˜çš„æ›¿èº«ï¼Œä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„ç†ç”±ï¼Œä»¥DEIå®˜åƒšæœºæ„å¯¹å¾…æ‹›è˜å†³å®šçš„æ–¹å¼è€Œä¸æ˜¯å·¥ç¨‹å¸ˆå¯¹å¾…è½¯ä»¶éƒ¨ç½²çš„æ–¹å¼æ¥å¯¹å¾…æ¨¡å‹å¼€å‘å’Œé€‰æ‹©ã€‚  

This is terrible for a whole bunch of reasons and we should not do it.  

è¿™æ˜¯å¾ˆç³Ÿç³•çš„ï¼Œå› ä¸ºæœ‰ä¸€å¤§å †åŸå› ï¼Œæˆ‘ä»¬ä¸åº”è¯¥è¿™æ ·åšã€‚  

Instead, we should insist that AI is treated according to the norms of engineering and not according to the norms of HR.  

ç›¸åï¼Œæˆ‘ä»¬åº”è¯¥åšæŒæŒ‰ç…§å·¥ç¨‹çš„è§„èŒƒæ¥å¯¹å¾…äººå·¥æ™ºèƒ½ï¼Œè€Œä¸æ˜¯æŒ‰ç…§äººåŠ›èµ„æºçš„è§„èŒƒã€‚
