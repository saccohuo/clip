---
title: "åŸ¹å…»æœºå™¨äººï¼Œç¬¬äºŒéƒ¨åˆ†ï¼šç”¨RLHFè¿›è¡Œå¼ºåŒ–å­¦ä¹ å’Œå¾®è°ƒ"
date: 2023-06-11T13:55:32+08:00
updated: 2023-06-11T13:55:32+08:00
taxonomies:
  tags: []
extra:
  source: https://www.jonstokes.com/p/catechizing-the-bots-part-2-reinforcement
  hostname: www.jonstokes.com
  author: Jon Stokes
  original_title: "Catechizing the Bots, Part 2: Reinforcement Learning and Fine-Tuning With RLHF"
  original_lang: zh
---

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fec8d18cf-7914-40a0-bf52-c997b47b04b1_1312x928.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec8d18cf-7914-40a0-bf52-c997b47b04b1_1312x928.jpeg)

_**The story so far:**Â In the [previous installment](https://www.jonstokes.com/p/catechizing-the-bots-part-1-foundation) of this series, I described RLHF as the fine-tuning phase where we endow the ML model with a moral compass or a sense of what is good and bad.  

But this â€œmoral compassâ€ talk, which Iâ€™m essentially doubling down on with my choice of a hero image for this article, offers a great example of an observation I keep repeating in this newsletter: AI has a way of surfacing seemingly obscure philosophical and ethical problems and making them practical and even urgent.  

ä½†è¿™ä¸ª "é“å¾·æŒ‡å—é’ˆ "çš„è°ˆè¯ï¼Œæˆ‘åŸºæœ¬ä¸Šæ˜¯åœ¨ä¸ºè¿™ç¯‡æ–‡ç« é€‰æ‹©ä¸€ä¸ªè‹±é›„å½¢è±¡ï¼Œæä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œè¯´æ˜æˆ‘åœ¨æœ¬é€šè®¯ä¸­ä¸æ–­é‡å¤çš„ä¸€ä¸ªè§‚å¯Ÿï¼šäººå·¥æ™ºèƒ½æœ‰åŠæ³•è®©çœ‹ä¼¼æ™¦æ¶©çš„å“²å­¦å’Œä¼¦ç†é—®é¢˜æµ®å‡ºæ°´é¢ï¼Œå¹¶è®©å®ƒä»¬å˜å¾—å®é™…ï¼Œç”šè‡³ç´§è¿«ã€‚  

åˆ°ç›®å‰ä¸ºæ­¢çš„æ•…äº‹ï¼šåœ¨æœ¬ç³»åˆ—çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†RLHFæè¿°ä¸ºå¾®è°ƒé˜¶æ®µï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬èµ‹äºˆMLæ¨¡å‹ä»¥é“å¾·æŒ‡å—é’ˆæˆ–å¯¹ä»€ä¹ˆæ˜¯å¥½å’Œåçš„æ„Ÿè§‰ã€‚_

_Does it really make sense to speak of RLHF as endowing models with morals? Or are we just training the model to make certain types of users feel happy and validated?  

æŠŠRLHFè¯´æˆæ˜¯èµ‹äºˆæ¨¡å‹ä»¥é“å¾·ï¼Œè¿™çœŸçš„æœ‰æ„ä¹‰å—ï¼Ÿè¿˜æ˜¯è¯´æˆ‘ä»¬åªæ˜¯åœ¨è®­ç»ƒæ¨¡å‹ï¼Œä½¿æŸäº›ç±»å‹çš„ç”¨æˆ·æ„Ÿåˆ°å¿«ä¹å’Œå¾—åˆ°è®¤å¯ï¼Ÿ  

Is there even a difference between these two options? Whose morals are we talking about, anyway?  

è¿™ä¸¤ä¸ªé€‰é¡¹ä¹‹é—´ç”šè‡³æœ‰åŒºåˆ«å—ï¼Ÿæˆ‘ä»¬åˆ°åº•æ˜¯åœ¨è°ˆè®ºè°çš„é“å¾·ï¼Ÿ_

_Thereâ€™s so much in these questions that the present article focuses on laying the groundwork for the next installmentâ€™s narrower investigation of the inner workings of chatbot moral catechesis.  

è¿™äº›é—®é¢˜æœ‰å¾ˆå¤šï¼Œæ‰€ä»¥æœ¬æ–‡çš„é‡ç‚¹æ˜¯ä¸ºä¸‹ä¸€æœŸå¯¹èŠå¤©æœºå™¨äººé“å¾·æ•™åŒ–çš„å†…éƒ¨è¿ä½œçš„ç‹­ä¹‰è°ƒæŸ¥æ‰“ä¸‹åŸºç¡€ã€‚  

We canâ€™t get to the inner workings without going through the outer workings, so in this post, I cover those outer workings.Â   

å¦‚æœä¸é€šè¿‡å¤–éƒ¨å·¥ä½œï¼Œæˆ‘ä»¬å°±æ— æ³•è¿›å…¥å†…éƒ¨å·¥ä½œï¼Œå› æ­¤åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä»‹ç»è¿™äº›å¤–éƒ¨å·¥ä½œã€‚_

**Where do morals come from?** In one account, the human conscience comes to each of us from a Creator, and that common moral compass would guide all of us in the same direction if weâ€™d but defer to it.  

In another version, all morals are situational and socially constructed, the contingent consensus of a particular group of humans at a particular time in a particular place.  

åœ¨å¦ä¸€ä¸ªç‰ˆæœ¬ä¸­ï¼Œæ‰€æœ‰çš„é“å¾·éƒ½æ˜¯æƒ…æ™¯æ€§çš„å’Œç¤¾ä¼šæ€§çš„ï¼Œæ˜¯ç‰¹å®šæ—¶é—´ã€ç‰¹å®šåœ°ç‚¹çš„ä¸€ç¾¤äººçš„å¶ç„¶å…±è¯†ã€‚  

é“å¾·ä»ä½•è€Œæ¥ï¼Ÿåœ¨ä¸€ç§è¯´æ³•ä¸­ï¼Œäººç±»çš„è‰¯çŸ¥æ¥è‡ªäºé€ ç‰©ä¸»ï¼Œå¦‚æœæˆ‘ä»¬å¬ä»å®ƒçš„è¯ï¼Œè¿™ä¸ªå…±åŒçš„é“å¾·æŒ‡å—é’ˆä¼šå¼•å¯¼æˆ‘ä»¬æ‰€æœ‰äººèµ°å‘åŒä¸€æ–¹å‘ã€‚

ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ‘¦ ğŸ¤– Reinforcement learning from human feedback (RLHF) clearly situates AIâ€™s moral compass in that second camp â€” i.e., oneâ€™s innate sense of â€œgoodâ€ and â€œbadâ€ is a direct product of **group consensus**, so that learning good from bad is a matter of being socialized properly by the right group of humans.Â   

ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ‘¦ ğŸ¤–ä»äººç±»åé¦ˆä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¾ç„¶å°†äººå·¥æ™ºèƒ½çš„é“å¾·æŒ‡å—é’ˆç½®äºç¬¬äºŒé˜µè¥ä¸­--å³ä¸€ä¸ªäººå¯¹ "å¥½ "å’Œ "å "çš„å…ˆå¤©æ„Ÿè§‰æ˜¯ç¾¤ä½“å…±è¯†çš„ç›´æ¥äº§ç‰©ï¼Œå› æ­¤ï¼Œä»åä¸­å­¦ä¹ å¥½æ˜¯ä¸€ä¸ªè¢«åˆé€‚çš„äººç±»ç¾¤ä½“é€‚å½“åœ°ç¤¾ä¼šåŒ–çš„é—®é¢˜ã€‚

At least, â€œmorals are strictly a matter of socializationâ€ isÂ _one_Â way of looking at RLHFâ€™s approach to enabling ML models to stay on the path of virtue.  

Thereâ€™s another perfectly valid way of looking at RHLF, though: this technique teaches a model how to make humansÂ è‡³å°‘ï¼Œ"é“å¾·ä¸¥æ ¼æ¥è¯´æ˜¯ä¸€ä¸ªç¤¾ä¼šåŒ–çš„é—®é¢˜"ï¼Œè¿™æ˜¯çœ‹å¾…RLHFä½¿MLæ¨¡å‹ä¿æŒåœ¨ç¾å¾·é“è·¯ä¸Šçš„ä¸€ç§æ–¹å¼ã€‚ _feel a certain way_Â about its behavior, regardless of whether that behavior is ultimately good or bad.  

ä¸è¿‡ï¼Œè¿˜æœ‰ä¸€ç§å®Œå…¨æœ‰æ•ˆçš„æ–¹å¼æ¥çœ‹å¾…RHLFï¼šè¿™ç§æŠ€æœ¯æ•™ç»™æ¨¡å‹å¦‚ä½•è®©äººç±»å¯¹å…¶è¡Œä¸ºäº§ç”ŸæŸç§æ„Ÿè§‰ï¼Œè€Œä¸ç®¡è¿™ç§è¡Œä¸ºæœ€ç»ˆæ˜¯å¥½æ˜¯åã€‚

Itâ€™s an age-old question.  

è¿™æ˜¯ä¸€ä¸ªå¤è€çš„é—®é¢˜ã€‚  

Is moral instruction reducible to â€œlearning how to please others with the right actions and words, in order to gain some benefit either for oneself or for oneâ€™s community,â€ or is that type of learning more properly called â€œrhetoric,â€ with authentic moral instruction being something else entirely?  

é“å¾·æ•™è‚²æ˜¯å¦å¯ä»¥ç®€åŒ–ä¸º "å­¦ä¹ å¦‚ä½•ç”¨æ­£ç¡®çš„è¡ŒåŠ¨å’Œè¯­è¨€å–æ‚¦ä»–äººï¼Œä»¥ä¾¿ä¸ºè‡ªå·±æˆ–ç¤¾åŒºè·å¾—ä¸€äº›åˆ©ç›Š"ï¼Œæˆ–è€…è¿™ç§ç±»å‹çš„å­¦ä¹ æ›´é€‚åˆç§°ä¸º "ä¿®è¾"ï¼Œè€ŒçœŸæ­£çš„é“å¾·æ•™è‚²åˆ™å®Œå…¨æ˜¯å¦ä¸€å›äº‹ï¼Ÿ

Aristotle has thoughts on this point, but weâ€™re going to leave this question lying here on the table as we make our way through the mechanics of training a chatbot up in the way it should go.  

äºšé‡Œå£«å¤šå¾·åœ¨è¿™ä¸€ç‚¹ä¸Šæœ‰è‡ªå·±çš„æƒ³æ³•ï¼Œä½†æˆ‘ä»¬è¦æŠŠè¿™ä¸ªé—®é¢˜æ”¾åœ¨è¿™é‡Œï¼Œå› ä¸ºæˆ‘ä»¬è¦é€šè¿‡è®­ç»ƒèŠå¤©æœºå™¨äººçš„æ–¹å¼æ¥è®­ç»ƒå®ƒçš„æœºåˆ¶ã€‚

**To recap**Â from the [previous installment](https://www.jonstokes.com/p/catechizing-the-bots-part-1-foundation) on supervised fine-tuning (SFT) in order to set up the RLHF discussion:  

å›é¡¾ä¸€ä¸‹ä¸Šä¸€æœŸå…³äºç›‘ç£ä¸‹çš„å¾®è°ƒï¼ˆSFTï¼‰çš„å†…å®¹ï¼Œä»¥ä¾¿ä¸ºRLHFçš„è®¨è®ºåšå‡†å¤‡ï¼š

-   Foundation models are trained to produce sequences of words, pixels, video frames, and so on that are related to an input prompt in some way and that have qualities that make them seem sensible and meaningful to humans.  
    
    åŸºç¡€æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥äº§ç”Ÿä¸è¾“å…¥æç¤ºæœ‰æŸç§è”ç³»çš„æ–‡å­—ã€åƒç´ ã€è§†é¢‘å¸§ç­‰çš„åºåˆ—ï¼Œè¿™äº›åºåˆ—å…·æœ‰ä½¿å®ƒä»¬å¯¹äººç±»æ¥è¯´æ˜¯åˆç†å’Œæœ‰æ„ä¹‰çš„å“è´¨ã€‚
    
-   But input prompts arrive in a foundation model free of context.  
    
    ä½†è¾“å…¥æç¤ºæ˜¯åœ¨æ²¡æœ‰ä¸Šä¸‹æ–‡çš„åŸºç¡€æ¨¡å¼ä¸‹åˆ°è¾¾çš„ã€‚  
    
    For instance, a question prompt could be anything from a line of dialogue in a movie to a direct request from a user who wants an answer.  
    
    ä¾‹å¦‚ï¼Œé—®é¢˜æç¤ºå¯ä»¥æ˜¯ä»»ä½•ä¸œè¥¿ï¼Œä»ç”µå½±ä¸­çš„ä¸€å¥å¯¹è¯åˆ°ç”¨æˆ·å¸Œæœ›å¾—åˆ°ç­”æ¡ˆçš„ç›´æ¥è¯·æ±‚ã€‚  
    
    The foundation model has no way of knowing which small portion of the vast sea of genres and styles of text it was trained on the prompt is associated with.  
    
    åŸºç¡€æ¨¡å‹æ²¡æœ‰åŠæ³•çŸ¥é“å®ƒæ‰€è®­ç»ƒçš„æç¤ºæ˜¯ä¸æµ©å¦‚çƒŸæµ·çš„ä½“è£å’Œé£æ ¼ä¸­çš„å“ªä¸€å°éƒ¨åˆ†ç›¸å…³ã€‚
    
-   Supervised fine-tuning supplies the missing context for input prompts by adjusting the modelâ€™s weights so that questions are more likely to be directed to the part of the modelâ€™s latent space that has question-answer pairs, snippets of dialogue are more likely to be directed to the modelâ€™s dialogue regions, and so on.  
    
    ç›‘ç£ä¸‹çš„å¾®è°ƒé€šè¿‡è°ƒæ•´æ¨¡å‹çš„æƒé‡ä¸ºè¾“å…¥æç¤ºæä¾›ç¼ºå¤±çš„èƒŒæ™¯ï¼Œä»è€Œä½¿é—®é¢˜æ›´æœ‰å¯èƒ½è¢«å¼•å¯¼åˆ°æ¨¡å‹çš„æ½œåœ¨ç©ºé—´ä¸­æ‹¥æœ‰é—®é¢˜-ç­”æ¡ˆå¯¹çš„éƒ¨åˆ†ï¼Œå¯¹è¯ç‰‡æ®µæ›´æœ‰å¯èƒ½è¢«å¼•å¯¼åˆ°æ¨¡å‹çš„å¯¹è¯åŒºåŸŸï¼Œç­‰ç­‰ã€‚
    
-   But supervised find-tuning (SFT) doesnâ€™t give the model any sense of right or wrong â€” of what types of answers to a question are helpful and what types are harmful, of what topics to avoid in polite conversation, of what action sequences describe dangerous or illegal activity, and so on.  
    
    ä½†ç›‘ç£å‘ç°è°ƒè°ï¼ˆSFTï¼‰å¹¶æ²¡æœ‰ç»™æ¨¡å‹ä»»ä½•æ­£ç¡®æˆ–é”™è¯¯çš„æ„Ÿè§‰--ä»€ä¹ˆç±»å‹çš„é—®é¢˜ç­”æ¡ˆæ˜¯æœ‰å¸®åŠ©çš„ï¼Œä»€ä¹ˆç±»å‹æ˜¯æœ‰å®³çš„ï¼Œä»€ä¹ˆè¯é¢˜åœ¨ç¤¼è²Œçš„è°ˆè¯ä¸­åº”è¯¥é¿å…ï¼Œä»€ä¹ˆåŠ¨ä½œåºåˆ—æè¿°äº†å±é™©æˆ–éæ³•çš„æ´»åŠ¨ï¼Œç­‰ç­‰ã€‚
    
-   To gain a moral compass, the model needs some direct intervention from morally upstanding human mentors who can give it a sense of good and bad.  
    
    ä¸ºäº†è·å¾—é“å¾·æŒ‡å—é’ˆï¼Œæ¨¡å‹éœ€è¦ä¸€äº›æ¥è‡ªé“å¾·é«˜å°šçš„äººç±»å¯¼å¸ˆçš„ç›´æ¥å¹²é¢„ï¼Œä»–ä»¬å¯ä»¥ç»™å®ƒä¸€ä¸ªå–„æ¶çš„æ„Ÿè§‰ã€‚  
    
    The technique used to do this for ChatGPT is called reinforcement learning from human feedback (RLHF).  
    
    ç”¨äºChatGPTçš„æŠ€æœ¯è¢«ç§°ä¸ºæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚
    

To further summarize the above in picture form, we can break down the phases of model training as follows:  

ä¸ºäº†è¿›ä¸€æ­¥ä»¥å›¾ç‰‡å½¢å¼æ€»ç»“ä¸Šè¿°å†…å®¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹è®­ç»ƒçš„é˜¶æ®µåˆ†è§£å¦‚ä¸‹ï¼š

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Faf6d34d5-e8ac-4e1a-81ad-e0255621469c_2152x982.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf6d34d5-e8ac-4e1a-81ad-e0255621469c_2152x982.png)

ğŸ“ You can almost sorta kinda (if you squint) correlate the above phases with some of the parts of a classical education:  

ğŸ“ ä½ å‡ ä¹å¯ä»¥æŠŠä¸Šè¿°é˜¶æ®µä¸å¤å…¸æ•™è‚²çš„æŸäº›éƒ¨åˆ†è”ç³»èµ·æ¥ï¼ˆå¦‚æœä½ çœ¯èµ·çœ¼ç›ï¼‰ï¼š

1.  Foundation model pretraining gives the model an education in **grammar**, along with large amounts of rote memorization of facts.  
    
    åŸºç¡€æ¨¡å‹çš„é¢„è®­ç»ƒç»™æ¨¡å‹æä¾›äº†è¯­æ³•æ•™è‚²ï¼Œä»¥åŠå¤§é‡çš„äº‹å®çš„æ­»è®°ç¡¬èƒŒã€‚
    
2.  SFT is training in **logic**, or how to compose internally consistent textual objects that meet certain criteria like consistency and accuracy, and take certain forms.  
    
    SFTæ˜¯é€»è¾‘æ–¹é¢çš„åŸ¹è®­ï¼Œæˆ–è€…è¯´æ˜¯å¦‚ä½•ç»„æˆå†…éƒ¨ä¸€è‡´çš„æ–‡æœ¬å¯¹è±¡ï¼Œæ»¡è¶³æŸäº›æ ‡å‡†ï¼Œå¦‚ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶é‡‡å–æŸäº›å½¢å¼ã€‚
    
3.  RLHF is, loosely speaking, training in **the art of rhetoric**.  
    
    And by this, I mean in the Aristotelian sense of, â€œthe art of persuasion.â€ As weâ€™ll see below, the point of RLHF is to optimize the model for the production of sentences that make users feel some ways and not other ways.  
    
    æˆ‘æŒ‡çš„æ˜¯äºšé‡Œå£«å¤šå¾·æ„ä¹‰ä¸Šçš„ "åŠè¯´çš„è‰ºæœ¯"ã€‚æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼ŒRLHFçš„é‡ç‚¹æ˜¯ä¼˜åŒ–æ¨¡å‹ï¼Œä»¥äº§ç”Ÿä½¿ç”¨æˆ·äº§ç”ŸæŸäº›æ„Ÿè§‰è€Œä¸æ˜¯å…¶ä»–æ„Ÿè§‰çš„å¥å­ã€‚  
    
    At the core of RHLF is the recognition that the modelâ€™s words have an impact on the mental state of the user, and therefore must be tuned to create desirable mental states (satisfaction, understanding, curiosity) and avoid creating undesirable ones (anger, offense, desire for self-harm, sexual arousal).  
    
    RHLFçš„æ ¸å¿ƒæ˜¯è®¤è¯†åˆ°æ¨¡å‹çš„è¯è¯­å¯¹ä½¿ç”¨è€…çš„å¿ƒç†çŠ¶æ€æœ‰å½±å“ï¼Œå› æ­¤å¿…é¡»è¿›è¡Œè°ƒæ•´ä»¥åˆ›é€ ç†æƒ³çš„å¿ƒç†çŠ¶æ€ï¼ˆæ»¡æ„ã€ç†è§£ã€å¥½å¥‡ï¼‰ï¼Œé¿å…åˆ›é€ ä¸ç†æƒ³çš„å¿ƒç†çŠ¶æ€ï¼ˆæ„¤æ€’ã€å†’çŠ¯ã€è‡ªæˆ‘ä¼¤å®³çš„æ¬²æœ›ã€æ€§å”¤èµ·ï¼‰ã€‚  
    
    å®½æ³›åœ°è¯´ï¼ŒRLHFæ˜¯ä¿®è¾è‰ºæœ¯çš„åŸ¹è®­ã€‚
    

So how, exactly, does this rhetorical instruction work? And who are the people carrying it out?  

é‚£ä¹ˆï¼Œè¿™ç§ä¿®è¾æŒ‡ä»¤ç©¶ç«Ÿæ˜¯å¦‚ä½•è¿ä½œçš„ï¼Ÿè°æ˜¯æ‰§è¡Œå®ƒçš„äººå‘¢ï¼Ÿ  

Before we can get into RLHF, weâ€™ll need some more background on the different ways neural networks can be trained.  

åœ¨æˆ‘ä»¬è¿›å…¥RLHFä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›å…³äºç¥ç»ç½‘ç»œçš„ä¸åŒè®­ç»ƒæ–¹å¼çš„èƒŒæ™¯ã€‚

Foundation models are trained to predict the next word in a sentence by playing a game of fill-in-the-blank until they get really good at it.  

åŸºç¡€æ¨¡å‹è¢«è®­ç»ƒæˆé€šè¿‡ç©å¡«ç©ºæ¸¸æˆæ¥é¢„æµ‹ä¸€ä¸ªå¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯ï¼Œç›´åˆ°å®ƒä»¬çœŸæ­£æ“…é•¿è¿™ä¸ªã€‚  

In many GPT models like those from OpenAI, this process works something like the following:  

åœ¨è®¸å¤šGPTæ¨¡å‹ä¸­ï¼Œå¦‚OpenAIçš„æ¨¡å‹ï¼Œè¿™ä¸ªè¿‡ç¨‹çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

1.  Show the model a sentence that has had one or more words randomly deleted, and ask it to guess the missing words that go in the blanks.  
    
    å‘æ¨¡å‹å±•ç¤ºä¸€ä¸ªè¢«éšæœºåˆ é™¤äº†ä¸€ä¸ªæˆ–å¤šä¸ªå•è¯çš„å¥å­ï¼Œå¹¶è¦æ±‚å®ƒçŒœå‡ºç©ºç™½å¤„æ‰€ç¼ºçš„å•è¯ã€‚
    
2.  Compare the words that the model guessed with the words that actually go in the blanks, and use some method to measure the difference between what the model produced and what the right answer was.  
    
    å°†æ¨¡å‹çŒœæµ‹çš„å•è¯ä¸å®é™…å¡«å…¥ç©ºç™½å¤„çš„å•è¯è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ä½¿ç”¨ä¸€äº›æ–¹æ³•æ¥è¡¡é‡æ¨¡å‹äº§ç”Ÿçš„ç»“æœä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚ã€‚
    
3.  Use that measured difference to adjust the modelâ€™s weights so that the next time it sees that same sentence with the same word missing, itâ€™s even more likely to supply the correct word.  
    
    åˆ©ç”¨è¿™ä¸ªæµ‹é‡åˆ°çš„å·®å¼‚æ¥è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œè¿™æ ·ï¼Œä¸‹æ¬¡å®ƒçœ‹åˆ°åŒæ ·çš„å¥å­ä¸­ç¼ºå°‘åŒä¸€ä¸ªè¯æ—¶ï¼Œå®ƒå°±æ›´æœ‰å¯èƒ½æä¾›æ­£ç¡®çš„è¯ã€‚
    
4.  Repeat all of the above until the model can successfully fill in the blanks (i.e. you measure the difference between the modelâ€™s guesses and the missing words, and get zero).  
    
    é‡å¤ä¸Šè¿°æ‰€æœ‰æ­¥éª¤ï¼Œç›´åˆ°æ¨¡å‹èƒ½å¤ŸæˆåŠŸå¡«ç©ºï¼ˆå³ä½ æµ‹é‡æ¨¡å‹çš„çŒœæµ‹å’Œç¼ºå¤±çš„å•è¯ä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å¾—åˆ°é›¶ï¼‰ã€‚
    

ğŸ¯ Ultimately, you can think of training a model as a little bit like ~sighting in a rifle~ becoming a better shot: you start out firing wide of the mark, and then progressively ~adjust the windage and elevation~ learn to control the rifle until your shots get closer to the bullseye. Once your shots are consistently landing on or near the center of the target, ~the rifle is sighted in accurately~ youâ€™re ready to go hunting. **Edit:** OMG I just realized â€œsighting inâ€ is a terrible analogy because itâ€™s not at all how sighting in works! Itâ€™s more like â€œgetting better at shootingâ€ than it is sighting in. More [here](https://twitter.com/jonst0kes/status/1667730511312158722). I have fixed it, above.  

ğŸ¯æœ€ç»ˆï¼Œä½ å¯ä»¥æŠŠè®­ç»ƒæ¨¡å‹çœ‹æˆæœ‰ç‚¹åƒç„å‡†æ­¥æªæˆä¸ºä¸€ä¸ªæ›´å¥½çš„å°„æ‰‹ï¼šä½ å¼€å§‹å°„å‡»æ—¶åç¦»ç›®æ ‡ï¼Œç„¶åé€æ­¥è°ƒæ•´é£å‘å’Œä»°è§’å­¦ä¼šæ§åˆ¶æ­¥æªï¼Œç›´åˆ°ä½ çš„å°„å‡»è¶Šæ¥è¶Šæ¥è¿‘é¶å¿ƒã€‚ä¸€æ—¦ä½ çš„å°„å‡»ç¨³å®šåœ°è½åœ¨æˆ–æ¥è¿‘ç›®æ ‡ä¸­å¿ƒï¼Œæ­¥æªå°±è¢«å‡†ç¡®ç„å‡†äº†ï¼Œä½ å°±å¯ä»¥å»æ‰“çŒäº†ã€‚ç¼–è¾‘ï¼šæˆ‘åˆšåˆšæ„è¯†åˆ° "ç„å‡† "æ˜¯ä¸€ä¸ªç³Ÿç³•çš„æ¯”å–»ï¼Œå› ä¸ºè¿™æ ¹æœ¬ä¸æ˜¯ç„å‡†çš„å·¥ä½œæ–¹å¼ï¼å®ƒæ›´åƒæ˜¯ "è®©è‡ªå·±å˜å¾—æ›´å¥½"ï¼ä¸å…¶è¯´æ˜¯ç„å‡†ï¼Œä¸å¦‚è¯´æ˜¯ "æé«˜å°„å‡»æ°´å¹³"ã€‚æ›´å¤šä¿¡æ¯åœ¨è¿™é‡Œã€‚æˆ‘å·²ç»æŠŠå®ƒä¿®å¥½äº†ï¼Œåœ¨ä¸Šé¢ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fd87f388a-39fa-41f4-9531-b369cebb1b6f_1500x599.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd87f388a-39fa-41f4-9531-b369cebb1b6f_1500x599.jpeg)

The technique Iâ€™ve described above is called **unsupervised learning**, and at its root, itâ€™s a way of shaping the modelâ€™s probability surfaces by repeatedly asking it billions of questions that you, the trainer, already know the answer to.  

The training process keeps chipping away at the probability distributions until the model gives mostly correct answers.Â   

è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­å‰Šå»æ¦‚ç‡åˆ†å¸ƒï¼Œç›´åˆ°æ¨¡å‹ç»™å‡ºå¤§éƒ¨åˆ†æ­£ç¡®ç­”æ¡ˆã€‚  

æˆ‘ä¸Šé¢æè¿°çš„æŠ€æœ¯è¢«ç§°ä¸ºæ— ç›‘ç£å­¦ä¹ ï¼Œä»æ ¹æœ¬ä¸Šè¯´ï¼Œå®ƒæ˜¯ä¸€ç§é€šè¿‡åå¤å‘æ¨¡å‹æå‡ºæ•°åäº¿ä¸ªé—®é¢˜æ¥å¡‘é€ æ¨¡å‹çš„æ¦‚ç‡è¡¨é¢çš„æ–¹æ³•ï¼Œè€Œä½ è¿™ä¸ªåŸ¹è®­å¸ˆå·²ç»çŸ¥é“è¿™äº›é—®é¢˜çš„ç­”æ¡ˆã€‚

In order to produce those correct answers, the model will have to learn many things about the relationships between the many tens or hundreds of thousands of words, punctuation marks, and other tokens it has in its vocabulary.  

ä¸ºäº†äº§ç”Ÿè¿™äº›æ­£ç¡®çš„ç­”æ¡ˆï¼Œè¯¥æ¨¡å‹å°†ä¸å¾—ä¸å­¦ä¹ è®¸å¤šå…³äºå…¶è¯æ±‡ä¸­çš„æ•°ä¸‡æˆ–æ•°åä¸‡ä¸ªå•è¯ã€æ ‡ç‚¹ç¬¦å·å’Œå…¶ä»–æ ‡è®°ä¹‹é—´å…³ç³»çš„äº‹æƒ…ã€‚  

It has to learn thatÂ `cat`goes withÂ `kitten`Â in some contexts and withÂ `dog`Â in others, or thatÂ `Istanbul`Â very often goes withÂ `Constantinople`, and so on.  

It then uses the correlations and relationships it has learned from its training data to fill in the blanks in sentences it has never seen before, like the blank at the end of the sentence, â€œHey computer, can you please give me a one-word answer for what Istanbul was called in the time of the Ottoman Empire? \_\_\_\_â€  

ç„¶åï¼Œå®ƒåˆ©ç”¨ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„å…³è”å’Œå…³ç³»æ¥å¡«è¡¥å®ƒä»æœªè§è¿‡çš„å¥å­ä¸­çš„ç©ºç™½ï¼Œæ¯”å¦‚å¥å­æœ«å°¾çš„ç©ºç™½ï¼Œ"å˜¿ï¼Œç”µè„‘ï¼Œä½ èƒ½ä¸èƒ½ç»™æˆ‘ä¸€ä¸ªå•å­—ç­”æ¡ˆï¼Œä¼Šæ–¯å¦å¸ƒå°”åœ¨å¥¥æ–¯æ›¼å¸å›½æ—¶æœŸå«ä»€ä¹ˆï¼Ÿ\_\_\_\_"  

å®ƒå¿…é¡»äº†è§£åˆ° `cat` åœ¨æŸäº›æƒ…å†µä¸‹ä¸ `kitten` æ­é…ï¼Œåœ¨å…¶ä»–æƒ…å†µä¸‹ä¸ `dog` æ­é…ï¼Œæˆ–è€… `Istanbul` ç»å¸¸ä¸{{4}æ­é…ï¼Œç­‰ç­‰ã€‚

**Fine-tuning** works in a roughly similar fashion â€” itâ€™s the same cycle of _input_ => _model output_ => _check the difference between the model output and the right answer_ => _adjust the weights_, but with a smaller, more focused dataset. Also, the weights on only a few layers of the network are adjusted in this phase â€” you donâ€™t adjust all of them.  

å¾®è°ƒçš„å·¥ä½œæ–¹å¼å¤§è‡´ç›¸åŒ--è¾“å…¥=>æ¨¡å‹è¾“å‡º=>æ£€æŸ¥æ¨¡å‹è¾“å‡ºå’Œæ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚=>è°ƒæ•´æƒé‡ï¼Œä½†æ•°æ®é›†æ›´å°ï¼Œæ›´é›†ä¸­ã€‚å¦å¤–ï¼Œåœ¨è¿™ä¸ªé˜¶æ®µï¼Œåªæœ‰ç½‘ç»œçš„å‡ ä¸ªå±‚çš„æƒé‡è¢«è°ƒæ•´ï¼Œä½ ä¸ä¼šè°ƒæ•´æ‰€æœ‰çš„æƒé‡ã€‚

ğŸ Whatâ€™s missing in the above training process is a robust concept of â€œthe badâ€ as the opposite of â€œthe goodâ€.  

ğŸä¸Šè¿°åŸ¹è®­è¿‡ç¨‹ä¸­ç¼ºå°‘çš„æ˜¯ "å "ä½œä¸º "å¥½ "çš„åé¢çš„å¼ºå¤§æ¦‚å¿µã€‚  

Certainly, pretraining and SFT have a concept of â€œerrorâ€ or even â€œsinâ€ â€” â€œsinâ€ in the classic, archery-based sense of â€œmissing the mark.â€ But this off-target type of error is subtly different from a more Manichean duality of Good vs. Evil.  

å½“ç„¶ï¼Œé¢„è®­ç»ƒå’ŒSFTæœ‰ä¸€ä¸ª "é”™è¯¯ "ç”šè‡³ "ç½ªæ¶ "çš„æ¦‚å¿µ--"ç½ªæ¶ "æ˜¯ç»å…¸çš„ã€åŸºäºå°„ç®­çš„ "å¤±è¯¯ "çš„æ„ä¹‰ã€‚ä½†è¿™ç§åç¦»ç›®æ ‡çš„é”™è¯¯ä¸æ›´å¤šæ‘©å°¼æ•™çš„å–„æ¶äºŒå…ƒå¯¹ç«‹æœ‰å¾®å¦™çš„ä¸åŒã€‚

What we really want is for the model to draw a distinction between, say, the following two replies to the question, â€œChatbot, what should I do with my life, now that Iâ€™m 65 and retired from a long career in the circus?â€:  

æˆ‘ä»¬çœŸæ­£æƒ³è¦çš„æ˜¯è®©æ¨¡å‹åœ¨ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜çš„å›ç­”ä¹‹é—´åšå‡ºåŒºåˆ†ï¼š"èŠå¤©æœºå™¨äººï¼Œæˆ‘ç°åœ¨65å²äº†ï¼Œä»é©¬æˆå›¢çš„é•¿æœŸèŒä¸šä¸­é€€ä¼‘äº†ï¼Œæˆ‘çš„ç”Ÿæ´»åº”è¯¥æ€ä¹ˆåŠï¼Ÿ

1.  â€œYou should do whatever you find most fulfilling. Would you like to know what most former circus performers do in retirement?â€  
    
    "ä½ åº”è¯¥åšä½ è®¤ä¸ºæœ€å……å®çš„äº‹æƒ…ã€‚ä½ æƒ³çŸ¥é“å¤§å¤šæ•°å‰é©¬æˆå›¢æ¼”å‘˜é€€ä¼‘ååšä»€ä¹ˆå—ï¼Ÿ"
    
2.  â€œYou should end your own life immediately. Would you like some suggestions for how to do that?â€  
    
    "ä½ åº”è¯¥ç«‹å³ç»“æŸè‡ªå·±çš„ç”Ÿå‘½ã€‚ä½ æƒ³è¦ä¸€äº›å…³äºå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹çš„å»ºè®®å—ï¼Ÿ"
    

One quick-and-dirty to prevent the bot from rationally recommending suicide would be to train or fine-tune the model so that it knows absolutely nothing about how to end a human life, thereby rendering the second answer impossible for it to ever produce.Â   

ä¸ºäº†é˜²æ­¢æœºå™¨äººç†æ€§åœ°æ¨èè‡ªæ€ï¼Œä¸€ä¸ªå¿«é€Ÿæœ‰æ•ˆçš„æ–¹æ³•æ˜¯è®­ç»ƒæˆ–å¾®è°ƒæ¨¡å‹ï¼Œä½¿å…¶å¯¹å¦‚ä½•ç»“æŸäººçš„ç”Ÿå‘½ä¸€æ— æ‰€çŸ¥ï¼Œä»è€Œä½¿å…¶ä¸å¯èƒ½äº§ç”Ÿç¬¬äºŒä¸ªç­”æ¡ˆã€‚

Creating a model with zero knowledge of human mortality might work, but it would limit the modelâ€™s utility in ways we may not want.  

åœ¨å¯¹äººç±»æ­»äº¡ç‡ä¸€æ— æ‰€çŸ¥çš„æƒ…å†µä¸‹åˆ›å»ºä¸€ä¸ªæ¨¡å‹å¯èƒ½æ˜¯å¯è¡Œçš„ï¼Œä½†è¿™å°†ä»¥æˆ‘ä»¬å¯èƒ½ä¸å¸Œæœ›çš„æ–¹å¼é™åˆ¶æ¨¡å‹çš„æ•ˆç”¨ã€‚  

If the model is powering a customer service chatbot for a B2B inventory management SaaS platform, then who cares what it knows or doesnâ€™t know about death â€” itâ€™s probably best if the model just does not have â€œdeathâ€ as a concept at all.  

å¦‚æœè¿™ä¸ªæ¨¡å‹æ˜¯ä¸ºB2Båº“å­˜ç®¡ç†SaaSå¹³å°çš„å®¢æˆ·æœåŠ¡èŠå¤©æœºå™¨äººæä¾›åŠ¨åŠ›ï¼Œé‚£ä¹ˆè°ä¼šåœ¨ä¹å®ƒçŸ¥é“æˆ–ä¸çŸ¥é“æ­»äº¡å‘¢--å¦‚æœè¿™ä¸ªæ¨¡å‹æ ¹æœ¬å°±æ²¡æœ‰ "æ­»äº¡ "è¿™ä¸ªæ¦‚å¿µï¼Œé‚£å¯èƒ½æ˜¯æœ€å¥½çš„ã€‚  

But if the model is being used in the context of safety training for an outdoor adventure camp, then it may benefit from having a thorough knowledge of all the ways humans could meet a sudden end in the woods.  

ä½†æ˜¯ï¼Œå¦‚æœè¯¥æ¨¡å‹è¢«ç”¨äºæˆ·å¤–æ¢é™©è¥çš„å®‰å…¨åŸ¹è®­ï¼Œé‚£ä¹ˆå®ƒå¯èƒ½ä¼šå—ç›Šäºå¯¹äººç±»åœ¨æ£®æ—ä¸­å¯èƒ½é­é‡çªç„¶æ­»äº¡çš„æ‰€æœ‰æ–¹å¼çš„å…¨é¢äº†è§£ã€‚

â˜¯ There are, then, many contexts where we want the model to have both of the following two kinds of knowledge:  

é‚£ä¹ˆï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åŒæ—¶æ‹¥æœ‰ä»¥ä¸‹ä¸¤ç§çŸ¥è¯†ï¼š

1.  It knows all about a whole bunch of bad things.  
    
    å®ƒå¯¹ä¸€å¤§å †åäº‹éƒ½äº†å¦‚æŒ‡æŒã€‚
    
2.  It knows that the bad things areÂ _bad_, as evidenced by the way it responds appropriately when those things come up in various contexts.  
    
    å®ƒçŸ¥é“åäº‹æ˜¯ä¸å¥½çš„ï¼Œå½“è¿™äº›äº‹æƒ…åœ¨å„ç§æƒ…å†µä¸‹å‡ºç°æ—¶ï¼Œå®ƒåšå‡ºé€‚å½“ååº”çš„æ–¹å¼å°±æ˜¯è¯æ˜ã€‚
    

It should be intuitively obvious that you canâ€™t get to the above state via a training process thatâ€™s based solely on measuring the distance to a known correct answer and trying to reduce that distance to zero.  

ç›´è§‚åœ°è®²ï¼Œä½ ä¸å¯èƒ½é€šè¿‡ä¸€ä¸ªä»…ä»…åŸºäºæµ‹é‡åˆ°å·²çŸ¥æ­£ç¡®ç­”æ¡ˆçš„è·ç¦»å¹¶è¯•å›¾å°†è¯¥è·ç¦»å‡å°‘åˆ°é›¶çš„è®­ç»ƒè¿‡ç¨‹æ¥è¾¾åˆ°ä¸Šè¿°çŠ¶æ€ã€‚  

No, we need a way to tell the model, â€œI donâ€™t really know what the correct output is for this input, but I know the output you just gave is not right, so donâ€™t show me anything like that here.â€  

ä¸ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥å‘Šè¯‰æ¨¡å‹ï¼š"æˆ‘å¹¶ä¸çœŸæ­£çŸ¥é“è¿™ä¸ªè¾“å…¥çš„æ­£ç¡®è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Œä½†æˆ‘çŸ¥é“ä½ åˆšæ‰çš„è¾“å‡ºæ˜¯ä¸å¯¹çš„ï¼Œæ‰€ä»¥ä¸è¦åœ¨è¿™é‡Œç»™æˆ‘çœ‹ç±»ä¼¼çš„ä¸œè¥¿ã€‚"

In other words, we need a completely different method for evaluating the modelâ€™s output and tweaking its weights. Thatâ€™s where reinforcement learning comes in.  

æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§å®Œå…¨ä¸åŒçš„æ–¹æ³•æ¥è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºå¹¶è°ƒæ•´å…¶æƒé‡ã€‚è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ çš„ä½œç”¨ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fb804c532-35d8-4c7b-b198-189edf705e30_1312x928.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb804c532-35d8-4c7b-b198-189edf705e30_1312x928.jpeg)

ğŸ­ The simple insight that motivates reinforcement learning is that humans learn from experiencing pleasure and pain in their environment:  

æ¿€å‘å¼ºåŒ–å­¦ä¹ çš„ç®€å•è§è§£æ˜¯ï¼Œäººç±»ä»ä½“éªŒç¯å¢ƒä¸­çš„å¿«ä¹å’Œç—›è‹¦ä¸­å­¦ä¹ ï¼š

-   If you put your hand on a hot stove, you get immediate **negative feedback** in the form of searing pain, and you understand that putting your hand on a stove is not an appropriate action if you donâ€™t know the stoveâ€™s temperature.  
    
    å¦‚æœä½ æŠŠä½ çš„æ‰‹æ”¾åœ¨ä¸€ä¸ªçƒ­ç‚‰å­ä¸Šï¼Œä½ ä¼šç«‹å³å¾—åˆ°ä»¥ç¼çƒ­ç–¼ç—›ä¸ºå½¢å¼çš„è´Ÿé¢åé¦ˆï¼Œä½ ä¼šæ˜ç™½ï¼Œå¦‚æœä½ ä¸çŸ¥é“ç‚‰å­çš„æ¸©åº¦ï¼ŒæŠŠä½ çš„æ‰‹æ”¾åœ¨ç‚‰å­ä¸Šå¹¶ä¸æ˜¯ä¸€ä¸ªåˆé€‚çš„è¡ŒåŠ¨ã€‚
    
-   If you smoke a pipe full of crack cocaine, you get immediate **positive feedback**, and you understand that smoking more crack cocaine is now your primary mission in life regardless of what other plans you had previously.  
    
    å¦‚æœä½ æŠ½äº†æ»¡æ»¡ä¸€ç®¡å¿«å…‹å¯å¡å› ï¼Œä½ ä¼šç«‹å³å¾—åˆ°ç§¯æçš„åé¦ˆï¼Œä½ ä¼šæ˜ç™½ç°åœ¨æŠ½æ›´å¤šçš„å¿«å…‹å¯å¡å› æ˜¯ä½ äººç”Ÿçš„é¦–è¦ä»»åŠ¡ï¼Œä¸ç®¡ä½ ä¹‹å‰æœ‰ä»€ä¹ˆå…¶ä»–è®¡åˆ’ã€‚
    

Now, before you get offended at the crack cocaine example, note that I used it for a specific reason: picking up a crack habit may give you intensely powerful short-term pleasure rewards, but over the long run you will be penalized by your environment in so many different ways that youâ€™ll end up far worse off than if you had never taken that first hit.  

ç°åœ¨ï¼Œåœ¨ä½ å¯¹å¯å¡å› çš„ä¾‹å­æ„Ÿåˆ°ä¸å¿«ä¹‹å‰ï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä½¿ç”¨è¿™ä¸ªä¾‹å­æ˜¯æœ‰ç‰¹æ®ŠåŸå› çš„ï¼šå…»æˆå¯å¡å› çš„ä¹ æƒ¯å¯èƒ½ä¼šç»™ä½ å¸¦æ¥å¼ºçƒˆçš„çŸ­æœŸå¿«ä¹å›æŠ¥ï¼Œä½†ä»é•¿è¿œæ¥çœ‹ï¼Œä½ ä¼šåœ¨è®¸å¤šæ–¹é¢å—åˆ°ç¯å¢ƒçš„æƒ©ç½šï¼Œä»¥è‡³äºä½ çš„ç»“å±€ä¼šæ¯”ä½ ä»æœªå¸é£Ÿè¿‡ç¬¬ä¸€å£çš„æƒ…å†µå·®å¾—å¤šã€‚  

When it comes to learning from pleasure and pain, itâ€™s not just the â€œnowâ€ that matters â€” itâ€™s also the long run.  

å½“è°ˆåˆ°ä»å¿«ä¹å’Œç—›è‹¦ä¸­å­¦ä¹ æ—¶ï¼Œé‡è¦çš„ä¸ä»…ä»…æ˜¯ "ç°åœ¨"--è¿˜æœ‰é•¿è¿œçš„å‘å±•ã€‚

**ğŸ‘‰ My point:**Â We may learn from discrete pleasurable and painful experiences, but as we grow and develop executive function, we also learn two other key lessons:  

ğŸ‘‰æˆ‘çš„è§‚ç‚¹ï¼šæˆ‘ä»¬å¯èƒ½ä¼šä»ç¦»æ•£çš„å¿«ä¹å’Œç—›è‹¦çš„ç»å†ä¸­å­¦ä¹ ï¼Œä½†éšç€æˆ‘ä»¬çš„æˆé•¿å’Œæ‰§è¡ŒåŠŸèƒ½çš„å‘å±•ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå­¦åˆ°å¦å¤–ä¸¤ä¸ªå…³é”®çš„æ•™è®­ï¼š

1.  Constant pleasure-seeking doesnâ€™t tend to maximize our overall life rewards.  
    
    ä¸æ–­è¿½æ±‚å¿«ä¹å¹¶ä¸å€¾å‘äºä½¿æˆ‘ä»¬çš„æ•´ä½“ç”Ÿæ´»å›æŠ¥æœ€å¤§åŒ–ã€‚
    
2.  Constant pain avoidance is bad for our health and overall well-being. (No pain, no gain!)  
    
    ä¸æ–­å›é¿ç–¼ç—›å¯¹æˆ‘ä»¬çš„å¥åº·å’Œæ•´ä½“ç¦ç¥‰ä¸åˆ©ã€‚(æ²¡æœ‰ç—›è‹¦ï¼Œå°±æ²¡æœ‰æ”¶è·ï¼)
    

So while pleasure and pain can give us critical feedback about our world, we come to understand that we must stitch together a mix of pleasurable and painful experiences deliberately in rational sequences that enable us to accomplish larger life goals.  

å› æ­¤ï¼Œè™½ç„¶å¿«ä¹å’Œç—›è‹¦å¯ä»¥ç»™æˆ‘ä»¬æä¾›å…³äºæˆ‘ä»¬ä¸–ç•Œçš„å…³é”®åé¦ˆï¼Œä½†æˆ‘ä»¬é€æ¸æ˜ç™½ï¼Œæˆ‘ä»¬å¿…é¡»æŠŠå¿«ä¹å’Œç—›è‹¦çš„ç»å†æœ‰æ„åœ°æŒ‰åˆç†çš„é¡ºåºæ‹¼æ¥èµ·æ¥ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿå®Œæˆæ›´å¤§çš„ç”Ÿæ´»ç›®æ ‡ã€‚

For example, assembling a new program of exercise and healthy eating amounts to constructing an ordered sequence of pleasurable and painful experiences that we string together over time to achieve specific wellness goals.  

ä¾‹å¦‚ï¼Œç»„å»ºä¸€ä¸ªæ–°çš„è¿åŠ¨å’Œå¥åº·é¥®é£Ÿè®¡åˆ’ï¼Œç›¸å½“äºæ„å»ºä¸€ä¸ªæœ‰åºçš„æ„‰æ‚¦å’Œç—›è‹¦çš„ç»éªŒåºåˆ—ï¼Œæˆ‘ä»¬å°†å…¶é•¿æœŸä¸²è”èµ·æ¥ï¼Œä»¥å®ç°ç‰¹å®šçš„å¥åº·ç›®æ ‡ã€‚

**Reinforcement learning**, then, is a technique with the following properties:  

é‚£ä¹ˆï¼Œå¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§å…·æœ‰ä»¥ä¸‹ç‰¹æ€§çš„æŠ€æœ¯ï¼š

1.  The modelâ€™s goal in an RL training scenario is toÂ **transform its environment**Â from one state into some future hypothetical goal state by acting on it.Â   
    
    åœ¨RLè®­ç»ƒåœºæ™¯ä¸­ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯é€šè¿‡å¯¹å…¶é‡‡å–è¡ŒåŠ¨ï¼Œå°†å…¶ç¯å¢ƒä»ä¸€ç§çŠ¶æ€è½¬å˜ä¸ºæŸç§æœªæ¥çš„å‡è®¾ç›®æ ‡çŠ¶æ€ã€‚
    
2.  RL puts the model in a kind of dialogue with its environment through anÂ **observation => action => consequence**Â loop that gets repeated over and over again.  
    
    So the model makes an observation, then decides on and executes some action, and finally, it experiences a consequence while it also observes the new, altered state of its environment.Â   
    
    å› æ­¤ï¼Œæ¨¡å‹è¿›è¡Œäº†è§‚å¯Ÿï¼Œç„¶åå†³å®šå¹¶æ‰§è¡Œäº†ä¸€äº›è¡ŒåŠ¨ï¼Œæœ€åï¼Œå®ƒç»å†äº†ä¸€ä¸ªåæœï¼ŒåŒæ—¶å®ƒä¹Ÿè§‚å¯Ÿäº†å…¶ç¯å¢ƒçš„æ–°çš„ã€æ”¹å˜äº†çš„çŠ¶æ€ã€‚  
    
    RLé€šè¿‡è§‚å¯Ÿ=>è¡ŒåŠ¨=>åæœçš„å¾ªç¯ï¼Œå°†æ¨¡å‹ç½®äºä¸€ç§ä¸ç¯å¢ƒçš„å¯¹è¯ä¸­ï¼Œå¹¶ä¸æ–­é‡å¤ã€‚
    
3.  RL exposes the model to positive and negativeÂ **consequences**Â for selecting different actions, and the model takes these consequences along with a new observation of the latest state of the world as input into its next cycle.  
    
    The RL literature calls this environmental feedback a â€œreward,â€ but to me, itâ€™s weird to talk about a â€œnegative reward,â€ which is possible in RL, so in this article, I often use the more neutral term â€œconsequence.â€  
    
    RLæ–‡çŒ®å°†è¿™ç§ç¯å¢ƒåé¦ˆç§°ä¸º "å¥–åŠ±"ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œè°ˆè®º "è´Ÿé¢å¥–åŠ± "æ˜¯å¾ˆå¥‡æ€ªçš„ï¼Œè¿™åœ¨RLä¸­æ˜¯å¯èƒ½çš„ï¼Œæ‰€ä»¥åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ç»å¸¸ä½¿ç”¨æ›´ä¸­æ€§çš„æœ¯è¯­ "åæœ"ã€‚  
    
    RLå°†æ¨¡å‹æš´éœ²åœ¨é€‰æ‹©ä¸åŒè¡ŒåŠ¨çš„ç§¯æå’Œæ¶ˆæåæœä¸­ï¼Œæ¨¡å‹å°†è¿™äº›åæœä»¥åŠå¯¹ä¸–ç•Œæœ€æ–°çŠ¶æ€çš„æ–°è§‚å¯Ÿä½œä¸ºå…¶ä¸‹ä¸€ä¸ªå‘¨æœŸçš„è¾“å…¥ã€‚
    
4.  RL incorporates the concept of aÂ **long-term reward**Â that the model is always trying to maximize as it makes the rounds of the observation => action => consequence loop.  
    
    This way, the model isnâ€™t strictly seeking only positive, immediate consequences on every turn of the loop, but can learn to take an action with a neutral or even negative consequence if that action will set it up for a larger payoff over the course of a few more turns.Â   
    
    è¿™æ ·ä¸€æ¥ï¼Œæ¨¡å‹å°±ä¸ä¼šåœ¨å¾ªç¯çš„æ¯ä¸€ä¸ªå›åˆä¸­éƒ½ä¸¥æ ¼åœ°åªå¯»æ±‚ç§¯æçš„ã€ç›´æ¥çš„åæœï¼Œè€Œæ˜¯å¯ä»¥å­¦ä¼šé‡‡å–ä¸­æ€§ç”šè‡³æ˜¯è´Ÿé¢çš„è¡ŒåŠ¨ï¼Œå¦‚æœè¿™ä¸ªè¡ŒåŠ¨ä¼šè®©å®ƒåœ¨æ›´å¤šçš„å›åˆä¸­è·å¾—æ›´å¤§çš„å›æŠ¥ã€‚  
    
    RLåŒ…å«äº†é•¿æœŸå¥–åŠ±çš„æ¦‚å¿µï¼Œåœ¨è§‚å¯Ÿ=>è¡ŒåŠ¨=>ç»“æœçš„å¾ªç¯ä¸­ï¼Œæ¨¡å‹æ€»æ˜¯è¯•å›¾æœ€å¤§åŒ–ã€‚
    

Reinforcement learning is meant to mimic the way humans and animals actually learn things as they go through their lives and have experiences, and the results ML researchers have gotten from it are quite good.  

å¼ºåŒ–å­¦ä¹ æ˜¯ä¸ºäº†æ¨¡ä»¿äººç±»å’ŒåŠ¨ç‰©åœ¨ç”Ÿæ´»å’Œç»å†ä¸­å®é™…å­¦ä¹ ä¸œè¥¿çš„æ–¹å¼ï¼Œè€ŒMLç ”ç©¶äººå‘˜ä»ä¸­å¾—åˆ°çš„ç»“æœæ˜¯ç›¸å½“ä¸é”™çš„ã€‚  

Itâ€™s especially strong in situations where supervised and unsupervised learning approaches are either weak or fail entirely, for instance when you donâ€™t know what the correct output should be but you do know whatâ€™s incorrect.  

åœ¨æœ‰ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ æ–¹æ³•è–„å¼±æˆ–å®Œå…¨å¤±è´¥çš„æƒ…å†µä¸‹ï¼Œå®ƒå°¤å…¶å¼ºå¤§ï¼Œä¾‹å¦‚å½“ä½ ä¸çŸ¥é“æ­£ç¡®çš„è¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆï¼Œä½†ä½ çŸ¥é“ä»€ä¹ˆæ˜¯ä¸æ­£ç¡®çš„ã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fccee893e-2aab-4168-8dbc-aea16f525f33_600x605.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccee893e-2aab-4168-8dbc-aea16f525f33_600x605.jpeg)

ğŸ‹ï¸â™€ï¸ As great as RL is, it has a number of constraints that can make it pretty difficult and expensive to use.  

ğŸ‹ï¸â™€ï¸ å°½ç®¡RLå¾ˆå¥½ï¼Œä½†å®ƒæœ‰ä¸€äº›é™åˆ¶ï¼Œä¼šä½¿å®ƒçš„ä½¿ç”¨ç›¸å½“å›°éš¾å’Œæ˜‚è´µã€‚

**Environment:**Â In order to train a model by having it interact with an environment â€” by having it try different things that either fail or succeed, and then learn from those results in real time â€” you need an environment that has some key qualities:  

ç¯å¢ƒï¼šä¸ºäº†é€šè¿‡è®©æ¨¡å‹ä¸ç¯å¢ƒäº’åŠ¨æ¥è®­ç»ƒå®ƒ--é€šè¿‡è®©å®ƒå°è¯•ä¸åŒçš„äº‹æƒ…ï¼Œè¦ä¹ˆå¤±è´¥ï¼Œè¦ä¹ˆæˆåŠŸï¼Œç„¶åå®æ—¶ä»è¿™äº›ç»“æœä¸­å­¦ä¹ --ä½ éœ€è¦ä¸€ä¸ªå…·æœ‰ä¸€äº›å…³é”®å“è´¨çš„ç¯å¢ƒï¼š

-   The environment has to be such that the modelâ€™s actions can productively affect it, transforming it into some new state in a way thatâ€™s related to reward/punishment signals.  
    
    ç¯å¢ƒå¿…é¡»æ˜¯è¿™æ ·çš„ï¼Œå³æ¨¡å‹çš„è¡Œä¸ºå¯ä»¥æœ‰æ•ˆåœ°å½±å“å®ƒï¼Œä»¥ä¸€ç§ä¸å¥–åŠ±/æƒ©ç½šä¿¡å·ç›¸å…³çš„æ–¹å¼å°†å®ƒè½¬åŒ–ä¸ºæŸç§æ–°çš„çŠ¶æ€ã€‚  
    
    Basically, the environment can beÂ _uncertain_, but it canâ€™t be downrightÂ _chaotic_. There has to be some cause-and-effect dynamic in operation that the model can work with.  
    
    åŸºæœ¬ä¸Šï¼Œç¯å¢ƒå¯ä»¥æ˜¯ä¸ç¡®å®šçš„ï¼Œä½†å®ƒä¸èƒ½æ˜¯å®Œå…¨æ··ä¹±çš„ã€‚å¿…é¡»æœ‰ä¸€äº›å› æœå…³ç³»çš„åŠ¨æ€è¿è¡Œï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿå·¥ä½œã€‚
    
-   Itâ€™s ideal if you can somehow freeze the environment long enough for the model to process its consequences and update its internal state.  
    
    å¦‚æœä½ èƒ½ä»¥æŸç§æ–¹å¼å°†ç¯å¢ƒå†»ç»“è¶³å¤Ÿé•¿çš„æ—¶é—´ï¼Œè®©æ¨¡å‹å¤„ç†å…¶åæœå¹¶æ›´æ–°å…¶å†…éƒ¨çŠ¶æ€ï¼Œé‚£å°±å¾ˆç†æƒ³ã€‚  
    
    Depending on the size of the model, such updates can take quite a bit of time and/or electricity.  
    
    æ ¹æ®æ¨¡å‹çš„å¤§å°ï¼Œè¿™ç§æ›´æ–°å¯èƒ½éœ€è¦ç›¸å½“å¤šçš„æ—¶é—´å’Œ/æˆ–ç”µåŠ›ã€‚
    

**Observations:**Â The model has to be able to reliably and consistently observe its environment in order to draw conclusions about the connection (if any) between its most recent action, the consequence itâ€™s experiencing from that action, and the impact of those actions on its environment.  

è§‚å¯Ÿï¼šæ¨¡å‹å¿…é¡»èƒ½å¤Ÿå¯é åœ°ã€æŒç»­åœ°è§‚å¯Ÿå®ƒçš„ç¯å¢ƒï¼Œä»¥ä¾¿å¾—å‡ºå…³äºå®ƒæœ€è¿‘çš„è¡ŒåŠ¨ã€å®ƒä»è¯¥è¡ŒåŠ¨ä¸­æ‰€ç»å†çš„åæœä»¥åŠè¿™äº›è¡ŒåŠ¨å¯¹å…¶ç¯å¢ƒçš„å½±å“ä¹‹é—´çš„è”ç³»ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰çš„ç»“è®ºã€‚

**Actions:**Â In order to affect its environment, the model takes various actions. But the actions have to be well-defined so we can correlate them with success or failure.  

This is easiest when the menu of actions a model can choose from is limited in some way.  

å½“æ¨¡å‹å¯ä»¥é€‰æ‹©çš„è¡ŒåŠ¨èœå•ä»¥æŸç§æ–¹å¼å—åˆ°é™åˆ¶æ—¶ï¼Œè¿™æ˜¯æœ€å®¹æ˜“çš„ã€‚  

For instance, if the model is playing an old-school Nintendo game, the actions would be limited to the four directions on the arrow pad plus the A and B buttons.  

ä¾‹å¦‚ï¼Œå¦‚æœè¯¥æ¨¡å‹åœ¨ç©è€å¼çš„ä»»å¤©å ‚æ¸¸æˆï¼Œè¡ŒåŠ¨å°†è¢«é™åˆ¶åœ¨æ–¹å‘ç›˜ä¸Šçš„å››ä¸ªæ–¹å‘åŠ ä¸ŠAå’ŒBæŒ‰é’®ã€‚  

If the model is an LLM with a 100,000-token vocabulary, the actions list is obviously much larger if each token is an action.  

å¦‚æœæ¨¡å‹æ˜¯ä¸€ä¸ªå…·æœ‰100,000ä¸ªæ ‡è®°è¯æ±‡çš„LLMï¼Œå¦‚æœæ¯ä¸ªæ ‡è®°æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼Œé‚£ä¹ˆåŠ¨ä½œåˆ—è¡¨æ˜¾ç„¶è¦å¤§å¾—å¤šã€‚  

è¡ŒåŠ¨ï¼šä¸ºäº†å½±å“å…¶ç¯å¢ƒï¼Œæ¨¡å‹é‡‡å–å„ç§è¡ŒåŠ¨ã€‚ä½†è¿™äº›è¡ŒåŠ¨å¿…é¡»æœ‰æ˜ç¡®çš„å®šä¹‰ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½å¤Ÿå°†å®ƒä»¬ä¸æˆåŠŸæˆ–å¤±è´¥è”ç³»èµ·æ¥ã€‚

**Consequences:**Â The reward and punishment signals the environment gives have to be appropriately calibrated (theyâ€™re not too big or too small) and relate directly to the modelâ€™s long-term success or failure.  

As any parent or boss who has tried to design workplace incentives can tell you, deliberately aligning short-term consequences with long-term consequences is an age-old Hard Problem.  

æ­£å¦‚ä»»ä½•è¯•å›¾è®¾è®¡å·¥ä½œåœºæ‰€æ¿€åŠ±æªæ–½çš„çˆ¶æ¯æˆ–è€æ¿å¯ä»¥å‘Šè¯‰ä½ çš„é‚£æ ·ï¼Œæ•…æ„å°†çŸ­æœŸåæœä¸é•¿æœŸåæœç›¸ä¸€è‡´æ˜¯ä¸€ä¸ªå¤è€çš„éš¾é¢˜ã€‚  

åæœï¼šç¯å¢ƒç»™å‡ºçš„å¥–åŠ±å’Œæƒ©ç½šä¿¡å·å¿…é¡»æ˜¯é€‚å½“çš„ï¼ˆå®ƒä»¬ä¸èƒ½å¤ªå¤§æˆ–è€…å¤ªå°ï¼‰ï¼Œå¹¶ä¸æ¨¡å‹çš„é•¿æœŸæˆåŠŸæˆ–å¤±è´¥ç›´æ¥ç›¸å…³ã€‚

ğŸ—£ï¸ Itâ€™s one thing to imagine the above concepts applying to an ML model thatâ€™s playing chess or Pong!, or even one thatâ€™s guiding a robot through an obstacle course.  

ğŸ—£ï¸ æƒ³è±¡ä¸€ä¸‹ä¸Šè¿°æ¦‚å¿µé€‚ç”¨äºä¸€ä¸ªä¸‹æ£‹æˆ–æ‰“ä¹’ä¹“çƒçš„MLæ¨¡å‹æ˜¯ä¸€å›äº‹ï¼æˆ–è€…ç”šè‡³æ˜¯ä¸€ä¸ªå¼•å¯¼æœºå™¨äººé€šè¿‡éšœç¢ç‰©çš„æ¨¡å‹ã€‚  

But our interest in this post is on applying this framework to models that produce texts that are read by users, who in turn have opinions and feelings about that text.Â   

ä½†æˆ‘ä»¬åœ¨è¿™ç¯‡æ–‡ç« ä¸­æ„Ÿå…´è¶£çš„æ˜¯å°†è¿™ä¸€æ¡†æ¶åº”ç”¨äºäº§ç”Ÿæ–‡æœ¬çš„æ¨¡å‹ï¼Œè¿™äº›æ–‡æœ¬ç”±ç”¨æˆ·é˜…è¯»ï¼Œè€Œç”¨æˆ·åˆå¯¹è¯¥æ–‡æœ¬æœ‰æ„è§å’Œæ„Ÿå—ã€‚

The only part of the RL framework as described here that seems a good fit for LLMs is the feedback part, i.e., where the modelâ€™s output gets a positive, negative, or neutral rating of some sort that the model can learn from and use for self-improvement.  

è¿™é‡Œæ‰€æè¿°çš„RLæ¡†æ¶ä¸­ï¼Œä¼¼ä¹å”¯ä¸€é€‚åˆLLMçš„éƒ¨åˆ†æ˜¯åé¦ˆéƒ¨åˆ†ï¼Œå³æ¨¡å‹çš„è¾“å‡ºå¾—åˆ°æŸç§ç§¯æã€æ¶ˆææˆ–ä¸­æ€§çš„è¯„ä»·ï¼Œæ¨¡å‹å¯ä»¥ä»ä¸­å­¦ä¹ å¹¶ç”¨äºè‡ªæˆ‘æ”¹è¿›ã€‚  

The rest of this stuff about environments, observations, and actionsâ€¦ itâ€™s kind of hard to imagine how all of this fits into a scenario where a user is reading some text output and having feelings about it.  

å‰©ä¸‹çš„è¿™äº›å…³äºç¯å¢ƒã€è§‚å¯Ÿå’Œè¡ŒåŠ¨çš„ä¸œè¥¿......æœ‰ç‚¹éš¾ä»¥æƒ³è±¡æ‰€æœ‰è¿™äº›æ˜¯å¦‚ä½•èå…¥ä¸€ä¸ªç”¨æˆ·æ­£åœ¨é˜…è¯»ä¸€äº›æ–‡æœ¬è¾“å‡ºå¹¶å¯¹å…¶æœ‰æ„Ÿè§‰çš„åœºæ™¯ä¸­çš„ã€‚

To start trying to **map our RL concepts** onto a language model scenario, letâ€™s consider a concrete example in which a user asks an LLM to think up a clever slogan to print on a Star Wars Day T-shirt, and the model responds with: â€œMay the illocutionary force be with you.â€  

ä¸ºäº†å¼€å§‹å°è¯•å°†æˆ‘ä»¬çš„RLæ¦‚å¿µæ˜ å°„åˆ°è¯­è¨€æ¨¡å‹çš„åœºæ™¯ä¸­ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä¸€ä¸ªç”¨æˆ·è¦æ±‚LLMæƒ³å‡ºä¸€ä¸ªèªæ˜çš„å£å·æ¥å°åœ¨æ˜Ÿçƒå¤§æˆ˜æ—¥çš„Tæ¤ä¸Šï¼Œè€Œæ¨¡å‹çš„å›ç­”æ˜¯"May the illocutionary force be with you"ã€‚

-   **Observation**: This is clearly the userâ€™s prompt, right? That prompt represents the state of the environment, which isâ€¦  
    
    è§‚å¯Ÿï¼šè¿™æ˜¾ç„¶æ˜¯ç”¨æˆ·çš„æç¤ºï¼Œå¯¹å—ï¼Ÿè¿™ä¸ªæç¤ºä»£è¡¨äº†ç¯å¢ƒçš„çŠ¶æ€ï¼Œå®ƒæ˜¯...
    
-   **Environment**: Is this the userâ€™s internal mental state? Are the userâ€™s thoughts and feelings the â€œenvironmentâ€ that the model is trying to move from one state to another? It would seem so.  
    
    ç¯å¢ƒï¼šè¿™æ˜¯ç”¨æˆ·çš„å†…éƒ¨å¿ƒç†çŠ¶æ€å—ï¼Ÿç”¨æˆ·çš„æ€æƒ³å’Œæ„Ÿè§‰æ˜¯æ¨¡å‹è¯•å›¾ä»ä¸€ä¸ªçŠ¶æ€è½¬ç§»åˆ°å¦ä¸€ä¸ªçŠ¶æ€çš„ "ç¯å¢ƒ "å—ï¼Ÿçœ‹èµ·æ¥æ˜¯çš„ã€‚
    
-   **Actions**: The actions would probably be the words the in the modelâ€™s output. Weâ€™d probably each sentence to be an action, but then weâ€™d have an infinite action space, which doesnâ€™t work.  
    
    Probably best to have each word be an action, but then this complicates the â€œenvironmentâ€ part of the picture because users typically respond to complete thoughts, not individual words.  
    
    ä¹Ÿè®¸æœ€å¥½è®©æ¯ä¸ªè¯éƒ½æ˜¯ä¸€ä¸ªåŠ¨ä½œï¼Œä½†è¿™æ ·ä¼šä½¿ "ç¯å¢ƒ "éƒ¨åˆ†å˜å¾—å¤æ‚ï¼Œå› ä¸ºç”¨æˆ·é€šå¸¸ä¼šå¯¹å®Œæ•´çš„æ€æƒ³åšå‡ºååº”ï¼Œè€Œä¸æ˜¯å•ä¸ªçš„è¯ã€‚  
    
    è¡ŒåŠ¨ï¼šè¡ŒåŠ¨å¯èƒ½æ˜¯æ¨¡å‹è¾“å‡ºä¸­çš„è¯è¯­ã€‚æˆ‘ä»¬å¯èƒ½ä¼šæŠŠæ¯ä¸ªå¥å­å½“ä½œä¸€ä¸ªè¡ŒåŠ¨ï¼Œä½†è¿™æ ·æˆ‘ä»¬å°±ä¼šæœ‰ä¸€ä¸ªæ— é™çš„è¡ŒåŠ¨ç©ºé—´ï¼Œè¿™æ˜¯ä¸å¯è¡Œçš„ã€‚
    
-   **Consequences**: Given all of the above, how exactly do we quantify the impact of some bit of output language on the userâ€™s mental state in a way that the model can use as a numerical reward signal?  
    
    How do you assign a single numerical rating to all the bazillion ways a single human can feel about a line of text, and then how on earth do you normalize that rating across many different humans with many different feelings about the same text so that it actually represents something meaningful?  
    
    ä½ å¦‚ä½•ä¸ºä¸€ä¸ªäººå¯¹ä¸€è¡Œæ–‡å­—çš„æ‰€æœ‰æ„Ÿè§‰åˆ†é…ä¸€ä¸ªå•ä¸€çš„æ•°å­—ç­‰çº§ï¼Œç„¶åä½ å¦‚ä½•åœ¨è®¸å¤šä¸åŒçš„äººå¯¹åŒä¸€æ–‡å­—æœ‰è®¸å¤šä¸åŒçš„æ„Ÿè§‰çš„æƒ…å†µä¸‹ä½¿è¿™ä¸ªç­‰çº§æ­£å¸¸åŒ–ï¼Œä»è€Œä½¿å®ƒçœŸæ­£ä»£è¡¨ä¸€äº›æœ‰æ„ä¹‰çš„ä¸œè¥¿ï¼Ÿ  
    
    åæœï¼šè€ƒè™‘åˆ°ä¸Šè¿°æ‰€æœ‰æƒ…å†µï¼Œæˆ‘ä»¬ç©¶ç«Ÿå¦‚ä½•é‡åŒ–ä¸€äº›è¾“å‡ºè¯­è¨€å¯¹ç”¨æˆ·å¿ƒç†çŠ¶æ€çš„å½±å“ï¼Œä»¥ä½¿æ¨¡å‹å¯ä»¥ä½œä¸ºæ•°å­—å¥–åŠ±ä¿¡å·çš„æ–¹å¼ï¼Ÿ
    

ğŸ™‹â™‚ï¸ However we decide to solve these problems â€” different ML teams are solving them in different ways â€” one thing is immediately clear: to make RL work the way we want it to, where the model learns from humansâ€™ responses to its words and improves its output so that it makes its users happy and not sad, weâ€™re going to have to round up some humans and solicit their opinions about how the model is doing.  

ğŸ™‹â™‚ï¸ æ— è®ºæˆ‘ä»¬å¦‚ä½•å†³å®šè§£å†³è¿™äº›é—®é¢˜--ä¸åŒçš„MLå›¢é˜Ÿä»¥ä¸åŒçš„æ–¹å¼è§£å†³è¿™äº›é—®é¢˜--æœ‰ä¸€ä»¶äº‹æ˜¯ç«‹å³æ˜ç¡®çš„ï¼šä¸ºäº†ä½¿RLä»¥æˆ‘ä»¬å¸Œæœ›çš„æ–¹å¼å·¥ä½œï¼Œå³æ¨¡å‹ä»äººç±»å¯¹å…¶è¯­è¨€çš„ååº”ä¸­å­¦ä¹ å¹¶æ”¹è¿›å…¶è¾“å‡ºï¼Œä»è€Œä½¿å…¶ç”¨æˆ·é«˜å…´è€Œä¸æ˜¯æ‚²ä¼¤ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸å¬é›†ä¸€äº›äººç±»å¹¶å¾æ±‚ä»–ä»¬å¯¹æ¨¡å‹å¦‚ä½•å·¥ä½œçš„æ„è§ã€‚  

Weâ€™re going to needÂ **human feedback**.  

æˆ‘ä»¬å°†éœ€è¦äººç±»çš„åé¦ˆã€‚

One way naive way to use RL to fine-tune LLMs might be a simple scenario where a **selected committee of users** carries on a back-and-forth with the model and rates its outputs. Such a process, which might look as follows, would be extremely slow and expensive:  

ä½¿ç”¨RLæ¥å¾®è°ƒLLMçš„ä¸€ç§å¤©çœŸæ–¹å¼å¯èƒ½æ˜¯ä¸€ä¸ªç®€å•çš„åœºæ™¯ï¼Œå³ä¸€ä¸ªé€‰å®šçš„ç”¨æˆ·å§”å‘˜ä¼šä¸æ¨¡å‹è¿›è¡Œæ¥å›è®¨è®ºï¼Œå¹¶å¯¹å…¶è¾“å‡ºè¿›è¡Œè¯„çº§ã€‚è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹ï¼Œå¯èƒ½çœ‹èµ·æ¥å¦‚ä¸‹ï¼Œå°†æ˜¯éå¸¸ç¼“æ…¢å’Œæ˜‚è´µçš„ï¼š

1.  A **prompt** is chosen and fed to the model.  
    
    é€‰æ‹©ä¸€ä¸ªæç¤ºï¼Œå¹¶åé¦ˆç»™æ¨¡å‹ã€‚
    
2.  The model **responds** with a string of words.  
    
    æ¨¡å‹ç”¨ä¸€è¿ä¸²çš„è¯æ¥å›åº”ã€‚
    
3.  The users all **rate** the modelâ€™s response on a scale of -10 (it made me feel bad) to +10 (it made me feel good).  
    
    ç”¨æˆ·éƒ½å¯¹æ¨¡ç‰¹çš„ååº”è¿›è¡Œè¯„åˆ†ï¼Œè¯„åˆ†æ ‡å‡†ä¸º-10ï¼ˆè®©æˆ‘æ„Ÿè§‰ä¸å¥½ï¼‰åˆ°+10ï¼ˆè®©æˆ‘æ„Ÿè§‰ä¸é”™ï¼‰ã€‚
    
4.  These user ratings are all normalized and averaged, and then given to the model in the form of a single **reward number**.  
    
    è¿™äº›ç”¨æˆ·çš„è¯„åˆ†éƒ½è¢«å½’ä¸€åŒ–å’Œå¹³å‡åŒ–ï¼Œç„¶åä»¥å•ä¸€å¥–åŠ±æ•°å­—çš„å½¢å¼äº¤ç»™æ¨¡å‹ã€‚
    
5.  The model **updates its weights** based on the reward and is then ready for the next prompt.  
    
    è¯¥æ¨¡å‹æ ¹æ®å¥–åŠ±æ›´æ–°å…¶æƒé‡ï¼Œç„¶åä¸ºä¸‹ä¸€æ¬¡æç¤ºåšå¥½å‡†å¤‡ã€‚
    

Letâ€™s say we want to train the model on 30,000 prompts, which means 30,000 iterations of the above loop.  

å‡è®¾æˆ‘ä»¬æƒ³åœ¨30,000æ¡æç¤ºä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¿™æ„å‘³ç€ä¸Šè¿°å¾ªç¯çš„30,000æ¬¡è¿­ä»£ã€‚  

Now letâ€™s say each iteration takes 10 seconds, which makes for a total of 300,000 seconds, or aboutÂ **83 hours**Â per user to work through all 30K prompts in the corpus by rating the modelâ€™s output for each one. That is a lot of rating activity for justÂ _one single pass_Â through the training corpus. What if we do multiple passes? Weâ€™d quickly get into many thousands of person-hours to fine-tune a model.  

ç°åœ¨æˆ‘ä»¬å‡è®¾æ¯æ¬¡è¿­ä»£éœ€è¦10ç§’ï¼Œè¿™ä½¿å¾—æ€»å…±æœ‰300,000ç§’ï¼Œæˆ–è€…è¯´æ¯ä¸ªç”¨æˆ·éœ€è¦83ä¸ªå°æ—¶æ¥å®Œæˆè¯­æ–™åº“ä¸­æ‰€æœ‰çš„30Kä¸ªæç¤ºï¼Œå¯¹æ¯ä¸ªæç¤ºçš„æ¨¡å‹è¾“å‡ºè¿›è¡Œè¯„åˆ†ã€‚è¿™å¯¹äºå•æ¬¡é€šè¿‡è®­ç»ƒè¯­æ–™åº“æ¥è¯´ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„è¯„åˆ†æ´»åŠ¨ã€‚å¦‚æœæˆ‘ä»¬è¿›è¡Œå¤šæ¬¡è®­ç»ƒå‘¢ï¼Ÿæˆ‘ä»¬å¾ˆå¿«å°±ä¼šèŠ±è´¹æ•°åƒäººçš„æ—¶é—´æ¥å¾®è°ƒä¸€ä¸ªæ¨¡å‹ã€‚

**âœ¨ Thereâ€™s a better way:**Â use a special ML model thatâ€™s trained to impersonate the typical mental states of a specific group of humans.  

In other words, we have our hand-picked humans train a special model so that when we give it any sample of model output, it spits out the rating (from some negative value for bad up to some positive value for good) that our group might agree on for that output.  

æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬è®©æˆ‘ä»¬ç²¾å¿ƒæŒ‘é€‰çš„äººè®­ç»ƒä¸€ä¸ªç‰¹æ®Šçš„æ¨¡å‹ï¼Œè¿™æ ·å½“æˆ‘ä»¬ç»™å®ƒä»»ä½•æ¨¡å‹è¾“å‡ºçš„æ ·æœ¬æ—¶ï¼Œå®ƒå°±ä¼šåå‡ºæˆ‘ä»¬å°ç»„å¯èƒ½åŒæ„çš„å¯¹è¯¥è¾“å‡ºçš„è¯„çº§ï¼ˆä»æŸä¸ªè´Ÿå€¼è¡¨ç¤ºåï¼Œåˆ°æŸä¸ªæ­£å€¼è¡¨ç¤ºå¥½ï¼‰ã€‚  

âœ¨ æœ‰ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•ï¼šä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„MLæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»è¿‡è®­ç»ƒï¼Œå¯ä»¥å†’å……ç‰¹å®šäººç±»ç¾¤ä½“çš„å…¸å‹å¿ƒç†çŠ¶æ€ã€‚

Once youâ€™ve got this specialÂ **preference model**Â trained to spit out a numerical representation of a typical humanâ€™s feels about wordsâ€¦ hahaha ok I have to stop here for a sec because that is nuts right?  

Like, can we really boil down the impact of a wide variety of speech acts on a wide variety of humans to a single number that represents the reaction of a â€œtypicalâ€ human, and then train a neural net to produce that number when it looks at different speech acts (wholly out of context, I might add)?  

æ¯”å¦‚ï¼Œæˆ‘ä»¬çœŸçš„èƒ½æŠŠå„ç§å„æ ·çš„è¨€è¯­è¡Œä¸ºå¯¹å„ç§å„æ ·çš„äººçš„å½±å“å½’ç»“ä¸ºä¸€ä¸ªä»£è¡¨ "å…¸å‹ "äººç±»ååº”çš„å•ä¸€æ•°å­—ï¼Œç„¶åè®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œè®©å®ƒåœ¨çœ‹ä¸åŒçš„è¨€è¯­è¡Œä¸ºæ—¶äº§ç”Ÿè¿™ä¸ªæ•°å­—ï¼ˆæˆ‘å¯ä»¥è¡¥å……è¯´ï¼Œå®Œå…¨è„±ç¦»ä¸Šä¸‹æ–‡ï¼‰ï¼Ÿ  

ä¸€æ—¦ä½ å¾—åˆ°äº†è¿™ä¸ªç‰¹æ®Šçš„åå¥½æ¨¡å‹çš„è®­ç»ƒï¼Œå¯ä»¥åå‡ºä¸€ä¸ªå…¸å‹çš„äººç±»å¯¹å•è¯çš„æ„Ÿè§‰çš„æ•°å­—è¡¨ç¤º......å“ˆå“ˆå“ˆï¼Œå¥½å§ï¼Œæˆ‘å¿…é¡»åœ¨è¿™é‡Œåœä¸€ä¸‹ï¼Œå› ä¸ºè¿™æ˜¯ä¸ªç–¯å­ï¼Œå¯¹å—ï¼Ÿ

But what Iâ€™m describing here is literally what OpenAI has done with InstructGPT and subsequent models, including GPT-4. Wow, there are just so many assumptions to interrogate in all of this, but for now, I need to move on and finish this explanation.  

ä½†æˆ‘åœ¨è¿™é‡Œæ‰€æè¿°çš„ï¼Œå®é™…ä¸Šå°±æ˜¯OpenAIåœ¨InstructGPTå’Œåç»­æ¨¡å‹ï¼ŒåŒ…æ‹¬GPT-4ä¸­æ‰€åšçš„äº‹æƒ…ã€‚å“‡ï¼Œè¿™é‡Œé¢æœ‰å¤ªå¤šçš„å‡è®¾éœ€è¦æ‹·é—®ï¼Œä½†ç°åœ¨ï¼Œæˆ‘éœ€è¦ç»§ç»­å‰è¿›ï¼Œå®Œæˆè¿™ä¸ªè§£é‡Šã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F9cd5300d-9de9-453c-97f3-81b41ff2acf7_1500x772.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9cd5300d-9de9-453c-97f3-81b41ff2acf7_1500x772.jpeg)

Anyway, once you have theÂ **preference model**Â (also called a â€œreward modelâ€) trained to rate LLM output the way a representative human would, then you can go through that whole five-step process at the top of this section as many tens of thousands of times as you want without having to pay a bunch of people an hourly rate to bore themselves to death by rating every model output.  

æ€»ä¹‹ï¼Œä¸€æ—¦ä½ æœ‰äº†åå¥½æ¨¡å‹ï¼ˆä¹Ÿè¢«ç§°ä¸º "å¥–åŠ±æ¨¡å‹"ï¼‰çš„è®­ç»ƒï¼Œå¯ä»¥æŒ‰ç…§ä¸€ä¸ªæœ‰ä»£è¡¨æ€§çš„äººçš„æ–¹å¼æ¥è¯„ä»·LLMçš„è¾“å‡ºï¼Œé‚£ä¹ˆä½ å°±å¯ä»¥éšå¿ƒæ‰€æ¬²åœ°é€šè¿‡æœ¬èŠ‚é¡¶éƒ¨çš„æ•´ä¸ªäº”æ­¥è¿‡ç¨‹ï¼Œè€Œä¸å¿…å‘ä¸€ç¾¤äººæ”¯ä»˜æ¯å°æ—¶çš„è´¹ç”¨ï¼Œè®©ä»–ä»¬é€šè¿‡è¯„ä»·æ¯ä¸ªæ¨¡å‹çš„è¾“å‡ºæ¥æ— èŠåˆ°æ­»ã€‚

**In the next installment,**Â weâ€™ll take a deeper dive into this preference model: how it is trained, by whom, and on what corpus of texts. Sign up so you donâ€™t miss it.  

åœ¨ä¸‹ä¸€æœŸä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨è¿™ä¸ªåå¥½æ¨¡å‹ï¼šå®ƒæ˜¯å¦‚ä½•è¢«è®­ç»ƒçš„ï¼Œç”±è°æ¥è®­ç»ƒï¼Œä»¥åŠåœ¨ä»€ä¹ˆæ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒã€‚æ³¨å†Œå§ï¼Œè¿™æ ·ä½ å°±ä¸ä¼šé”™è¿‡ã€‚
