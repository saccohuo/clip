---
title: ""
date: 2023-06-25T17:32:54+08:00
updated: 2023-06-25T17:32:54+08:00
taxonomies:
  tags: []
extra:
  source: moz-extension://f4306eac-58d6-4001-b31f-a5a8b084f2ff/_generated_background_page.html
  hostname: f4306eac-58d6-4001-b31f-a5a8b084f2ff
  author: Jon Stokes
  original_title: "How To Regulate AI, If You Must"
  original_lang: zh
---

## How To Regulate AI, If You Must  

å¦‚ä½•ç›‘ç®¡äººå·¥æ™ºèƒ½ï¼Œå¦‚æœä½ å¿…é¡»è¿™æ ·åšçš„è¯

### The rules are definitely coming, so let's make sure they lead to a future we want.  

è§„åˆ™è‚¯å®šä¼šåˆ°æ¥ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç¡®ä¿å®ƒä»¬å¯¼è‡´ä¸€ä¸ªæˆ‘ä»¬æƒ³è¦çš„æœªæ¥ã€‚

[![](https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com2Fpublic2Fimages2F0d74c421-05b6-4f07-a9c7-09120a9bfb94_982x855.jpeg)](https://substack.com/profile/22541131-jon-stokes)

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2Fda3e8f29-7c4e-4988-8fe1-317cce204a04_1312x928.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda3e8f29-7c4e-4988-8fe1-317cce204a04_1312x928.png)

AI is both extremely powerful and entirely new to the human experience.  

äººå·¥æ™ºèƒ½æ—¢éå¸¸å¼ºå¤§ï¼Œåˆæ˜¯äººç±»ç»éªŒä¸­çš„å…¨æ–°äº‹ç‰©ã€‚  

Its power means that we are definitely going to make rules about it, and its novelty means those rules will initially be of the â€œfighting the last warâ€ variety and we will mostly regret them.  

å®ƒçš„åŠ›é‡æ„å‘³ç€æˆ‘ä»¬è‚¯å®šä¼šåˆ¶å®šæœ‰å…³å®ƒçš„è§„åˆ™ï¼Œè€Œå®ƒçš„æ–°é¢–æ€§æ„å‘³ç€è¿™äº›è§„åˆ™æœ€åˆå°†æ˜¯ "æ‰“æœ€åä¸€ä»— "çš„ç±»å‹ï¼Œè€Œä¸”æˆ‘ä»¬å¤§å¤šä¼šåæ‚”ã€‚

While we do not get to pick whether or not AI rules will exist (a certainty) or whether our first, clumsy stab at them will be a backward-looking, misguided net negative for humanity (also certain), the news isnâ€™t all bad.  

è™½ç„¶æˆ‘ä»¬æ— æ³•é€‰æ‹©äººå·¥æ™ºèƒ½è§„åˆ™æ˜¯å¦ä¼šå­˜åœ¨ï¼ˆè¿™æ˜¯è‚¯å®šçš„ï¼‰ï¼Œæˆ–è€…æˆ‘ä»¬ç¬¬ä¸€æ¬¡ç¬¨æ‹™çš„å°è¯•æ˜¯å¦ä¼šå¯¹äººç±»é€ æˆè½åçš„ã€è¯¯å¯¼çš„å‡€è´Ÿå€¼ï¼ˆä¹Ÿæ˜¯è‚¯å®šçš„ï¼‰ï¼Œä½†æ–°é—»å¹¶ä¸å…¨æ˜¯åäº‹ã€‚  

In fact, weâ€™re in a moment of incredible opportunity that comes around rarely in human history: those of us building in and writing about AI right now get to set the terms of the unfolding multi-generational debate over what this new thing is and what it should be.  

äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ­£å¤„äºä¸€ä¸ªäººç±»å†å²ä¸Šå¾ˆå°‘å‡ºç°çš„ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æœºä¼šæ—¶åˆ»ï¼šæˆ‘ä»¬è¿™äº›äººç°åœ¨æ­£åœ¨å»ºè®¾äººå·¥æ™ºèƒ½å’Œæ’°å†™å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œå¯ä»¥ä¸ºæ­£åœ¨å±•å¼€çš„å…³äºè¿™ä¸ªæ–°äº‹ç‰©æ˜¯ä»€ä¹ˆä»¥åŠå®ƒåº”è¯¥æ˜¯ä»€ä¹ˆçš„å¤šä»£äººè¾©è®ºè®¾å®šæ¡ä»¶ã€‚

But as much as Iâ€™d like to ramble on about LLMs as a type of [can-opener problem](https://www.jonstokes.com/p/the-can-opener-problem), and explore what it would look like to develop a new companion discipline to hermeneutics thatâ€™s aimed at theorizing about text generation, the rule-making around AI has already started in earnest, and I am by and large not a fan of the people who are making the rules and I am not expecting good results.  

ä½†æ˜¯ï¼Œå°½ç®¡æˆ‘å¾ˆæƒ³æŠŠæ³•å¾‹ç¡•å£«ä½œä¸ºä¸€ç§å¼€ç½å™¨çš„é—®é¢˜å–‹å–‹ä¸ä¼‘ï¼Œå¹¶æ¢ç´¢å‘å±•ä¸€é—¨æ—¨åœ¨å¯¹æ–‡æœ¬ç”Ÿæˆè¿›è¡Œç†è®ºç ”ç©¶çš„è§£é‡Šå­¦çš„æ–°ä¼™ä¼´å­¦ç§‘ä¼šæ˜¯ä»€ä¹ˆæ ·å­ï¼Œä½†å›´ç»•äººå·¥æ™ºèƒ½çš„è§„åˆ™åˆ¶å®šå·¥ä½œå·²ç»è®¤çœŸå¼€å§‹äº†ï¼Œè€Œæˆ‘åŸºæœ¬ä¸Šä¸å–œæ¬¢åˆ¶å®šè§„åˆ™çš„äººï¼Œæˆ‘å¯¹ç»“æœä¹Ÿä¸æŠ±æœŸæœ›ã€‚

âš”ï¸ So this post is aimed at people who, like me, are eyeing most of the would-be AI rule-makers with extreme suspicion and a sense that they are up to no good.  

âš”ï¸ æ‰€ä»¥è¿™ç¯‡æ–‡ç« æ˜¯é’ˆå¯¹é‚£äº›åƒæˆ‘ä¸€æ ·ï¼Œå¯¹å¤§å¤šæ•°å¯èƒ½çš„äººå·¥æ™ºèƒ½è§„åˆ™åˆ¶å®šè€…æŠ±æœ‰æåº¦æ€€ç–‘å’Œæ„Ÿè§‰ä»–ä»¬ä¸æ€€å¥½æ„çš„äººã€‚  

Those of us who are aligned on the answers to some key questions around technology, society, and human flourishing must immediately start talking about how we can wrest control of the rule-making process fromÂ **the safety-industrial complex**Â that is already dominating it.  

æˆ‘ä»¬ä¸­é‚£äº›å¯¹å›´ç»•æŠ€æœ¯ã€ç¤¾ä¼šå’Œäººç±»ç¹è£çš„ä¸€äº›å…³é”®é—®é¢˜çš„ç­”æ¡ˆä¿æŒä¸€è‡´çš„äººï¼Œå¿…é¡»ç«‹å³å¼€å§‹è®¨è®ºæˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿä»å·²ç»ä¸»å¯¼è§„åˆ™åˆ¶å®šè¿‡ç¨‹çš„å®‰å…¨å·¥ä¸šç»¼åˆä½“æ‰‹ä¸­å¤ºå›æ§åˆ¶æƒã€‚

Notice how in the opener to this post, I said â€œrulesâ€ and not â€œlaws.â€ In the network era, there are many more effective ways to enact rules that govern the lives of billions than the old paradigm of governments passing laws.  

æ³¨æ„åˆ°åœ¨è¿™ç¯‡æ–‡ç« çš„å¼€å¤´ï¼Œæˆ‘è¯´çš„æ˜¯ "è§„åˆ™ "è€Œä¸æ˜¯ "æ³•å¾‹"ã€‚åœ¨ç½‘ç»œæ—¶ä»£ï¼Œä¸æ”¿åºœé€šè¿‡æ³•å¾‹çš„æ—§æ¨¡å¼ç›¸æ¯”ï¼Œæœ‰è®¸å¤šæ›´æœ‰æ•ˆçš„æ–¹æ³•æ¥åˆ¶å®šè§„åˆ™ï¼Œç®¡ç†æ•°åäº¿äººçš„ç”Ÿæ´»ã€‚  

Moderation rules, terms of service, and the private arrangements that structure network architectures are examples of rules that touch more people on a visible, day-to-day level than most laws on the books.  

ç®¡ç†è§„åˆ™ã€æœåŠ¡æ¡æ¬¾ä»¥åŠæ„å»ºç½‘ç»œæ¶æ„çš„ç§äººå®‰æ’ï¼Œéƒ½æ˜¯åœ¨å¯è§çš„ã€æ—¥å¸¸çš„å±‚é¢ä¸Šæ¥è§¦åˆ°çš„è§„åˆ™çš„ä¾‹å­ï¼Œæ¯”ä¹¦æœ¬ä¸Šçš„å¤§å¤šæ•°æ³•å¾‹æ›´å¤šã€‚  

The more generalized term here is **governance**, a term that covers all the different kinds of rules at different layers of the stack.  

è¿™é‡Œæ›´æ¦‚æ‹¬çš„æœ¯è¯­æ˜¯æ²»ç†ï¼Œè¿™ä¸ªæœ¯è¯­æ¶µç›–äº†å †æ ˆä¸­ä¸åŒå±‚æ¬¡çš„æ‰€æœ‰ä¸åŒç§ç±»çš„è§„åˆ™ã€‚

So as I said, weâ€™re going to have rules about AI, but many of those rules may not take the form of laws passed by legacy nation-states.  

å› æ­¤ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œæˆ‘ä»¬å°†æœ‰å…³äºäººå·¥æ™ºèƒ½çš„è§„åˆ™ï¼Œä½†å…¶ä¸­è®¸å¤šè§„åˆ™å¯èƒ½ä¸ä¼šé‡‡å–ä¼ ç»Ÿæ°‘æ—å›½å®¶é€šè¿‡çš„æ³•å¾‹å½¢å¼ã€‚  

But I do want to focus narrowly on the â€œlawsâ€ part of the picture because I think this is the type of AI governance thatâ€™s in the most danger of going seriously sideways in short order.  

ä½†æˆ‘ç¡®å®æƒ³ç‹­éš˜åœ°å…³æ³¨ "æ³•å¾‹ "éƒ¨åˆ†ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºè¿™æ˜¯äººå·¥æ™ºèƒ½æ²»ç†ä¸­æœ€å±é™©çš„ç±»å‹ï¼Œä¼šåœ¨çŸ­æœŸå†…ä¸¥é‡åç¦»æ–¹å‘ã€‚

ğŸ¤¦â™‚ï¸ Why do I think that our legal process is about to mangle this whole AI thing? Two reasons:  

ğŸ¤¦â™‚ï¸ ä¸ºä»€ä¹ˆæˆ‘è®¤ä¸ºæˆ‘ä»¬çš„æ³•å¾‹ç¨‹åºå³å°†æŠŠæ•´ä¸ªäººå·¥æ™ºèƒ½çš„äº‹æƒ…å¼„ç³Ÿï¼Ÿæœ‰ä¸¤ä¸ªåŸå› ï¼š

1.  Who is working on the problem of AI governance right now  
    
    ç°åœ¨è°åœ¨ç ”ç©¶äººå·¥æ™ºèƒ½æ²»ç†çš„é—®é¢˜
    
2.  Who isÂ **not**Â working on the problem of AI governance right now  
    
    è°ç°åœ¨æ²¡æœ‰åœ¨ç ”ç©¶äººå·¥æ™ºèƒ½æ²»ç†çš„é—®é¢˜
    

### **Who is working on AI governance  

è°åœ¨ä»äº‹äººå·¥æ™ºèƒ½æ²»ç†å·¥ä½œ**

ğŸš” Per my contacts in policy circles and my own observation in my network, the burgeoning AI policy debate is presently dominated by the sameÂ **safety-industrial complex**Â that has come to dominate platform governance conversations in the social media era.  

æ ¹æ®æˆ‘åœ¨æ”¿ç­–åœˆå­é‡Œçš„æ¥è§¦å’Œæˆ‘è‡ªå·±åœ¨ç½‘ç»œä¸­çš„è§‚å¯Ÿï¼Œæ­£åœ¨å…´èµ·çš„äººå·¥æ™ºèƒ½æ”¿ç­–è¾©è®ºç›®å‰æ˜¯ç”±å®‰å…¨-å·¥ä¸šç»¼åˆä½“ä¸»å¯¼çš„ï¼Œè¿™ç§ç»¼åˆä½“å·²ç»ä¸»å¯¼äº†ç¤¾äº¤åª’ä½“æ—¶ä»£çš„å¹³å°æ²»ç†å¯¹è¯ã€‚

Iâ€™ll say more about this groupâ€™s tactics below in the section on social engineering, but my point here is that this safety-industrial complex was already fully mature and dug in at large companies and in universities and policy shops when AI cropped up as an apparently adjacent issue that they could all seamlessly expand their â€œwatchdogâ€ franchise into.  

æˆ‘å°†åœ¨ä¸‹é¢å…³äºç¤¾ä¼šå·¥ç¨‹çš„ç« èŠ‚ä¸­è¯¦ç»†ä»‹ç»è¿™ä¸ªå›¢ä½“çš„ç­–ç•¥ï¼Œä½†æˆ‘æƒ³è¯´çš„æ˜¯ï¼Œå½“äººå·¥æ™ºèƒ½ä½œä¸ºä¸€ä¸ªæ˜æ˜¾ç›¸é‚»çš„é—®é¢˜å‡ºç°æ—¶ï¼Œè¿™ä¸ªå®‰å…¨å·¥ä¸šç»¼åˆä½“å·²ç»å®Œå…¨æˆç†Ÿï¼Œå¹¶åœ¨å¤§å…¬å¸ã€å¤§å­¦å’Œæ”¿ç­–æœºæ„ä¸­æ·±è€•ç»†ä½œï¼Œä»–ä»¬éƒ½å¯ä»¥æ— ç¼åœ°å°†ä»–ä»¬çš„ "ç›‘ç£ "ç‰¹è®¸æƒæ‰©å±•åˆ°å…¶ä¸­ã€‚  

And expand it they have, with the result that everyone (like myself) who is opposed to this safeyist network will have to scramble to catch up to it if we want to save AI from it.  

è€Œä¸”ä»–ä»¬å·²ç»æ‰©å¤§äº†ï¼Œç»“æœæ˜¯æ¯ä¸ªåå¯¹è¿™ä¸ªå®‰å…¨ä¸»ä¹‰ç½‘ç»œçš„äººï¼ˆæ¯”å¦‚æˆ‘ï¼‰éƒ½ä¸å¾—ä¸äº‰å…ˆæååœ°è¿½èµ¶å®ƒï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä»å®ƒé‚£é‡Œæ‹¯æ•‘äººå·¥æ™ºèƒ½çš„è¯ã€‚

In the US, my impression is that the two main camps in the safety-industrial complex are what Iâ€™veÂ [previously called](https://www.jonstokes.com/p/ai-safety-a-technical-and-ethnographic)Â theÂ **X-riskers**Â and theÂ **language police**. Europe appears to be more dominated byÂ **Chernobylists**.Â   

åœ¨ç¾å›½ï¼Œæˆ‘çš„å°è±¡æ˜¯ï¼Œå®‰å…¨å·¥ä¸šç»¼åˆä½“çš„ä¸¤ä¸ªä¸»è¦é˜µè¥æ˜¯æˆ‘ä¹‹å‰æ‰€è¯´çš„X-é£é™©è€…å’Œè¯­è¨€è­¦å¯Ÿã€‚æ¬§æ´²çš„åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…ä¼¼ä¹æ›´å ä¼˜åŠ¿ã€‚

Hereâ€™s my earlier pieceâ€™s characterization of these camps:  

ä»¥ä¸‹æ˜¯æˆ‘ä¹‹å‰çš„æ–‡ç« å¯¹è¿™äº›é˜µè¥çš„æè¿°ï¼š

> _**The language police**: Worried that LLMs will say mean words, be used to spread disinformation, or be used for phishing attempts or other social manipulation on a large scale.  
> 
> AI ethicist Gary Marcus is in this camp, as are most â€œdisinfoâ€ and DEI advocacy types in the media and academia who are not deep into AI professionally but are opining about it.  
> 
> äººå·¥æ™ºèƒ½ä¼¦ç†å­¦å®¶åŠ é‡Œ-é©¬åº“æ–¯å°±åœ¨è¿™ä¸ªé˜µè¥ä¸­ï¼Œåª’ä½“å’Œå­¦æœ¯ç•Œä¸­çš„å¤§å¤šæ•° "å‡æƒ…æŠ¥ "å’ŒDEIå€¡å¯¼è€…ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œä»–ä»¬åœ¨ä¸“ä¸šä¸Šæ²¡æœ‰æ·±å…¥ç ”ç©¶äººå·¥æ™ºèƒ½ï¼Œä½†å´å¯¹å…¶å‘è¡¨æ„è§ã€‚  
> 
> è¯­è¨€è­¦å¯Ÿï¼šæ‹…å¿ƒæ³•å¾‹ç¡•å£«ä¼šè¯´åˆ»è–„çš„è¯ï¼Œè¢«ç”¨æ¥ä¼ æ’­è™šå‡ä¿¡æ¯ï¼Œæˆ–è¢«ç”¨æ¥è¿›è¡Œç½‘ç»œé’“é±¼æˆ–å…¶ä»–å¤§è§„æ¨¡çš„ç¤¾ä¼šæ“çºµã€‚_
> 
> _**The Chernobylists**: Worried about what will happen if we hook ML models we donâ€™t fully understand to real-life systems, especially critical ones or ones with weapons on them.  
> 
> David Chapman is in this camp, as am I.Â   
> 
> å¤§å«-æŸ¥æ™®æ›¼æ˜¯è¿™ä¸ªé˜µè¥çš„ï¼Œæˆ‘ä¹Ÿæ˜¯ã€‚  
> 
> åˆ‡å°”è¯ºè´åˆ©ä¸»ä¹‰è€…ï¼šæ‹…å¿ƒå¦‚æœæˆ‘ä»¬å°†æˆ‘ä»¬ä¸å®Œå…¨ç†è§£çš„MLæ¨¡å‹ä¸ç°å®ç”Ÿæ´»ä¸­çš„ç³»ç»ŸæŒ‚é’©ï¼Œç‰¹åˆ«æ˜¯å…³é”®ç³»ç»Ÿæˆ–æœ‰æ­¦å™¨çš„ç³»ç»Ÿï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚_
> 
> _**The x-riskers**: Absolutely convinced that the moment an AGI comes on the scene, humanity is doomed.  
> 
> Eliezer Yudkowsky is the most prominent person in this camp, but there are many others in rationalist and EA circles who fall into it.  
> 
> Eliezer Yudkowskyæ˜¯è¿™ä¸ªé˜µè¥ä¸­æœ€çªå‡ºçš„äººï¼Œä½†åœ¨ç†æ€§ä¸»ä¹‰å’ŒEAåœˆå­é‡Œè¿˜æœ‰å¾ˆå¤šäººå±äºè¿™ä¸ªé˜µè¥ã€‚  
> 
> å†’é£é™©è€…ï¼šä»–ä»¬ç»å¯¹ç›¸ä¿¡ï¼Œä¸€æ—¦AGIå‡ºç°ï¼Œäººç±»å°±æ³¨å®šè¦å¤±è´¥ã€‚_

I think the above description is still pretty accurate, but it is interesting to me to see the difference in who dominates the debate in which part of the world.  

æˆ‘è®¤ä¸ºä¸Šè¿°æè¿°ä»ç„¶ç›¸å½“å‡†ç¡®ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œçœ‹åˆ°è°åœ¨ä¸–ç•Œå“ªä¸ªåœ°æ–¹çš„è¾©è®ºä¸­å ä¸»å¯¼åœ°ä½çš„å·®å¼‚æ˜¯å¾ˆæœ‰è¶£çš„ã€‚  

Thereâ€™s something about the X-risk and language police camps that feel particularly American, to me, so it sort of scans that those are our main options while the Europeans are taking a more sensible (IMO) â€œproduct safetyâ€ approach â€” but more on that in a moment.  

å¯¹æˆ‘æ¥è¯´ï¼ŒX-é£é™©å’Œè¯­è¨€è­¦å¯Ÿé˜µè¥æœ‰ä¸€ç§ç‰¹åˆ«çš„ç¾å›½æ„Ÿè§‰ï¼Œæ‰€ä»¥è¿™æœ‰ç‚¹åƒæ‰«æï¼Œè¿™äº›æ˜¯æˆ‘ä»¬çš„ä¸»è¦é€‰æ‹©ï¼Œè€Œæ¬§æ´²äººæ­£åœ¨é‡‡å–ä¸€ç§æ›´æ˜æ™ºçš„ï¼ˆIMOï¼‰"äº§å“å®‰å…¨ "æ–¹æ³•--ä½†ä¸€ä¼šå„¿ä¼šæœ‰æ›´å¤šå…³äºè¿™ä¸ªé—®é¢˜çš„ä¿¡æ¯ã€‚

### **Who is not working on AI governance?  

è°æ²¡æœ‰åœ¨ä»äº‹äººå·¥æ™ºèƒ½æ²»ç†å·¥ä½œï¼Ÿ**

In the US, every industry has a trade association that typically lobbies Congress for favorable regulatory treatment.  

åœ¨ç¾å›½ï¼Œæ¯ä¸ªè¡Œä¸šéƒ½æœ‰ä¸€ä¸ªè¡Œä¸šåä¼šï¼Œé€šå¸¸ä¼šæ¸¸è¯´å›½ä¼šï¼Œäº‰å–æœ‰åˆ©çš„ç›‘ç®¡å¾…é‡ã€‚  

From advertisers to builders to Catholic colleges and universities â€” you name it, thereâ€™s a trade association for it.  

ä»å¹¿å‘Šå•†åˆ°å»ºç­‘å•†ï¼Œå†åˆ°å¤©ä¸»æ•™å­¦é™¢å’Œå¤§å­¦--ä½ çš„åå­—ï¼Œéƒ½æœ‰ä¸€ä¸ªè´¸æ˜“åä¼šã€‚

**Except for AI.** When it comes to AI, we have the exceedingly bizarre spectacle of prominent industry figures approaching Congress and asking for some kind of regulation, all without any apparent coordinating or governing body that speaks for the industry as a whole.  

é™¤äº†äººå·¥æ™ºèƒ½ã€‚è°ˆåˆ°äººå·¥æ™ºèƒ½ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæå…¶å¥‡æ€ªçš„æ™¯è±¡ï¼Œå³çŸ¥åçš„è¡Œä¸šäººå£«æ¥è¿‘å›½ä¼šï¼Œè¦æ±‚è¿›è¡ŒæŸç§ç›‘ç®¡ï¼Œè€Œæ‰€æœ‰è¿™äº›éƒ½æ²¡æœ‰ä»»ä½•æ˜æ˜¾çš„åè°ƒæˆ–ç®¡ç†æœºæ„æ¥ä»£è¡¨æ•´ä¸ªè¡Œä¸šã€‚

<iframe src="https://www.youtube-nocookie.com/embed/iqVxOZuqiSg?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0" loading="lazy" gesture="media" allow="autoplay; fullscreen" allowautoplay="true" allowfullscreen="true" width="728" height="409" frameborder="0"></iframe>

There is no **trade group** that most AI-focused funds and startups belong to and that is tirelessly working to ensure that startups can do basic things like buy or rent GPUs, train foundation models, launch new products based on new foundation models, and generally operate and iterate without an army of lawyers approving every code deploy.  

æ²¡æœ‰ä¸€ä¸ªè´¸æ˜“é›†å›¢æ˜¯å¤§å¤šæ•°ä¸“æ³¨äºäººå·¥æ™ºèƒ½çš„åŸºé‡‘å’Œåˆåˆ›å…¬å¸çš„æˆå‘˜ï¼Œä»–ä»¬ä¸çŸ¥ç–²å€¦åœ°å·¥ä½œï¼Œä»¥ç¡®ä¿åˆåˆ›å…¬å¸å¯ä»¥åšä¸€äº›åŸºæœ¬çš„äº‹æƒ…ï¼Œå¦‚è´­ä¹°æˆ–ç§Ÿç”¨GPUï¼Œè®­ç»ƒåŸºç¡€æ¨¡å‹ï¼Œæ¨å‡ºåŸºäºæ–°åŸºç¡€æ¨¡å‹çš„æ–°äº§å“ï¼Œä»¥åŠä¸€èˆ¬çš„æ“ä½œå’Œè¿­ä»£ï¼Œè€Œæ— éœ€å¾‹å¸ˆå¤§å†›æ‰¹å‡†æ¯ä¸€ä¸ªä»£ç éƒ¨ç½²ã€‚

This is weird and bad, and it must be remedied **ASAP**.  

è¿™å¾ˆå¥‡æ€ªï¼Œä¹Ÿå¾ˆç³Ÿç³•ï¼Œå¿…é¡»å°½å¿«è¡¥æ•‘ã€‚

Here are the frameworks that I see emerging for AI regs, frameworks that map pretty directly onto the aforementioned three main AI safety camps:  

ä»¥ä¸‹æ˜¯æˆ‘çœ‹åˆ°çš„äººå·¥æ™ºèƒ½æ³•è§„çš„æ¡†æ¶ï¼Œè¿™äº›æ¡†æ¶éå¸¸ç›´æ¥åœ°æ˜ å°„åˆ°ä¸Šè¿°ä¸‰ä¸ªä¸»è¦çš„äººå·¥æ™ºèƒ½å®‰å…¨é˜µè¥ï¼š

1.  **Existential risk**: This is the bucket things like nuclear weapons technology goes under.  
    
    å­˜åœ¨çš„é£é™©ï¼šè¿™å°±æ˜¯åƒæ ¸æ­¦å™¨æŠ€æœ¯è¿™æ ·çš„æ¡¶çŠ¶ç‰©çš„ä¸‹åœºã€‚
    
2.  **Product safety**: This is where the bulk of industry regulation has historically lived both in the US and Europe, and includes things like seatbelt laws, building standards, and other regulations meant to keep consumers physically safe.  
    
    äº§å“å®‰å…¨ï¼šè¿™æ˜¯ç¾å›½å’Œæ¬§æ´²å†å²ä¸Šå¤§éƒ¨åˆ†è¡Œä¸šç›‘ç®¡çš„åœ°æ–¹ï¼ŒåŒ…æ‹¬åƒå®‰å…¨å¸¦æ³•ã€å»ºç­‘æ ‡å‡†å’Œå…¶ä»–æ—¨åœ¨ä¿è¯æ¶ˆè´¹è€…èº«ä½“å®‰å…¨çš„æ³•è§„ã€‚
    
3.  **Social engineering**: This category contains laws like the Community Reinvestment Act, various affirmative action laws, and other laws aimed at changing society in a certain way by engineering certain types of outcomes.  
    
    ç¤¾ä¼šå·¥ç¨‹ï¼šè¿™ç±»æ³•å¾‹åŒ…æ‹¬ã€Šç¤¾åŒºå†æŠ•èµ„æ³•ã€‹ã€å„ç§å¹³ç­‰æƒåˆ©è¡ŒåŠ¨æ³•ä»¥åŠå…¶ä»–æ—¨åœ¨é€šè¿‡è®¾è®¡æŸäº›ç±»å‹çš„ç»“æœä»¥æŸç§æ–¹å¼æ”¹å˜ç¤¾ä¼šçš„æ³•å¾‹ã€‚
    

Iâ€™ll take the last two in reverse order, ignoring x-risks entirely because the Europeans are ignoring that issue and that is great.  

æˆ‘å°†æŒ‰ç›¸åçš„é¡ºåºé‡‡å–åä¸¤è€…ï¼Œå®Œå…¨å¿½ç•¥X-é£é™©ï¼Œå› ä¸ºæ¬§æ´²äººæ­£åœ¨å¿½ç•¥è¿™ä¸ªé—®é¢˜ï¼Œè¿™å¾ˆå¥½ã€‚  

I wish we could ignore them here in the US, but unfortunately, we canâ€™t. We still have to fight them.  

æˆ‘å¸Œæœ›æˆ‘ä»¬åœ¨ç¾å›½å¯ä»¥æ— è§†ä»–ä»¬ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½ã€‚æˆ‘ä»¬ä»ç„¶å¿…é¡»ä¸ä»–ä»¬æ–—äº‰ã€‚  

(First, we fight the doomers, then we laugh at them, then we ignore them, then we win.) I have fought them in other articles, though, so IÂ [refer you](https://www.jonstokes.com/p/ai-safety-a-technical-and-ethnographic)Â toÂ [those](https://www.jonstokes.com/p/heres-what-it-would-take-to-slow).  

(é¦–å…ˆï¼Œæˆ‘ä»¬ä¸å„è¿è€…æ–—äº‰ï¼Œç„¶åæˆ‘ä»¬å˜²ç¬‘ä»–ä»¬ï¼Œç„¶åæˆ‘ä»¬æ— è§†ä»–ä»¬ï¼Œç„¶åæˆ‘ä»¬èµ¢äº†ï¼‰ã€‚ä¸è¿‡ï¼Œæˆ‘åœ¨å…¶ä»–æ–‡ç« ä¸­ä¸ä»–ä»¬è¿›è¡Œäº†æ–—äº‰ï¼Œæ‰€ä»¥æˆ‘è¯·ä½ çœ‹é‚£äº›æ–‡ç« ã€‚

### **Social engineeringÂ ç¤¾ä¼šå·¥ç¨‹**

Thereâ€™s a lot of potential slippage between categories two and three, especially in the post-COVID era. Itâ€™s worth unpacking why this is the case.  

ç¬¬äºŒç±»å’Œç¬¬ä¸‰ç±»ä¹‹é—´æœ‰å¾ˆå¤šæ½œåœ¨çš„æ»‘å¡ï¼Œç‰¹åˆ«æ˜¯åœ¨åCOVIDæ—¶ä»£ã€‚å€¼å¾—è§£è¯»çš„æ˜¯ï¼Œä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚

**Threat inflation**Â is a core tactic of the safety-industrial complex, and this is mainly accomplished by discovering new types of â€œharmsâ€ that can be framed as â€œviolenceâ€ or otherwise â€œtrauma-inducingâ€ and therefore placed under the banner of â€œsafety.â€ This threat inflation has the effect of raising the status (and typically the funding) of people doing the â€œharmsâ€ or â€œviolenceâ€ identification, and it also gives them more leverage in arguments by raising the stakes so that literal lives are on the line because whatever dispute weâ€™re trying to adjudicate isÂ _really_Â a life-or-death struggle between a clear hero and a clear villain.  

å¨èƒè†¨èƒ€æ˜¯å®‰å…¨å·¥ä¸šç»¼åˆä½“çš„æ ¸å¿ƒç­–ç•¥ï¼Œä¸»è¦æ˜¯é€šè¿‡å‘ç°æ–°çš„ "ä¼¤å®³ "ç±»å‹æ¥å®ç°çš„ï¼Œè¿™äº›ä¼¤å®³å¯ä»¥è¢«å®šä¹‰ä¸º "æš´åŠ› "æˆ–å…¶ä»– "è¯±å‘åˆ›ä¼¤"ï¼Œä»è€Œè¢«ç½®äº "å®‰å…¨ "çš„æ——å¸œä¸‹ã€‚è¿™ç§å¨èƒè†¨èƒ€çš„æ•ˆæœæ˜¯æé«˜äº†è¿›è¡Œ "ä¼¤å®³ "æˆ– "æš´åŠ› "é‰´å®šçš„äººçš„åœ°ä½ï¼ˆé€šå¸¸æ˜¯èµ„é‡‘ï¼‰ï¼Œè€Œä¸”è¿˜é€šè¿‡æé«˜èµŒæ³¨ä½¿ä»–ä»¬åœ¨äº‰è®ºä¸­æ‹¥æœ‰æ›´å¤šçš„ç­¹ç ï¼Œå› ä¸ºæ— è®ºæˆ‘ä»¬è¦è£å†³çš„æ˜¯ä»€ä¹ˆäº‰ç«¯ï¼Œéƒ½æ˜¯ä¸€åœºæ˜æ˜¾çš„è‹±é›„å’Œæ˜æ˜¾çš„æ¶æ£ä¹‹é—´çš„ç”Ÿæ­»ä¹‹æˆ˜ã€‚

The end result of threat inflation is that anyone trying to enforce a distinction between **physical harm** or violence and **psychological harm** or efforts to address historical inequities (which are said to lead directly to physical harm) does so in the face of increasing opposition from the safety-industrial complex.  

å¨èƒè†¨èƒ€çš„æœ€ç»ˆç»“æœæ˜¯ï¼Œä»»ä½•è¯•å›¾åœ¨èº«ä½“ä¼¤å®³æˆ–æš´åŠ›ä¸å¿ƒç†ä¼¤å®³æˆ–è§£å†³å†å²ä¸å¹³ç­‰ï¼ˆæ®è¯´ç›´æ¥å¯¼è‡´èº«ä½“ä¼¤å®³ï¼‰çš„åŠªåŠ›ä¹‹é—´å®æ–½åŒºåˆ†çš„äººï¼Œéƒ½ä¼šé¢ä¸´æ¥è‡ªå®‰å…¨å·¥ä¸šç»¼åˆä½“è¶Šæ¥è¶Šå¤šçš„åå¯¹ã€‚

âœ‹ But I think we have toÂ **insist on this line**Â so that we can actually find a reasonable basis for doing basic product safety regulation, because if every product safety regulation discussion turns into a shouting match over â€œequityâ€ then we will end up with the worst possible outcome, i.e., no actual product safety but a thicket of tyrannical, dysfunctional rules that makes a few activists and consultants happy and everyone else miserable.  

(The term â€œanarcho-tyrannyâ€ is relevant, here.)Â   

(åœ¨è¿™é‡Œï¼Œ"æ— æ”¿åºœä¸»ä¹‰è€… "ä¸€è¯æ˜¯ç›¸å…³çš„ï¼‰ã€‚  

ä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬å¿…é¡»åšæŒè¿™æ¡è·¯çº¿ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½çœŸæ­£æ‰¾åˆ°ä¸€ä¸ªåˆç†çš„åŸºç¡€æ¥è¿›è¡ŒåŸºæœ¬çš„äº§å“å®‰å…¨ç›‘ç®¡ï¼Œå› ä¸ºå¦‚æœæ¯ä¸€æ¬¡äº§å“å®‰å…¨ç›‘ç®¡çš„è®¨è®ºéƒ½å˜æˆäº†ä¸€åœºå…³äº "å…¬å¹³ "çš„å¤§å–Šå¤§å«ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°æœ€åçš„ç»“æœï¼Œå³æ²¡æœ‰å®é™…çš„äº§å“å®‰å…¨ï¼Œè€Œæ˜¯ä¸€ä¸›æš´è™çš„ã€åŠŸèƒ½å¤±è°ƒçš„è§„åˆ™ï¼Œè®©å°‘æ•°æ´»åŠ¨å®¶å’Œé¡¾é—®é«˜å…´ï¼Œè€Œå…¶ä»–äººåˆ™å¾ˆç—›è‹¦ã€‚

I should note that Iâ€™m **not actually opposed** to social engineering â€” Iâ€™m just going to insist that when we do it, itâ€™s under two conditions:  

æˆ‘åº”è¯¥æŒ‡å‡ºï¼Œæˆ‘å®é™…ä¸Šå¹¶ä¸åå¯¹ç¤¾ä¼šå·¥ç¨‹--æˆ‘åªæ˜¯è¦åšæŒï¼Œå½“æˆ‘ä»¬è¿™æ ·åšçš„æ—¶å€™ï¼Œæ˜¯åœ¨ä¸¤ä¸ªæ¡ä»¶ä¸‹ï¼š

1.  Itâ€™s clearly marked as social engineering, and is not conflated with â€œsafety.â€  
    
    å®ƒè¢«æ¸…æ¥šåœ°æ ‡è®°ä¸ºç¤¾ä¼šå·¥ç¨‹ï¼Œå¹¶æ²¡æœ‰ä¸ "å®‰å…¨ "æ··ä¸ºä¸€è°ˆã€‚
    
2.  Itâ€™s in the service of engineering the kinds of outcomesÂ _I_Â want and not the kinds of outcomes my culture war opponents want.  
    
    For instance, social engineering that protects kids from algorithmic manipulation is good, as is pro-natalist social engineering that encourages families to stay together and to have children.  
    
    ä¾‹å¦‚ï¼Œä¿æŠ¤å­©å­ä¸å—ç®—æ³•æ“çºµçš„ç¤¾ä¼šå·¥ç¨‹æ˜¯å¥½çš„ï¼Œé¼“åŠ±å®¶åº­å‘†åœ¨ä¸€èµ·å’Œç”Ÿå­©å­çš„äº²äº§ä¸»ä¹‰ç¤¾ä¼šå·¥ç¨‹ä¹Ÿæ˜¯å¥½çš„ã€‚  
    
    è¿™æ˜¯åœ¨ä¸ºæˆ‘æƒ³è¦çš„é‚£ç§ç»“æœè€Œä¸æ˜¯æˆ‘çš„æ–‡åŒ–æˆ˜äº‰å¯¹æ‰‹æƒ³è¦çš„é‚£ç§ç»“æœæœåŠ¡ã€‚
    

So if in the name of â€œprogress,â€ you want to require AI models to promote some specific vision of how society should be ordered that is different from the way it is presently ordered, I am gonna give that the big old Chad â€œYesâ€ because I have my own opinions about how everything should go and if Iâ€™m going to have to hear yours then **youâ€™re going to have to hear mine**.Â   

å› æ­¤ï¼Œå¦‚æœä»¥ "è¿›æ­¥ "çš„åä¹‰ï¼Œä½ æƒ³è¦æ±‚äººå·¥æ™ºèƒ½æ¨¡å‹ä¿ƒè¿›ä¸€äº›ç‰¹å®šçš„æ„¿æ™¯ï¼Œå³ç¤¾ä¼šåº”è¯¥å¦‚ä½•æ’åºï¼Œä¸ç›®å‰çš„æ’åºæ–¹å¼ä¸åŒï¼Œæˆ‘ä¼šç»™è¿™ä¸ªå¤§çš„è€ä¹å¾— "æ˜¯"ï¼Œå› ä¸ºæˆ‘æœ‰è‡ªå·±çš„æ„è§ï¼Œå…³äºä¸€åˆ‡åº”è¯¥å¦‚ä½•è¿›è¡Œï¼Œå¦‚æœæˆ‘ä¸å¾—ä¸å¬ä½ çš„ï¼Œé‚£ä¹ˆä½ å°†ä¸å¾—ä¸å¬æˆ‘çš„ã€‚

[![Yes Chad | Know Your Meme](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F7b6fe720-90e7-4caa-b11e-dc1e7ac39b2d_680x709.png "Yes Chad | Know Your Meme")](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7b6fe720-90e7-4caa-b11e-dc1e7ac39b2d_680x709.png)

If thereâ€™s social engineering to be done, then I, my co-religionists, and anyone willing to make common cause with us are going to form a coalition and use every means at our disposal to ensure thatÂ _our_Â values and mores are the ones enshrined in the models that everyone else has to use.  

å¦‚æœæœ‰ç¤¾ä¼šå·¥ç¨‹è¦åšï¼Œé‚£ä¹ˆæˆ‘ã€æˆ‘çš„å…±åŒå®—æ•™ä¿¡ä»°è€…ä»¥åŠä»»ä½•æ„¿æ„ä¸æˆ‘ä»¬å»ºç«‹å…±åŒäº‹ä¸šçš„äººéƒ½å°†ç»„æˆä¸€ä¸ªè”ç›Ÿï¼Œå¹¶åˆ©ç”¨æˆ‘ä»¬æ‰€æŒæ¡çš„ä¸€åˆ‡æ‰‹æ®µæ¥ç¡®ä¿æˆ‘ä»¬çš„ä»·å€¼è§‚å’Œé“å¾·è§‚è¢«è½½å…¥å…¶ä»–äººå¿…é¡»ä½¿ç”¨çš„æ¨¡å¼ä¸­ã€‚

I actually tend to think that social engineering efforts should be confined to the extralegal parts of the governance stack, i.e., terms of service, moderation, and the like.  

å®é™…ä¸Šï¼Œæˆ‘å€¾å‘äºè®¤ä¸ºç¤¾ä¼šå·¥ç¨‹çš„åŠªåŠ›åº”è¯¥å±€é™äºæ²»ç†å †æ ˆçš„æ³•å¾‹ä¹‹å¤–çš„éƒ¨åˆ†ï¼Œå³æœåŠ¡æ¡æ¬¾ã€å®¡æ ¸ï¼Œä»¥åŠç±»ä¼¼å†…å®¹ã€‚  

The one place it seems obvious to me thatÂ **the law**Â should play a role in social engineering is in ensuring that each group has a representative on the social engineering committee.  

åœ¨æˆ‘çœ‹æ¥ï¼Œæ³•å¾‹åº”è¯¥åœ¨ç¤¾ä¼šå·¥ç¨‹ä¸­å‘æŒ¥ä½œç”¨çš„ä¸€ä¸ªæ˜æ˜¾çš„åœ°æ–¹æ˜¯ï¼Œç¡®ä¿æ¯ä¸ªç¾¤ä½“åœ¨ç¤¾ä¼šå·¥ç¨‹å§”å‘˜ä¼šä¸­æœ‰ä¸€ä¸ªä»£è¡¨ã€‚

**Hereâ€™s the TL;DR of this section:  

è¿™é‡Œæ˜¯æœ¬èŠ‚çš„TL;DRï¼š**

-   Social engineering isÂ **good**, actually.  
    
    ç¤¾ä¼šå·¥ç¨‹æ˜¯å¥½çš„ï¼Œå®é™…ä¸Šã€‚
    
-   When we do social engineering, we have toÂ **be clear**Â that this is what weâ€™re doing, and that itâ€™s different from product safety.  
    
    å½“æˆ‘ä»¬åšç¤¾ä¼šå·¥ç¨‹æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»æ¸…æ¥šè¿™å°±æ˜¯æˆ‘ä»¬æ­£åœ¨åšçš„äº‹æƒ…ï¼Œè€Œä¸”å®ƒä¸äº§å“å®‰å…¨ä¸åŒã€‚
    
-   If weâ€™re doing social engineering, then the law should ensure thatÂ **all the stakeholders**Â must be represented.  
    
    Not just technocrats from a certain set of schools and institutions, but everybody, including many groups that the legacy media and the SPLC are trying really hard to unperson.Â   
    
    ä¸ä»…ä»…æ˜¯æ¥è‡ªæŸä¸€å¥—å­¦æ ¡å’Œæœºæ„çš„æŠ€æœ¯å®˜åƒšï¼Œè€Œæ˜¯æ‰€æœ‰äººï¼ŒåŒ…æ‹¬ä¼ ç»Ÿåª’ä½“å’ŒSPLCéå¸¸åŠªåŠ›åœ°æƒ³è¦æ¶ˆé™¤çš„è®¸å¤šç¾¤ä½“ã€‚  
    
    å¦‚æœæˆ‘ä»¬åœ¨åšç¤¾ä¼šå·¥ç¨‹ï¼Œé‚£ä¹ˆæ³•å¾‹åº”è¯¥ç¡®ä¿æ‰€æœ‰çš„åˆ©ç›Šç›¸å…³è€…éƒ½å¿…é¡»å¾—åˆ°ä»£è¡¨ã€‚
    
-   In cases thereâ€™sÂ **only one**Â widely deployed model and one possible set of socially engineered outcomes that this model can be tuned for, then you should expect me to stop at nothing to ensure that we to tune it forÂ _my_Â preferred outcomes and not yours. If this attitude shocks you, then you should ask yourself why you were expecting me to just roll over and let you have your way.  
    
    å¦‚æœåªæœ‰ä¸€ä¸ªå¹¿æ³›éƒ¨ç½²çš„æ¨¡å‹å’Œä¸€å¥—å¯èƒ½çš„ç¤¾ä¼šå·¥ç¨‹ç»“æœï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥è¢«è°ƒæ•´ï¼Œé‚£ä¹ˆä½ åº”è¯¥æœŸæœ›æˆ‘ä¸æƒœä¸€åˆ‡ä»£ä»·ï¼Œç¡®ä¿æˆ‘ä»¬æŠŠå®ƒè°ƒæ•´ä¸ºæˆ‘å–œæ¬¢çš„ç»“æœï¼Œè€Œä¸æ˜¯ä½ çš„ã€‚å¦‚æœè¿™ç§æ€åº¦è®©ä½ æ„Ÿåˆ°éœ‡æƒŠï¼Œé‚£ä¹ˆä½ åº”è¯¥é—®é—®è‡ªå·±ï¼Œä¸ºä»€ä¹ˆä½ æœŸæœ›æˆ‘åªæ˜¯ç¿»èº«ï¼Œè®©ä½ å¾—é€ã€‚
    
-   My instinct is that we should prefer to do social engineering via governance methods thatÂ **donâ€™t require passing**Â **laws** but that are overseen by the law. I may change my mind on this, though, as I think about it more.  
    
    æˆ‘çš„ç›´è§‰æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥æ›´å€¾å‘äºé€šè¿‡ä¸éœ€è¦é€šè¿‡æ³•å¾‹ä½†å—æ³•å¾‹ç›‘ç£çš„æ²»ç†æ–¹æ³•æ¥è¿›è¡Œç¤¾ä¼šå·¥ç¨‹ã€‚ä¸è¿‡ï¼Œéšç€æˆ‘çš„æ·±å…¥æ€è€ƒï¼Œæˆ‘å¯èƒ½ä¼šæ”¹å˜æˆ‘çš„æƒ³æ³•ã€‚
    

### **Product safetyÂ äº§å“å®‰å…¨**

Iâ€™veÂ [written quite a bit](https://www.jonstokes.com/p/ai-safety-is-ai-the-genie-or-the)Â recently on the necessity of treating AI as a software tool and not as an agent or a coworker.  

I harp on this distinction for a practical reason: the tool framework for AI naturally lends itself to a product safety-based governance regime, and the coworker framework naturally lends itself to a social engineering-based governance regime, and I prefer the former to the latter.  

æˆ‘å¼ºè°ƒè¿™ä¸€åŒºåˆ«æ˜¯å‡ºäºä¸€ä¸ªå®é™…åŸå› ï¼šäººå·¥æ™ºèƒ½çš„å·¥å…·æ¡†æ¶è‡ªç„¶é€‚åˆåŸºäºäº§å“å®‰å…¨çš„æ²»ç†åˆ¶åº¦ï¼Œè€ŒåŒäº‹æ¡†æ¶è‡ªç„¶é€‚åˆåŸºäºç¤¾ä¼šå·¥ç¨‹çš„æ²»ç†åˆ¶åº¦ï¼Œæˆ‘æ›´å–œæ¬¢å‰è€…è€Œä¸æ˜¯åè€…ã€‚  

æˆ‘æœ€è¿‘å†™äº†ä¸å°‘å…³äºæŠŠäººå·¥æ™ºèƒ½å½“ä½œè½¯ä»¶å·¥å…·è€Œä¸æ˜¯ä»£ç†æˆ–åŒäº‹çš„å¿…è¦æ€§ã€‚

The essence of the â€œproduct safetyâ€ approach is to stay away from making too many rules about â€œAIâ€ in the abstract, or even about foundation models, and to focus onÂ **validating specific implementations**Â of machine learning models. In other words, focus on the products and services that are in the market, not the tech that is in the lab.  

"äº§å“å®‰å…¨ "æ–¹æ³•çš„æœ¬è´¨æ˜¯è¿œç¦»å¯¹æŠ½è±¡çš„ "äººå·¥æ™ºèƒ½"ï¼Œç”šè‡³å¯¹åŸºç¡€æ¨¡å‹åˆ¶å®šè¿‡å¤šçš„è§„åˆ™ï¼Œè€Œä¸“æ³¨äºéªŒè¯æœºå™¨å­¦ä¹ æ¨¡å‹çš„å…·ä½“å®ç°ã€‚æ¢å¥è¯è¯´ï¼Œä¸“æ³¨äºå¸‚åœºä¸Šçš„äº§å“å’ŒæœåŠ¡ï¼Œè€Œä¸æ˜¯å®éªŒå®¤é‡Œçš„æŠ€æœ¯ã€‚

TheÂ [concept of scope](https://www.jonstokes.com/i/118247675/ai-safety-and-the-concept-of-scope)Â that Iâ€™ve previously written about applies here.  

æˆ‘ä»¥å‰å†™è¿‡çš„èŒƒå›´çš„æ¦‚å¿µåœ¨è¿™é‡Œä¹Ÿé€‚ç”¨ã€‚

> _So in this example, Iâ€™m taking into account the specific use case to which I plan to put the model, and then trying to adapt the model so that its performance in that use case meets my needs.  
> 
> å› æ­¤ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘è€ƒè™‘åˆ°äº†æˆ‘è®¡åˆ’å°†æ¨¡å‹ç”¨äºçš„å…·ä½“ç”¨ä¾‹ï¼Œç„¶åè¯•å›¾è°ƒæ•´æ¨¡å‹ï¼Œä½¿å…¶åœ¨è¯¥ç”¨ä¾‹ä¸­çš„æ€§èƒ½æ»¡è¶³æˆ‘çš„éœ€æ±‚ã€‚  
> 
> Iâ€™ve defined a project scope, Iâ€™ve developed a solution, and Iâ€™ve validated that solution based on some predefined acceptance criteria.  
> 
> æˆ‘å·²ç»å®šä¹‰äº†ä¸€ä¸ªé¡¹ç›®èŒƒå›´ï¼Œå¼€å‘äº†ä¸€ä¸ªè§£å†³æ–¹æ¡ˆï¼Œå¹¶æ ¹æ®ä¸€äº›é¢„å®šçš„éªŒæ”¶æ ‡å‡†éªŒè¯äº†è¯¥è§£å†³æ–¹æ¡ˆã€‚_

With this in mind, I was encouraged to learn that the Europeans are taking a product safety approach with their recently announced EU AI Act.  

è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘å¾ˆå—é¼“èˆåœ°äº†è§£åˆ°ï¼Œæ¬§æ´²äººæ­£åœ¨é€šè¿‡ä»–ä»¬æœ€è¿‘å®£å¸ƒçš„ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹é‡‡å–äº§å“å®‰å…¨çš„æ–¹æ³•ã€‚  

I hosted a Twitter space on this with two of the people working on this law, and it left me feeling a lot less panicked about the state of AI law in the EU.  

æˆ‘ä¸ä»äº‹è¿™é¡¹æ³•å¾‹å·¥ä½œçš„ä¸¤ä¸ªäººä¸€èµ·ä¸»æŒäº†ä¸€ä¸ªTwitterç©ºé—´ï¼Œå®ƒè®©æˆ‘å¯¹æ¬§ç›Ÿçš„äººå·¥æ™ºèƒ½æ³•å¾‹çŠ¶å†µæ„Ÿåˆ°ä¸é‚£ä¹ˆæƒŠæ…Œã€‚

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F6659bf15-6f8a-44ec-a76c-99ef10d6592a_1190x780.png)](https://twitter.com/jonst0kes/status/1671217986693214209)

ğŸ¦ºÂ It sounds like the Europeans are going to focus their rule-making efforts on specific AI implementations, not so much on the underlying tech.  

å¬èµ·æ¥ï¼Œæ¬§æ´²äººå°†æŠŠåˆ¶å®šè§„åˆ™çš„åŠªåŠ›é›†ä¸­åœ¨å…·ä½“çš„äººå·¥æ™ºèƒ½å®æ–½ä¸Šï¼Œè€Œä¸æ˜¯åœ¨åº•å±‚æŠ€æœ¯ä¸Šã€‚  

This is good because the right way to regulate AI under a product safety regime is not to regulate â€œAIâ€ in the abstract but toÂ **regulate specific products**, in the same way we already do. Is the product itself safe and does it do what itâ€™s supposed to do when itâ€™s being used the way it was designed to be used?  

If the answer to both those questions is â€œyes,â€ then who cares what state the underlying model is in?  

å¦‚æœè¿™ä¸¤ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ "æ˜¯"ï¼Œé‚£ä¹ˆè°ä¼šå…³å¿ƒåº•å±‚æ¨¡å‹å¤„äºä»€ä¹ˆçŠ¶æ€ï¼Ÿ  

è¿™æ˜¯å¥½äº‹ï¼Œå› ä¸ºåœ¨äº§å“å®‰å…¨åˆ¶åº¦ä¸‹ç›‘ç®¡äººå·¥æ™ºèƒ½çš„æ­£ç¡®æ–¹å¼ä¸æ˜¯æŠ½è±¡åœ°ç›‘ç®¡ "äººå·¥æ™ºèƒ½"ï¼Œè€Œæ˜¯ä»¥æˆ‘ä»¬å·²ç»åšçš„åŒæ ·æ–¹å¼æ¥ç›‘ç®¡å…·ä½“äº§å“ã€‚äº§å“æœ¬èº«æ˜¯å¦å®‰å…¨ï¼Œå½“å®ƒä»¥å…¶è®¾è®¡çš„æ–¹å¼è¢«ä½¿ç”¨æ—¶ï¼Œå®ƒæ˜¯å¦åšäº†å®ƒåº”è¯¥åšçš„äº‹ï¼Ÿ

As I said in the Twitter Space if a model is being used in the very narrow context of, say, product support, itâ€™s properly sandboxed so that users canâ€™t interact with it on out-of-scope topics, then what does it matter if the model is on one side or the other of some hot-button issue?  

æ­£å¦‚æˆ‘åœ¨Twitterç©ºé—´ä¸­æ‰€è¯´çš„ï¼Œå¦‚æœä¸€ä¸ªæ¨¡å‹è¢«ç”¨åœ¨éå¸¸ç‹­çª„çš„èŒƒå›´å†…ï¼Œæ¯”å¦‚è¯´ï¼Œäº§å“æ”¯æŒï¼Œå®ƒè¢«é€‚å½“åœ°æ²™ç›’åŒ–ï¼Œä½¿ç”¨æˆ·ä¸èƒ½åœ¨èŒƒå›´å¤–çš„è¯é¢˜ä¸Šä¸ä¹‹äº’åŠ¨ï¼Œé‚£ä¹ˆï¼Œå¦‚æœè¿™ä¸ªæ¨¡å‹åœ¨æŸäº›çƒ­ç‚¹é—®é¢˜ä¸Šç«™åœ¨ä¸€è¾¹ï¼Œåˆæœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ  

The answer is that it shouldnâ€™t matter to anyone who isnâ€™t trying to do backdoor social engineering by trying to limit the market for â€œproblematicâ€ models.  

ç­”æ¡ˆæ˜¯ï¼Œå¯¹äºé‚£äº›ä¸æ˜¯è¯•å›¾é€šè¿‡é™åˆ¶ "æœ‰é—®é¢˜ "è½¦å‹çš„å¸‚åœºæ¥è¿›è¡Œåé—¨ç¤¾ä¼šå·¥ç¨‹çš„äººæ¥è¯´ï¼Œè¿™ä¸åº”è¯¥æ˜¯é—®é¢˜ã€‚

On a practical level, a product safety approach to AI regulation would mainly consist of **updating existing product safety laws** to take into account possible ML integrations.  

åœ¨å®è·µå±‚é¢ä¸Šï¼Œäººå·¥æ™ºèƒ½ç›‘ç®¡çš„äº§å“å®‰å…¨æ–¹æ³•ä¸»è¦åŒ…æ‹¬æ›´æ–°ç°æœ‰çš„äº§å“å®‰å…¨æ³•ï¼Œä»¥è€ƒè™‘åˆ°å¯èƒ½çš„MLæ•´åˆã€‚

And sure, we can worry a little about out-of-scope uses for products, but worrying too much is bad.  

å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºäº§å“çš„è¶…èŒƒå›´ä½¿ç”¨æ‹…å¿ƒä¸€ä¸‹ï¼Œä½†æ‹…å¿ƒå¾—å¤ªå¤šå°±ä¸å¥½äº†ã€‚  

If you murder someone with a kitchen knife, that is an out-of-scope use that in America (in contrast to the UK) we donâ€™t tend to try and address with regulation.  

å¦‚æœä½ ç”¨èœåˆ€æ€äººï¼Œè¿™æ˜¯ä¸€ç§è¶…å‡ºèŒƒå›´çš„ä½¿ç”¨ï¼Œåœ¨ç¾å›½ï¼ˆä¸è‹±å›½ç›¸åï¼‰ï¼Œæˆ‘ä»¬ä¸å€¾å‘äºå°è¯•ç”¨ç›‘ç®¡æ¥è§£å†³ã€‚  

Itâ€™s good that we treat kitchen knives this way in America, and we should treat models this way, as well. I hope the Europeans adopt this approach to AI (and to kitchen knives).  

åœ¨ç¾å›½ï¼Œæˆ‘ä»¬è¿™æ ·å¯¹å¾…èœåˆ€æ˜¯ä»¶å¥½äº‹ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥è¿™æ ·å¯¹å¾…æ¨¡å‹ã€‚æˆ‘å¸Œæœ›æ¬§æ´²äººå¯¹äººå·¥æ™ºèƒ½ï¼ˆä»¥åŠå¯¹å¨æˆ¿åˆ€å…·ï¼‰é‡‡å–è¿™ç§åšæ³•ã€‚

ğŸ¤ One thing it does sound like the EU is worried about with the AI Act isÂ **industry capture**.  

Theyâ€™ve apparently learned some hard lessons from GDPR, and the fact that this legislation favors deep-pocketed incumbents who can hire armies of compliance lawyers was brought up repeatedly in the space as an example of what the EU wants to avoid with AI.  

ä»–ä»¬æ˜¾ç„¶å·²ç»ä»GDPRä¸­å¸å–äº†ä¸€äº›æƒ¨ç—›çš„æ•™è®­ï¼Œè¿™é¡¹ç«‹æ³•æœ‰åˆ©äºé‚£äº›å¯ä»¥é›‡ä½£åˆè§„å¾‹å¸ˆå¤§å†›çš„è´¢å¤§æ°”ç²—çš„ç°æœ‰ä¼ä¸šï¼Œè¿™ä¸€äº‹å®åœ¨è¯¥é¢†åŸŸè¢«åå¤æèµ·ï¼Œä½œä¸ºæ¬§ç›Ÿæƒ³è¦é¿å…äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªä¾‹å­ã€‚  

Again, this is encouraging.  

åŒæ ·ï¼Œè¿™ä¹Ÿæ˜¯ä»¤äººé¼“èˆçš„ã€‚  

ğŸ¤ å¬èµ·æ¥æ¬§ç›Ÿå¯¹ã€Šäººå·¥æ™ºèƒ½æ³•ã€‹æ‰€æ‹…å¿ƒçš„ä¸€ä»¶äº‹æ˜¯äº§ä¸šä¿˜è·ã€‚

The US should definitely adopt this approach from the EU. Regulatory capture should come up constantly at AI-related congressional hearings.  

ç¾å›½è‚¯å®šåº”è¯¥é‡‡ç”¨æ¬§ç›Ÿçš„è¿™ç§æ–¹æ³•ã€‚åœ¨ä¸äººå·¥æ™ºèƒ½ç›¸å…³çš„å›½ä¼šå¬è¯ä¼šä¸Šï¼Œåº”è¯¥ä¸æ–­æå‡ºç›‘ç®¡çš„é—®é¢˜ã€‚  

Unfortunately, Iâ€™ve yet to hear the term brought up by lawmakers in the hearings Iâ€™ve watched, though I have heard a lot from them about China, bias, and other hot topics.  

ä¸å¹¸çš„æ˜¯ï¼Œåœ¨æˆ‘è§‚çœ‹çš„å¬è¯ä¼šä¸Šï¼Œæˆ‘è¿˜æ²¡æœ‰å¬åˆ°ç«‹æ³•è€…æå‡ºè¿™ä¸ªè¯ï¼Œå°½ç®¡æˆ‘ä»ä»–ä»¬é‚£é‡Œå¬åˆ°äº†å¾ˆå¤šå…³äºä¸­å›½ã€åè§å’Œå…¶ä»–çƒ­é—¨è¯é¢˜ã€‚

The other positive thing about the EU approach thatâ€™s worth imitating is the specific carve-outs forÂ **open-source models**.  

The Eurocrats seem to be bullish on open-source AI, and as well they should be because itâ€™s Europeâ€™s best hope for transitioning to a world thatâ€™s no longer dominated by US-based Big Tech platforms.  

æ¬§æ´²å®˜åƒšä¼¼ä¹çœ‹å¥½å¼€æºäººå·¥æ™ºèƒ½ï¼Œä»–ä»¬ä¹Ÿåº”è¯¥è¿™æ ·åšï¼Œå› ä¸ºè¿™æ˜¯æ¬§æ´²è¿‡æ¸¡åˆ°ä¸€ä¸ªä¸å†ç”±ç¾å›½å¤§ç§‘æŠ€å¹³å°ä¸»å¯¼çš„ä¸–ç•Œçš„æœ€å¤§å¸Œæœ›ã€‚  

æ¬§ç›Ÿçš„åšæ³•å€¼å¾—æ¨¡ä»¿çš„å¦ä¸€ä¸ªç§¯ææ–¹é¢æ˜¯å¯¹å¼€æºæ¨¡å¼çš„å…·ä½“åˆ’åˆ†ã€‚

### **Other product safety laws that might be good  

å…¶ä»–å¯èƒ½æœ‰å¥½å¤„çš„äº§å“å®‰å…¨æ³•**

If I were to tweak my libertarian readers by proposing some laws that might place positive obligations on companies, then in the spirit ofÂ **transparency**Â I might suggest we require product makers to disclose where theyâ€™re using ML in their products, and to what end.Â   

å¦‚æœæˆ‘å¯¹æˆ‘çš„è‡ªç”±ä¸»ä¹‰è¯»è€…è¿›è¡Œè°ƒæ•´ï¼Œæå‡ºä¸€äº›å¯èƒ½ç»™å…¬å¸å¸¦æ¥ç§¯æä¹‰åŠ¡çš„æ³•å¾‹ï¼Œé‚£ä¹ˆæœ¬ç€é€æ˜çš„ç²¾ç¥ï¼Œæˆ‘å¯èƒ½å»ºè®®æˆ‘ä»¬è¦æ±‚äº§å“åˆ¶é€ å•†æŠ«éœ²ä»–ä»¬åœ¨äº§å“ä¸­ä½¿ç”¨MLçš„åœ°æ–¹ï¼Œä»¥åŠä¸ºäº†ä»€ä¹ˆç›®çš„ã€‚

ğŸªª Iâ€™d also suggest that Congress find ways to support the development of open-source, interoperableÂ **licensure and credentialing**Â protocols for data labelers and RLHF preference model trainers. The idea here is that the public should be able to see who isÂ [catechizing the bots](https://www.jonstokes.com/p/catechizing-the-bots-part-1-foundation)Â theyâ€™re using, and what those bot trainersâ€™ backgrounds, credentials, and values are.  

We can do this in either a maximally privacy-preserving way with crypto or a minimally privacy-preserving way with a network of centralized authorities.  

æˆ‘ä»¬å¯ä»¥ç”¨åŠ å¯†çš„æ–¹å¼æœ€å¤§é™åº¦åœ°ä¿æŠ¤éšç§ï¼Œä¹Ÿå¯ä»¥ç”¨é›†ä¸­å¼æœºæ„ç½‘ç»œçš„æ–¹å¼æœ€å°é™åº¦åœ°ä¿æŠ¤éšç§ã€‚  

æˆ‘è¿˜å»ºè®®å›½ä¼šæƒ³åŠæ³•æ”¯æŒä¸ºæ•°æ®æ ‡æ³¨è€…å’ŒRLHFåå¥½æ¨¡å‹åŸ¹è®­è€…åˆ¶å®šå¼€æºçš„ã€å¯äº’æ“ä½œçš„è®¸å¯å’Œè®¤è¯åè®®ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œå…¬ä¼—åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°è°åœ¨ä¸ºä»–ä»¬æ­£åœ¨ä½¿ç”¨çš„æœºå™¨äººæä¾›è¾…å¯¼ï¼Œä»¥åŠè¿™äº›æœºå™¨äººåŸ¹è®­å¸ˆçš„èƒŒæ™¯ã€è¯ä¹¦å’Œä»·å€¼è§‚æ˜¯ä»€ä¹ˆã€‚

These product safety ideas arenâ€™t the only thing we should be doing as far as AI governance.  

å°±äººå·¥æ™ºèƒ½æ²»ç†è€Œè¨€ï¼Œè¿™äº›äº§å“å®‰å…¨ç†å¿µå¹¶ä¸æ˜¯æˆ‘ä»¬å”¯ä¸€åº”è¯¥åšçš„äº‹æƒ…ã€‚  

We also should do things that are outside the â€œsafetyâ€ framework entirely but that will still make upstream contributions to AI safety efforts. For instance, we should:  

æˆ‘ä»¬è¿˜åº”è¯¥åšä¸€äº›å®Œå…¨åœ¨ "å®‰å…¨ "æ¡†æ¶ä¹‹å¤–çš„äº‹æƒ…ï¼Œä½†è¿™äº›äº‹æƒ…ä»ç„¶ä¼šå¯¹äººå·¥æ™ºèƒ½å®‰å…¨å·¥ä½œåšå‡ºä¸Šæ¸¸è´¡çŒ®ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åº”è¯¥ï¼š

-   Create a positive right to buy/rent GPUs and train foundation models.  
    
    åˆ›é€ ä¸€ä¸ªè´­ä¹°/ç§Ÿç”¨GPUå’Œè®­ç»ƒåŸºç¡€æ¨¡å‹çš„ç§¯ææƒåˆ©ã€‚
    
-   FollowÂ [Japanâ€™s lead](https://decrypt.co/143461/ai-art-wars-japan-says-ai-model-training-doesnt-violate-copyright)Â and declare all copyrighted material available for model training.  
    
    è·Ÿéšæ—¥æœ¬çš„æ­¥ä¼ï¼Œå®£å¸ƒæ‰€æœ‰å—ç‰ˆæƒä¿æŠ¤çš„ææ–™å¯ç”¨äºæ¨¡å‹åŸ¹è®­ã€‚
    
-   Support the development of control surfaces (RLHF, soft prompting, editing correlations and concepts out of models) that make it easier to fit models to specific applications.  
    
    æ”¯æŒå¼€å‘æ§åˆ¶é¢ï¼ˆRLHFã€è½¯æç¤ºã€ç¼–è¾‘å…³è”å’Œæ¦‚å¿µå‡ºæ¨¡å‹ï¼‰ï¼Œä½¿ä¹‹æ›´å®¹æ˜“é€‚åˆæ¨¡å‹çš„å…·ä½“åº”ç”¨ã€‚
    
-   Support explainability research.  
    
    æ”¯æŒå¯è§£é‡Šæ€§ç ”ç©¶ã€‚
    
-   Support the development of deployment environments that sandbox the model in such a way that its inputs and outputs are narrowly constrained at can easily be scoped to a specific application, with the user being unable to use the model out-of-scope.  
    
    æ”¯æŒéƒ¨ç½²ç¯å¢ƒçš„å‘å±•ï¼Œä½¿æ¨¡å‹çš„è¾“å…¥å’Œè¾“å‡ºå—åˆ°ä¸¥æ ¼çš„é™åˆ¶ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°ä¸€ä¸ªç‰¹å®šçš„åº”ç”¨ï¼Œè€Œç”¨æˆ·æ— æ³•åœ¨èŒƒå›´ä¹‹å¤–ä½¿ç”¨æ¨¡å‹ã€‚
    

[![](https3A2F2Fsubstack-post-media.s3.amazonaws.com2Fpublic2Fimages2F9b2a3572-3a55-4125-987d-d59d6a937401_1312x928.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9b2a3572-3a55-4125-987d-d59d6a937401_1312x928.png)

Silicon Valley in general, and especially startups, donâ€™t like to really think about Washington DC until something happens that brings regulations down on them.  

ä¸€èˆ¬æ¥è¯´ï¼Œç¡…è°·ï¼Œç‰¹åˆ«æ˜¯åˆåˆ›ä¼ä¸šï¼Œä¸å–œæ¬¢çœŸæ­£è€ƒè™‘åç››é¡¿ç‰¹åŒºï¼Œç›´åˆ°å‘ç”Ÿäº†ä¸€äº›äº‹æƒ…ï¼Œä½¿ä»–ä»¬çš„æ³•è§„ä¸‹é™ã€‚

But in the case of AI, that something hasÂ _already_Â happened: OpenAIâ€™s Sam Altman has gone to Congress and invited that body to regulate this technology (with his input and on his terms, of course).  

So generative AI is already not going to play out like crypto, where the technology had over a decade to get big enough for regulators to care about.  

å› æ­¤ï¼Œç”Ÿæˆæ€§äººå·¥æ™ºèƒ½å·²ç»ä¸ä¼šåƒåŠ å¯†è´§å¸é‚£æ ·å‘å±•ï¼Œåœ¨é‚£é‡Œï¼Œè¯¥æŠ€æœ¯æœ‰è¶…è¿‡åå¹´çš„æ—¶é—´å˜å¾—è¶³å¤Ÿå¤§ï¼Œè®©ç›‘ç®¡æœºæ„å…³æ³¨ã€‚  

ä½†å°±äººå·¥æ™ºèƒ½è€Œè¨€ï¼Œè¿™ç§æƒ…å†µå·²ç»å‘ç”Ÿäº†ï¼šOpenAIçš„è¨å§†-å¥¥ç‰¹æ›¼ï¼ˆSam Altmanï¼‰å·²ç»å‰å¾€å›½ä¼šï¼Œé‚€è¯·è¯¥æœºæ„å¯¹è¿™é¡¹æŠ€æœ¯è¿›è¡Œç›‘ç®¡ï¼ˆå½“ç„¶æ˜¯åœ¨ä»–çš„æ„è§å’Œæ¡ä»¶ä¸‹ï¼‰ã€‚

Itâ€™s also the case that â€œAIâ€ is the slowest-moving sudden revolution anyone has ever seen.  

è¿˜æœ‰ä¸€ç§æƒ…å†µæ˜¯ï¼Œ"äººå·¥æ™ºèƒ½ "æ˜¯äººä»¬æ‰€è§è¿‡çš„è¿›å±•æœ€æ…¢çš„çªç„¶é©å‘½ã€‚  

The big breakthrough moment for this tech, which some would put at ChatGPT but I personally would put earlier at the public launch of DALL-E, was decades in the making even if all the pieces havenâ€™t come together fully until the last two years.  

è¿™é¡¹æŠ€æœ¯çš„é‡å¤§çªç ´æ—¶åˆ»ï¼Œæœ‰äº›äººè®¤ä¸ºæ˜¯åœ¨ChatGPTï¼Œä½†æˆ‘ä¸ªäººè®¤ä¸ºæ˜¯åœ¨DALL-Eçš„å…¬å¼€å‘å¸ƒä¹‹å‰ï¼Œå³ä½¿æ‰€æœ‰çš„ç¢ç‰‡ç›´åˆ°æœ€è¿‘ä¸¤å¹´æ‰å®Œå…¨ç»„åˆèµ·æ¥ï¼Œä¹Ÿå·²ç»æœ‰å‡ åå¹´çš„æ—¶é—´äº†ã€‚

**My point:**Â Those of us investing and building in AI need to start acting like weâ€™re in a mature industry that everyone can plainly see is a Very Big Deal and is imminently coming under regulatory scrutiny.  

Whatever off-the-radar â€œmove fast and break thingsâ€ grace period AI had is over.  

æ— è®ºäººå·¥æ™ºèƒ½æœ‰ä»€ä¹ˆä¸å¼•äººæ³¨ç›®çš„ "å¿«é€Ÿè¡ŒåŠ¨å¹¶æ‰“ç ´ä¸œè¥¿ "çš„å®½é™æœŸï¼Œéƒ½å·²ç»ç»“æŸã€‚  

æˆ‘çš„è§‚ç‚¹ï¼šæˆ‘ä»¬è¿™äº›æŠ•èµ„å’Œå»ºè®¾äººå·¥æ™ºèƒ½çš„äººéœ€è¦å¼€å§‹è¡¨ç°å¾—åƒæˆ‘ä»¬èº«å¤„ä¸€ä¸ªæˆç†Ÿçš„è¡Œä¸šï¼Œæ¯ä¸ªäººéƒ½å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„é—®é¢˜ï¼Œå¹¶ä¸”å¾ˆå¿«å°±ä¼šå—åˆ°ç›‘ç®¡éƒ¨é—¨çš„å®¡æŸ¥ã€‚

âœŠ Those of us who are optimistic about AI and who donâ€™t want to see this new technology suffocated in the cradle by self-appointed safety tzars have a lot of catching up to do.  

æˆ‘ä»¬è¿™äº›å¯¹äººå·¥æ™ºèƒ½æŒä¹è§‚æ€åº¦çš„äººï¼Œä»¥åŠä¸å¸Œæœ›çœ‹åˆ°è¿™é¡¹æ–°æŠ€æœ¯è¢«è‡ªå°çš„å®‰å…¨ä¸“å®¶æ‰¼æ€åœ¨æ‘‡ç¯®é‡Œçš„äººï¼Œè¿˜æœ‰å¾ˆå¤šäº‹æƒ…è¦åšã€‚  

Itâ€™s time we started to organize and maybe even produce some open letters of our own.  

ç°åœ¨æ˜¯æˆ‘ä»¬å¼€å§‹ç»„ç»‡èµ·æ¥çš„æ—¶å€™äº†ï¼Œä¹Ÿè®¸ç”šè‡³å¯ä»¥åˆ¶ä½œä¸€äº›æˆ‘ä»¬è‡ªå·±çš„å…¬å¼€ä¿¡ã€‚

As the anti-safety-industrial-complex coalition develops, Iâ€™ll keep my readers apprised of next steps and concrete ways to get involved.  

éšç€åå®‰å…¨å·¥ä¸šç»¼åˆä½“è”ç›Ÿçš„å‘å±•ï¼Œæˆ‘å°†å‘æˆ‘çš„è¯»è€…é€šæŠ¥ä¸‹ä¸€æ­¥è¡ŒåŠ¨å’Œå‚ä¸çš„å…·ä½“æ–¹æ³•ã€‚
